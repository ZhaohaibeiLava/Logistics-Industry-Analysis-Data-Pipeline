{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754cd750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "库导入完成。\n",
      "项目文件夹结构设置/检查完毕。请按以下结构组织文件：\n",
      "将10个安监数据Excel文件放入: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输入/安监数据\n",
      "⚠️ 请修改“京邦达”的名字为“京东”\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 1: 导入库并设置项目结构\n",
    "# --------------------------------------------------\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"库导入完成。\")\n",
    "\n",
    "# --- 项目路径设置  ---\n",
    "# .\n",
    "# ├── 1_data_preprocess.ipynb\n",
    "# └── 报告数据/\n",
    "#     ├── 输入/\n",
    "#     │   ├── 安监数据/              (存放老师给的原始Excel文件；10家公司，10个文件)\n",
    "#     │   └── basic_data.xlsx          (城市信息、线路信息)\n",
    "#     ├── 输出/                      (存放所有最终生成的报告)\n",
    "#     ├── temp/\n",
    "#     │   ├── 1_待上传猪猪云数据/        (需要逐个手动上传到猪猪云的文件；8家公司，16个文件，排除顺丰和中通)\n",
    "#     │   ├── 2_猪猪云下载数据/          (【手动放入】存放从猪猪云下载的结果文件；8家公司，16个文件，排除顺丰和中通)\n",
    "#     │   ├── 3_猪猪云合并数据/         （猪猪云下载数据按公司合并后数据；8家公司，8个文件，排除顺丰和中通）\n",
    "#     │   ├── 4_logistics数据         （存放logistics数据——提取完整物流信息的时间戳后的数据；8家公司，8个文件，排除顺丰和中通）\n",
    "#     └── └── 5_中转数据/               (存放中转数据——提取中转城市和平均中转次数后的数据；8家公司，8个文件，排除顺丰和中通)\n",
    "# 根目录\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "# 输入路径\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "# 输出路径\n",
    "output_path = report_path / \"输出\"\n",
    "# 中间过程文件路径（自动创建，用于存放临时文件）\n",
    "temp_path = report_path / \"temp\"\n",
    "upload_split_path = temp_path / \"1_待上传猪猪云文件\"  # 存放拆分后待上传的文件\n",
    "zhuzhuyun_download_path = temp_path / \"2_猪猪云下载数据\"  # 关键：这是手动放置文件的目录\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "transit_data_path = temp_path / \"5_中转数据\"\n",
    "\n",
    "# 创建所有需要的文件夹\n",
    "for p in [\n",
    "    report_path,\n",
    "    input_path,\n",
    "    anjian_data_path,\n",
    "    zhuzhuyun_download_path,\n",
    "    zhuzhuyun_merge_path,\n",
    "    transit_data_path,\n",
    "    output_path,\n",
    "    temp_path,\n",
    "    upload_split_path,\n",
    "    pycharm_input_path,\n",
    "]:\n",
    "    p.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"项目文件夹结构设置/检查完毕。请按以下结构组织文件：\")\n",
    "print(\n",
    "    f\"将10个安监数据Excel文件放入: {anjian_data_path}\\n⚠️ 请修改“京邦达”的名字为“京东”\"\n",
    ")\n",
    "# 计算口径问题\n",
    "# 1.所有的均值、最优值、排名均不计算快包（和EMS本质是一家公司的不同产品）\n",
    "# 2.线路的送达天数计算使用的是80分位数\n",
    "# 3.线路层的均值不加权（计算全部线路均值，需要用∑（单一线路均值*线路快递数量）/快递总数量，或直接对全部线路求均值不延续适用线路均值；但行业层面可直接对公司层面的均值（不包括快包），直接求简单的算术平均（并非全样本真实均值、只是简单对各公司水平求均值）\n",
    "# 4.“单一数据源原则”：logistics经过\n",
    "### (data[\"寄出地处理时限\"].between(0.1，48))\n",
    "### (data[\"运输时限\"].between(0.5，200))&(data[\"寄达地处理时限\"].between(0.1，60))&\n",
    "### (data[“投递时限\"].between(0，36))]\n",
    "### 筛选后得到data_analysis_result；后续的数据只能够基于data_analysis_result进一步计算，不能再重新回到 logistics 数据\n",
    "# 5.“48小时妥投率”口径：经过筛选后的数据（即data_analysis_result）的基础上，仅使用“Top 30城市互寄”的数据来计算；而“72小时妥投率”直接用data_analysis_result数据，不经过“Top 30城市“筛选\n",
    "# 6.邮政月报.xlsx中的“分城市明细”和“分省份明细”sheet中\n",
    "### 全程时限（双挂）是该城市按照所有以该城市为寄出地和寄达地的邮件进行聚合\n",
    "### 全程时限（寄出地）是对应的城市按照所有以该城市为寄出地的邮件进行聚合\n",
    "### 寄出地处理时限、揽收-到达寄出地分拣中心时长、到达寄出地分拣中心-离开寄出地城市时长是按照所有该城市作为寄出地的邮件聚合\n",
    "### 寄达地处理时限、到达寄达地城市-离开寄达地分拣中心时长、离开寄达地分拣中心-派件是按照所有该城市作为寄达地的邮件聚合\n",
    "### 投递时限是按照所有该城市作为寄达地的邮件聚合\n",
    "### 寄出地处理时限+寄达地处理时限+投递时限是前面的简单加和，即寄出地处理时限（按寄出地聚合）+寄达地处理时限（按寄达地聚合）+投递时限（按寄达地聚合）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22767ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 第1部分：开始准备上传数据...\n",
      "正在处理: 2025年7月韵达抽样.xlsx (公司: 韵达)\n",
      "  -> 韵达 数据已拆分为 2 个文件。\n",
      "公司 '顺丰' 在排除列表内，跳过准备上传。\n",
      "正在处理: 2025年7月京东抽样.xlsx (公司: 京东)\n",
      "  -> 京东 数据已拆分为 2 个文件。\n",
      "正在处理: 2025年7月极兔抽样.xlsx (公司: 极兔)\n",
      "  -> 极兔 数据已拆分为 2 个文件。\n",
      "正在处理: 2025年7月德邦抽样.xlsx (公司: 德邦)\n",
      "  -> 德邦 数据已拆分为 2 个文件。\n",
      "正在处理: 2025年7月申通抽样.xlsx (公司: 申通)\n",
      "  -> 申通 数据已拆分为 2 个文件。\n",
      "正在处理: 2025年7月EMS抽样.xlsx (公司: EMS)\n",
      "  -> EMS 数据已拆分为 1 个文件。\n",
      "正在处理: 2025年7月圆通抽样.xlsx (公司: 圆通)\n",
      "  -> 圆通 数据已拆分为 2 个文件。\n",
      "正在处理: 2025年7月中国邮政抽样.xlsx (公司: 邮政)\n",
      "  -> 邮政 数据已拆分为 2 个文件。\n",
      "公司 '中通' 在排除列表内，跳过准备上传。\n",
      "\n",
      "====================================================================================================\n",
      "【第一部分完成】数据准备完毕！\n",
      "请前往文件夹: \n",
      "/Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/1_待上传猪猪云文件\n",
      "将里面的所有Excel文件手动上传到“猪猪快递云”网站，\n",
      "\n",
      "最后将所有下载结果放入“/报告数据/输入/猪猪云下载数据/”文件夹中。\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 2: 拆分安监数据，准备用于猪猪云上传的数据\n",
    "# --------------------------------------------------\n",
    "def prepare_data_for_upload(\n",
    "    source_dir: Path,\n",
    "    target_dir: Path,\n",
    "    companies_to_exclude: list,\n",
    "    chunk_size: int = 50000,\n",
    "):\n",
    "    \"\"\"\n",
    "    读取安监数据，保留'企业'和'单号'列，并按chunk_size动态拆分为n个文件，为手动上传做准备。\n",
    "    \"\"\"\n",
    "    # 确保运行此函数所需的库已导入\n",
    "    import math\n",
    "    from pathlib import Path\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\">>> 第1部分：开始准备上传数据...\")\n",
    "    # 确保目标文件夹存在，并清空旧的待上传文件\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for f in target_dir.glob(\"*.xlsx\"):\n",
    "        f.unlink()\n",
    "\n",
    "    anjian_files = list(source_dir.glob(\"*.xlsx\"))\n",
    "    if not anjian_files:\n",
    "        print(f\"错误：在 {source_dir} 中未找到任何安监数据文件。请先放置文件。\")\n",
    "        return\n",
    "\n",
    "    for file_path in anjian_files:\n",
    "        company_name_found = False\n",
    "        # 公司列表根据文件名灵活匹配\n",
    "        companies = [\n",
    "            \"EMS\",\n",
    "            \"中通\",\n",
    "            \"京东\",\n",
    "            \"圆通\",\n",
    "            \"德邦\",\n",
    "            \"极兔\",\n",
    "            \"申通\",\n",
    "            \"韵达\",\n",
    "            \"邮政\",\n",
    "            \"顺丰\",\n",
    "        ]\n",
    "        for company in companies:\n",
    "            if company in file_path.stem:\n",
    "                company_name = company\n",
    "                company_name_found = True\n",
    "                break\n",
    "        if not company_name_found:\n",
    "            print(f\"警告: 文件 {file_path.name} 未能匹配到已知公司名，已跳过。\")\n",
    "            continue\n",
    "\n",
    "        if company_name in companies_to_exclude:\n",
    "            print(f\"公司 '{company_name}' 在排除列表内，跳过准备上传。\")\n",
    "            continue\n",
    "\n",
    "        print(f\"正在处理: {file_path.name} (公司: {company_name})\")\n",
    "\n",
    "        # 读取并校验Excel文件\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, dtype={\"单号\": str})\n",
    "            if \"企业\" not in df.columns or \"单号\" not in df.columns:\n",
    "                print(\n",
    "                    f\"  -> 警告: 文件 {file_path.name} 缺少 '企业' 或 '单号' 列，已跳过。\"\n",
    "                )\n",
    "                continue\n",
    "            df_to_upload = df[[\"企业\", \"单号\"]].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"  -> 错误: 读取文件 {file_path.name} 时出错: {e}，已跳过。\")\n",
    "            continue\n",
    "\n",
    "        # --- 这是核心修改部分：动态拆分为 n 个文件 ---\n",
    "        total_rows = len(df_to_upload)\n",
    "        if total_rows == 0:\n",
    "            print(f\"  -> {company_name} 数据为空，跳过保存。\")\n",
    "            continue\n",
    "\n",
    "        # 1. 根据总行数和 chunk_size 计算需要拆分的文件数量 (n)\n",
    "        num_chunks = math.ceil(total_rows / chunk_size)\n",
    "\n",
    "        # 2. 循环生成每个分片文件\n",
    "        for i in range(num_chunks):\n",
    "            start_row = i * chunk_size\n",
    "            end_row = start_row + chunk_size\n",
    "            chunk_df = df_to_upload.iloc[start_row:end_row]\n",
    "\n",
    "            # 文件名从 1 开始计数, 例如: 圆通1.xlsx, 圆通2.xlsx ...\n",
    "            output_path = target_dir / f\"{company_name}{i + 1}.xlsx\"\n",
    "            chunk_df.to_excel(output_path, index=False)\n",
    "\n",
    "        # 3. 打印最终结果\n",
    "        print(f\"  -> {company_name} 数据已拆分为 {num_chunks} 个文件。\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"【第一部分完成】数据准备完毕！\")\n",
    "    # 使用传入的 target_dir 变量显示路径，更具通用性\n",
    "    print(f\"请前往文件夹: \\n{target_dir.resolve()}\")\n",
    "    print(\n",
    "        \"将里面的所有Excel文件手动上传到“猪猪快递云”网站，\\n\\n最后将所有下载结果放入“/报告数据/输入/猪猪云下载数据/”文件夹中。\"\n",
    "    )\n",
    "    print(\"=\" * 100)\n",
    "    # 定义需要排除的公司列表\n",
    "\n",
    "\n",
    "companies_to_exclude_from_upload = [\"顺丰\", \"中通\"]\n",
    "prepare_data_for_upload(\n",
    "    anjian_data_path, upload_split_path, companies_to_exclude_from_upload\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a381f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 3: 合并猪猪云下载数据，生成猪猪云合并数据\n",
    "# --------------------------------------------------\n",
    "companies = [\n",
    "    \"邮政\",\n",
    "    \"EMS\",\n",
    "    \"中通\",\n",
    "    \"京东\",\n",
    "    \"圆通\",\n",
    "    \"德邦\",\n",
    "    \"极兔\",\n",
    "    \"申通\",\n",
    "    \"韵达\",\n",
    "    \"顺丰\",\n",
    "]\n",
    "companies_lower = [comp.lower() for comp in companies]\n",
    "\n",
    "company_files = {}\n",
    "\n",
    "for file_path in zhuzhuyun_download_path.iterdir():\n",
    "    if file_path.is_file() and file_path.suffix == \".xlsx\":\n",
    "        filename_stem_lower = file_path.stem.lower()  # 获取不带扩展名的文件名并转为小写\n",
    "        company_name_found = False\n",
    "        found_company = None\n",
    "        for i, company_lower in enumerate(companies_lower):\n",
    "            if company_lower in filename_stem_lower:\n",
    "                found_company = companies[i]\n",
    "                company_name_found = True\n",
    "                break\n",
    "\n",
    "        if company_name_found:\n",
    "            if found_company not in company_files:\n",
    "                company_files[found_company] = []\n",
    "            company_files[found_company].append(file_path)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  警告: 文件 '{file_path.name}' 未匹配到任何已知公司名，将跳过此文件。\"\n",
    "            )\n",
    "\n",
    "print(\"\\n开始处理各公司文件...\")\n",
    "\n",
    "for company_name, files_list in company_files.items():\n",
    "    print(f\"\\n正在合并 {company_name} 的文件 ({len(files_list)} 个)...\")\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for file_path in files_list:\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"  已读取: {file_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  读取文件失败: {file_path.name}, 错误: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        output_filename = f\"{company_name}.xlsx\"\n",
    "        output_file_path = zhuzhuyun_merge_path / output_filename\n",
    "\n",
    "        # 1. 检查 '快递单号' 列是否存在\n",
    "        if \"快递单号\" in merged_df.columns:\n",
    "            # 记录原始行数\n",
    "            rows_before_dedup = len(merged_df)\n",
    "            # 2. 基于'快递单号'列删除重复项，保留第一个出现的\n",
    "            merged_df.drop_duplicates(subset=[\"快递单号\"], keep=\"first\", inplace=True)\n",
    "\n",
    "            # 记录删除后的行数\n",
    "            rows_after_dedup = len(merged_df)\n",
    "\n",
    "            # 3. 计算并打印移除了多少重复值\n",
    "            num_duplicates_removed = rows_before_dedup - rows_after_dedup\n",
    "            if num_duplicates_removed > 0:\n",
    "                print(f\"  已基于'快递单号'移除 {num_duplicates_removed} 个重复值。\")\n",
    "            else:\n",
    "                print(f\"  未发现'快递单号'的重复值。\")\n",
    "        else:\n",
    "            print(f\"  警告: 合并后的数据中未找到 '快递单号' 列，无法执行去重操作。\")\n",
    "\n",
    "        # 4. 保存处理后的数据\n",
    "        try:\n",
    "            merged_df.to_excel(output_file_path, index=False)\n",
    "            print(\n",
    "                f\"  成功保存合并数据到: {output_file_path.name} (共 {len(merged_df)} 行)\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  保存文件失败: {output_file_path.name}, 错误: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"  没有成功读取 {company_name} 的任何文件，跳过合并。\")\n",
    "\n",
    "print(\"\\n所有公司文件合并完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36892b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####-------- logistics数据提取原则-------------#####\n",
    "# 1. “到达分拣中心时间”本质是“到达寄件城市分拣中心时间”\n",
    "#### 寄达城市（即收件城市）分拣中心时间以提取到的第一个分拣中心为准\n",
    "#### 寄出城市（即寄件城市）分拣时间以提取到的最后一个分拣中心为准\n",
    "# 2. “签收时间”分为3种情况，\n",
    "#### 1）上门送件，以上门的时间为签收时间；\n",
    "#### 2）放在取件网点，顾客之后自己取走，以放在取件网点的时间为签收时间；\n",
    "#### 3）放在快递柜、丰巢等，顾客之后自己取走，以放在快递柜、丰巢的时间为签收时间。\n",
    "# 3. “转运中心”中只有省份名称，没有城市名称的，认为该转运中心就在该省的省会，如果相应的寄出/寄达城市就是该省的省会，则认为快件仍在该寄出地/寄达地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8892e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通用库导入完成。\n",
      "项目文件夹结构设置/检查完毕。\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.0: logistics数据提取-通用库导入与项目结构设置\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"通用库导入完成。\")\n",
    "\n",
    "# --- 项目路径设置 ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "# 确保文件夹存在\n",
    "for p in [anjian_data_path, zhuzhuyun_merge_path, pycharm_input_path]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"项目文件夹结构设置/检查完毕。\")\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff47a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 EMS ---\n",
      "  -> 使用 'parse_logistics_events_ems_ultimate' 解析器通过 .apply() 运行...\n",
      "✅ EMS 处理完成，耗时 229.98 秒。文件已保存至: EMS_logistics_data_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.1: EMS logistics数据提取（定稿版）\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 EMS ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: EMS ---\n",
    "COMPANY_PROFILES_EMS = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "    },\n",
    "    \"EMS\": {\n",
    "        \"collect\": r\"已收取快件|已收寄|已收取邮件|揽收|物流项目组\",\n",
    "        \"delivery\": r\"已安排派送|正在派送中|已派送(?!至)|为您派件|为您派送\",\n",
    "        \"leave_p1\": r\"已乘机|已搭乘邮航专机\",\n",
    "        \"leave_hard\": r\"离开\",\n",
    "        \"leave_soft\": r\"准备发出|已发出\",\n",
    "        \"center_p1\": r\"邮区中心|航空枢纽|处理中心|网路中心|航空中心\",\n",
    "        \"center_p2\": r\"包件车间|集散中心|集散点|航站|快件处理车间|快件处理中心\",\n",
    "        \"exclude\": r\"揽投部|邮政支局|直投中心|揽收部|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|到达【拼多多中转仓】|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点|已暂存至\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收|家门口签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "        \"sign_ignore\": r\"完成取件\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: EMS (终极版) ---\n",
    "def parse_logistics_events_ems_ultimate(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    time_cols = [\n",
    "        \"揽收时间_zzy\",\n",
    "        \"离开寄件城市时间_zzy\",\n",
    "        \"到达收件城市时间_zzy\",\n",
    "        \"派送时间_zzy\",\n",
    "        \"签收时间_zzy\",\n",
    "        \"到达分拣中心时间_zzy\",\n",
    "        \"离开收件城市分拣中心时间_zzy\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave_p1, kw_leave_hard, kw_leave_soft = (\n",
    "        profile.get(\"leave_p1\"),\n",
    "        profile.get(\"leave_hard\"),\n",
    "        profile.get(\"leave_soft\"),\n",
    "    )\n",
    "    kw_arrive, kw_exclude = profile.get(\"arrive\"), profile.get(\"exclude\", \"\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_center_p1, kw_center_p2 = profile.get(\"center_p1\"), profile.get(\"center_p2\")\n",
    "    kw_all_centers = \"|\".join(filter(None, [kw_center_p1, kw_center_p2]))\n",
    "\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str):\n",
    "            return None\n",
    "        aliases = location_maps[\"city_alias_map\"].get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        if province:\n",
    "            aliases.append(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "\n",
    "        # <<< MODIFIED >>>: 增加更严格的过滤和类型转换，防止NaN或数字类型导致错误\n",
    "        clean_aliases = {str(a) for a in aliases if pd.notna(a) and a != \"\"}\n",
    "        return \"|\".join(map(re.escape, clean_aliases))\n",
    "\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "    for event in all_events:\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"派送时间_zzy\"])\n",
    "            and kw_delivery\n",
    "            and re.search(kw_delivery, event[\"line\"])\n",
    "        ):\n",
    "            extracted_times[\"派送时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "    if kw_sign_p1:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p1, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_p2:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p2, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_fallback:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_fallback, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "\n",
    "    if pd.notna(extracted_times[\"揽收时间_zzy\"]) and sender_pattern:\n",
    "        true_leave_kws_parts = [kw_leave_p1, kw_leave_hard]\n",
    "        true_leave_kws = \"|\".join(filter(None, true_leave_kws_parts))\n",
    "        last_true_leave_event = None\n",
    "        if true_leave_kws:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]:\n",
    "                    if re.search(true_leave_kws, event[\"line\"]) and re.search(\n",
    "                        sender_pattern, event[\"line\"]\n",
    "                    ):\n",
    "                        last_true_leave_event = event\n",
    "        if last_true_leave_event:\n",
    "            extracted_times[\"离开寄件城市时间_zzy\"] = last_true_leave_event[\"dt\"]\n",
    "\n",
    "        last_sender_center_arrival = None\n",
    "        for event in all_events:\n",
    "            if event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]:\n",
    "                if (\n",
    "                    pd.notna(extracted_times[\"离开寄件城市时间_zzy\"])\n",
    "                    and event[\"dt\"] >= extracted_times[\"离开寄件城市时间_zzy\"]\n",
    "                ):\n",
    "                    break\n",
    "                if (\n",
    "                    kw_all_centers\n",
    "                    and re.search(kw_all_centers, event[\"line\"])\n",
    "                    and kw_arrive\n",
    "                    and re.search(kw_arrive, event[\"line\"])\n",
    "                    and re.search(sender_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    if not (kw_exclude and re.search(kw_exclude, event[\"line\"])):\n",
    "                        last_sender_center_arrival = event\n",
    "\n",
    "        if last_sender_center_arrival:\n",
    "            extracted_times[\"到达分拣中心时间_zzy\"] = last_sender_center_arrival[\"dt\"]\n",
    "\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "    if dest_pattern:\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zzy\"]):\n",
    "            for event in all_events:\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "        start_time_for_dest_search = extracted_times.get(\"到达收件城市时间_zzy\", pd.NaT)\n",
    "        if pd.isna(start_time_for_dest_search) and pd.notna(\n",
    "            extracted_times[\"揽收时间_zzy\"]\n",
    "        ):\n",
    "            start_time_for_dest_search = extracted_times[\"揽收时间_zzy\"] + pd.Timedelta(\n",
    "                hours=12\n",
    "            )\n",
    "        if pd.notna(start_time_for_dest_search) and kw_all_centers:\n",
    "            if kw_leave_hard:\n",
    "                for event in all_events:\n",
    "                    if event[\"dt\"] > start_time_for_dest_search:\n",
    "                        if (\n",
    "                            re.search(kw_all_centers, event[\"line\"])\n",
    "                            and re.search(kw_leave_hard, event[\"line\"])\n",
    "                            and re.search(dest_pattern, event[\"line\"])\n",
    "                        ):\n",
    "                            if kw_exclude and re.search(kw_exclude, event[\"line\"]):\n",
    "                                continue\n",
    "                            extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\n",
    "                                \"dt\"\n",
    "                            ]\n",
    "                            break\n",
    "            if (\n",
    "                pd.isna(extracted_times[\"离开收件城市分拣中心时间_zzy\"])\n",
    "                and kw_leave_soft\n",
    "            ):\n",
    "                for event in all_events:\n",
    "                    if event[\"dt\"] > start_time_for_dest_search:\n",
    "                        if (\n",
    "                            re.search(kw_all_centers, event[\"line\"])\n",
    "                            and re.search(kw_leave_soft, event[\"line\"])\n",
    "                            and re.search(dest_pattern, event[\"line\"])\n",
    "                        ):\n",
    "                            if kw_exclude and re.search(kw_exclude, event[\"line\"]):\n",
    "                                continue\n",
    "                            extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\n",
    "                                \"dt\"\n",
    "                            ]\n",
    "                            break\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: EMS ---\n",
    "def process_ems_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file):\n",
    "    company_name = \"EMS\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": \"EMS.xlsx\",\n",
    "        \"internal_name\": \"EMS\",\n",
    "        \"anjian_map_key\": \"EMS\",\n",
    "        \"parser\": parse_logistics_events_ems_ultimate,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(\n",
    "                base_data_file,\n",
    "                sheet_name=\"city_hierarchy\",\n",
    "                dtype={\"Province\": str, \"City\": str, \"District\": str},\n",
    "            )\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    all_anjian_files = [\n",
    "        f for f in anjian_dir.glob(\"*.xlsx\") if not f.name.startswith(\"~$\")\n",
    "    ]\n",
    "    if not all_anjian_files:\n",
    "        print(\"[ERROR] 安监数据文件夹中未找到有效的Excel文件！\")\n",
    "        return\n",
    "    all_anjian_df = pd.concat(\n",
    "        [\n",
    "            pd.read_excel(f, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str})\n",
    "            for f in all_anjian_files\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "    anjian_to_internal_map = {config[\"anjian_map_key\"]: \"EMS\"}\n",
    "    all_anjian_df[\"企业\"] = (\n",
    "        all_anjian_df[\"企业\"].map(anjian_to_internal_map).fillna(all_anjian_df[\"企业\"])\n",
    "    )\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_EMS[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_EMS.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "    time_cols_to_map = [\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "    ]\n",
    "    for col in time_cols_to_map:\n",
    "        df_final[col] = df_final.get(f\"{col}_zzy\", pd.NaT)\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: EMS ---\n",
    "process_ems_data(\n",
    "    zhuzhuyun_merge_path, anjian_data_path, pycharm_input_path, base_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 邮政 ---\n",
      "  -> 使用 'parse_logistics_events_postal_perfected' 解析器通过 .apply() 运行...\n",
      "✅ 邮政 处理完成，耗时 203.61 秒。文件已保存至: 邮政_logistics_data_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.2: 邮政logistics数据提取（定稿版）\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 邮政 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 邮政 ---\n",
    "COMPANY_PROFILES_POSTAL = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "    },\n",
    "    \"邮政国内小包\": {\n",
    "        \"collect\": r\"已收取快件|已收寄|已收取邮件|揽收|物流项目组\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派件|已安排派送\",\n",
    "        \"leave_p1\": r\"已乘机|已搭乘邮航专机\",\n",
    "        \"leave_hard\": r\"离开\",\n",
    "        \"leave_soft\": r\"准备发出|已发出|发往\",\n",
    "        \"leave\": r\"离开|已发出|发往|准备发出\",\n",
    "        \"center_p1\": r\"邮区中心|航空枢纽|处理中心|网路中心|航空中心\",\n",
    "        \"center_p2\": r\"包件车间|集散中心|集散点|航站|快件处理车间|快件处理班\",\n",
    "        \"exclude\": r\"揽投部|邮政支局|直投中心|揽收部|营销中心|直投点\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|到达【拼多多中转仓】|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点|已暂存至\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收|家门口签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "        \"sign_ignore\": r\"完成取件\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 邮政 (终极完美版) ---\n",
    "def parse_logistics_events_postal_perfected(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    time_cols = [\n",
    "        \"揽收时间_zzy\",\n",
    "        \"离开寄件城市时间_zzy\",\n",
    "        \"到达收件城市时间_zzy\",\n",
    "        \"派送时间_zzy\",\n",
    "        \"签收时间_zzy\",\n",
    "        \"到达分拣中心时间_zzy\",\n",
    "        \"离开收件城市分拣中心时间_zzy\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave_p1, kw_leave_hard, kw_leave_soft, kw_leave = (\n",
    "        profile.get(\"leave_p1\"),\n",
    "        profile.get(\"leave_hard\"),\n",
    "        profile.get(\"leave_soft\"),\n",
    "        profile.get(\"leave\"),\n",
    "    )\n",
    "    kw_arrive, kw_exclude = profile.get(\"arrive\"), profile.get(\"exclude\", \"\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_center_p1, kw_center_p2 = profile.get(\"center_p1\"), profile.get(\"center_p2\")\n",
    "    kw_all_centers = \"|\".join(filter(None, [kw_center_p1, kw_center_p2]))\n",
    "\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_patterns(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str):\n",
    "            return None, None\n",
    "        aliases = location_maps[\"city_alias_map\"].get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        city_pattern = \"|\".join(\n",
    "            map(re.escape, {str(a) for a in aliases if pd.notna(a) and a != \"\"})\n",
    "        )\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        province_pattern = None\n",
    "        if province:\n",
    "            province_clean = province.replace(\"省\", \"\").replace(\"市\", \"\")\n",
    "            if province_clean and not any(\n",
    "                alias in province_clean\n",
    "                for alias in {\n",
    "                    a.replace(\"市\", \"\") for a in aliases if pd.notna(a) and a != \"\"\n",
    "                }\n",
    "            ):\n",
    "                province_pattern = re.escape(province_clean)\n",
    "        return city_pattern, province_pattern\n",
    "\n",
    "    for event in all_events:\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"揽收时间_zzy\"])\n",
    "            and kw_collect\n",
    "            and re.search(kw_collect, event[\"line\"])\n",
    "        ):\n",
    "            extracted_times[\"揽收时间_zzy\"] = event[\"dt\"]\n",
    "    for event in all_events:\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"派送时间_zzy\"])\n",
    "            and kw_delivery\n",
    "            and re.search(kw_delivery, event[\"line\"])\n",
    "        ):\n",
    "            extracted_times[\"派送时间_zzy\"] = event[\"dt\"]\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_p1:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p1, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_p2:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p2, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_fallback:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_fallback, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "\n",
    "    sender_city_pattern, sender_province_pattern = get_location_patterns(\n",
    "        sender_city_key, location_maps\n",
    "    )\n",
    "    if pd.notna(extracted_times[\"揽收时间_zzy\"]):\n",
    "        sender_candidates = []\n",
    "        if sender_city_pattern:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > extracted_times[\"揽收时间_zzy\"] and re.search(\n",
    "                    sender_city_pattern, event[\"line\"]\n",
    "                ):\n",
    "                    sender_candidates.append(event)\n",
    "        if not sender_candidates and sender_province_pattern:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > extracted_times[\"揽收时间_zzy\"] and re.search(\n",
    "                    sender_province_pattern, event[\"line\"]\n",
    "                ):\n",
    "                    sender_candidates.append(event)\n",
    "\n",
    "        last_center_arrival, last_center_leave = None, None\n",
    "        true_leave_kws = \"|\".join(filter(None, [kw_leave_p1, kw_leave]))\n",
    "        for event in sender_candidates:\n",
    "            if (\n",
    "                kw_all_centers\n",
    "                and re.search(kw_all_centers, event[\"line\"])\n",
    "                and not (kw_exclude and re.search(kw_exclude, event[\"line\"]))\n",
    "            ):\n",
    "                if kw_arrive and re.search(kw_arrive, event[\"line\"]):\n",
    "                    last_center_arrival = event\n",
    "                if true_leave_kws and re.search(true_leave_kws, event[\"line\"]):\n",
    "                    last_center_leave = event\n",
    "            elif kw_leave_p1 and re.search(kw_leave_p1, event[\"line\"]):\n",
    "                last_center_leave = event\n",
    "\n",
    "        if last_center_arrival:\n",
    "            extracted_times[\"到达分拣中心时间_zzy\"] = last_center_arrival[\"dt\"]\n",
    "        if last_center_leave:\n",
    "            extracted_times[\"离开寄件城市时间_zzy\"] = last_center_leave[\"dt\"]\n",
    "\n",
    "    dest_city_pattern, dest_province_pattern = get_location_patterns(\n",
    "        dest_city_key, location_maps\n",
    "    )\n",
    "    if dest_city_pattern:\n",
    "        for event in all_events:\n",
    "            if (\n",
    "                pd.isna(extracted_times[\"到达收件城市时间_zzy\"])\n",
    "                and kw_all_centers\n",
    "                and re.search(kw_all_centers, event[\"line\"])\n",
    "                and not (kw_exclude and re.search(kw_exclude, event[\"line\"]))\n",
    "                and kw_arrive\n",
    "                and re.search(kw_arrive, event[\"line\"])\n",
    "                and re.search(dest_city_pattern, event[\"line\"])\n",
    "            ):\n",
    "                extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"到达收件城市时间_zzy\"]) and dest_province_pattern:\n",
    "        for event in all_events:\n",
    "            if (\n",
    "                pd.isna(extracted_times[\"到达收件城市时间_zzy\"])\n",
    "                and kw_all_centers\n",
    "                and re.search(kw_all_centers, event[\"line\"])\n",
    "                and not (kw_exclude and re.search(kw_exclude, event[\"line\"]))\n",
    "                and kw_arrive\n",
    "                and re.search(kw_arrive, event[\"line\"])\n",
    "                and re.search(dest_province_pattern, event[\"line\"])\n",
    "            ):\n",
    "                extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "\n",
    "    start_time = extracted_times.get(\"到达收件城市时间_zzy\", pd.NaT)\n",
    "    if pd.isna(start_time) and pd.notna(extracted_times[\"揽收时间_zzy\"]):\n",
    "        start_time = extracted_times[\"揽收时间_zzy\"]\n",
    "\n",
    "    if pd.notna(start_time):\n",
    "        if kw_leave_hard:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > start_time:\n",
    "                    if (\n",
    "                        dest_city_pattern\n",
    "                        and re.search(dest_city_pattern, event[\"line\"])\n",
    "                        or (\n",
    "                            dest_province_pattern\n",
    "                            and re.search(dest_province_pattern, event[\"line\"])\n",
    "                        )\n",
    "                    ):\n",
    "                        if (\n",
    "                            kw_all_centers\n",
    "                            and re.search(kw_all_centers, event[\"line\"])\n",
    "                            and not (\n",
    "                                kw_exclude and re.search(kw_exclude, event[\"line\"])\n",
    "                            )\n",
    "                        ):\n",
    "                            if re.search(kw_leave_hard, event[\"line\"]):\n",
    "                                extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\n",
    "                                    \"dt\"\n",
    "                                ]\n",
    "                                break\n",
    "        if pd.isna(extracted_times[\"离开收件城市分拣中心时间_zzy\"]) and kw_leave_soft:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > start_time:\n",
    "                    if (\n",
    "                        dest_city_pattern\n",
    "                        and re.search(dest_city_pattern, event[\"line\"])\n",
    "                        or (\n",
    "                            dest_province_pattern\n",
    "                            and re.search(dest_province_pattern, event[\"line\"])\n",
    "                        )\n",
    "                    ):\n",
    "                        if (\n",
    "                            kw_all_centers\n",
    "                            and re.search(kw_all_centers, event[\"line\"])\n",
    "                            and not (\n",
    "                                kw_exclude and re.search(kw_exclude, event[\"line\"])\n",
    "                            )\n",
    "                        ):\n",
    "                            if re.search(kw_leave_soft, event[\"line\"]):\n",
    "                                extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\n",
    "                                    \"dt\"\n",
    "                                ]\n",
    "                                break\n",
    "\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 邮政 ---\n",
    "def process_postal_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file):\n",
    "    company_name = \"邮政\"\n",
    "    company_start_time = perf_counter()\n",
    "    COMPANY_CONFIG_MAP = {\n",
    "        \"EMS\": {\"internal_name\": \"EMS\", \"anjian_map_key\": \"EMS\"},\n",
    "        \"邮政\": {\"internal_name\": \"邮政国内小包\", \"anjian_map_key\": \"ZGYZ\"},\n",
    "        \"京东\": {\"internal_name\": \"京东\", \"anjian_map_key\": \"JBD\"},\n",
    "        \"圆通\": {\"internal_name\": \"圆通\", \"anjian_map_key\": \"YTO\"},\n",
    "        \"申通\": {\"internal_name\": \"申通\", \"anjian_map_key\": \"STO\"},\n",
    "        \"韵达\": {\"internal_name\": \"韵达\", \"anjian_map_key\": \"YUNDA\"},\n",
    "        \"极兔\": {\"internal_name\": \"极兔\", \"anjian_map_key\": \"JT\"},\n",
    "        \"德邦\": {\"internal_name\": \"德邦\", \"anjian_map_key\": \"DEPPON\"},\n",
    "    }\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": \"邮政.xlsx\",\n",
    "        \"internal_name\": \"邮政国内小包\",\n",
    "        \"parser\": parse_logistics_events_postal_perfected,\n",
    "    }\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            # <<< MODIFIED >>>: 强制指定地理信息列为字符串\n",
    "            df_hierarchy = pd.read_excel(\n",
    "                base_data_file,\n",
    "                sheet_name=\"city_hierarchy\",\n",
    "                dtype={\"Province\": str, \"City\": str, \"District\": str},\n",
    "            )\n",
    "            # <<< MODIFIED >>>: 放宽数据清洗条件\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                # <<< MODIFIED >>>: 增加对NaN值的过滤\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [\n",
    "                    a for a in aliases if pd.notna(a) and a != \"\"\n",
    "                ]\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    all_anjian_files = [\n",
    "        f for f in anjian_dir.glob(\"*.xlsx\") if not f.name.startswith(\"~$\")\n",
    "    ]\n",
    "    if not all_anjian_files:\n",
    "        print(\"[ERROR] 安监数据文件夹中未找到有效的Excel文件！\")\n",
    "        return\n",
    "    all_anjian_df = pd.concat(\n",
    "        [\n",
    "            pd.read_excel(f, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str})\n",
    "            for f in all_anjian_files\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "    anjian_to_internal_map = {\n",
    "        v[\"anjian_map_key\"]: v[\"internal_name\"]\n",
    "        for k, v in COMPANY_CONFIG_MAP.items()\n",
    "        if \"anjian_map_key\" in v\n",
    "    }\n",
    "    all_anjian_df[\"企业\"] = (\n",
    "        all_anjian_df[\"企业\"].map(anjian_to_internal_map).fillna(all_anjian_df[\"企业\"])\n",
    "    )\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_company_zhu[\"企业\"] = config[\"internal_name\"]\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_POSTAL[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_POSTAL.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "    time_cols_to_map = [\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "    ]\n",
    "    for col in time_cols_to_map:\n",
    "        df_final[col] = df_final.get(f\"{col}_zzy\", pd.NaT)\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 邮政 ---\n",
    "process_postal_data(\n",
    "    zhuzhuyun_merge_path, anjian_data_path, pycharm_input_path, base_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe4632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 京东 ---\n",
      "  -> 已删除 3082 行缺少'完整物流信息'的数据。\n",
      "  -> 使用 'parse_logistics_events_jd' 解析器通过 .apply() 运行...\n",
      "✅ 京东 处理完成，耗时 185.99 秒。文件已保存至: 京东_logistics_data_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.3: 京东logistics数据提取（定稿版）\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 京东 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 京东 ---\n",
    "COMPANY_PROFILES_JD = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件|揽收完成\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心|分拣中心|接货仓\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心|项目营业点|校园服务站|接驳点\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点|已送达至|已送至|校园服务站|快递柜|云柜|丰巢柜|便民驿站\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收|已送达至|已由.*?代收|已由.*?签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收|已送达\",\n",
    "        \"sign_ignore\": r\"完成取件\",\n",
    "    },\n",
    "    \"京东\": {},\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 京东 (基于新原则重构) ---\n",
    "def parse_logistics_events_jd(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    time_cols = [\n",
    "        \"揽收时间_zzy\",\n",
    "        \"离开寄件城市时间_zzy\",\n",
    "        \"到达收件城市时间_zzy\",\n",
    "        \"派送时间_zzy\",\n",
    "        \"签收时间_zzy\",\n",
    "        \"到达分拣中心时间_zzy\",\n",
    "        \"离开收件城市分拣中心时间_zzy\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "\n",
    "    kw = {\n",
    "        key: profile.get(key)\n",
    "        for key in [\n",
    "            \"collect\",\n",
    "            \"delivery\",\n",
    "            \"leave\",\n",
    "            \"arrive\",\n",
    "            \"center\",\n",
    "            \"exclude\",\n",
    "            \"sign_p1\",\n",
    "            \"sign_p2\",\n",
    "            \"sign_fallback\",\n",
    "            \"sign_ignore\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            location_in_brackets = re.search(r\"【(.*?)】\", line)\n",
    "            is_center = False\n",
    "            location_name = \"\"\n",
    "            if location_in_brackets:\n",
    "                location_name = location_in_brackets.group(1)\n",
    "                if kw[\"center\"] and re.search(kw[\"center\"], location_name):\n",
    "                    if not (kw[\"exclude\"] and re.search(kw[\"exclude\"], location_name)):\n",
    "                        is_center = True\n",
    "\n",
    "            all_events.append(\n",
    "                {\n",
    "                    \"dt\": pd.to_datetime(match.group(1), errors=\"coerce\"),\n",
    "                    \"line\": line,\n",
    "                    \"is_center\": is_center,\n",
    "                    \"location\": location_name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    all_events = sorted(\n",
    "        [e for e in all_events if pd.notna(e[\"dt\"])], key=lambda x: x[\"dt\"]\n",
    "    )\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str):\n",
    "            return None\n",
    "        aliases = set(\n",
    "            location_maps[\"city_alias_map\"].get(city_key, [city_key.replace(\"市\", \"\")])\n",
    "        )\n",
    "\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        if province:\n",
    "            aliases.add(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "            if location_maps.get(\"province_capital_map\", {}).get(province) == city_key:\n",
    "                aliases.add(province.replace(\"省\", \"\"))\n",
    "\n",
    "        return \"|\".join(map(re.escape, {a for a in aliases if a}))\n",
    "\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "\n",
    "    # 1. 揽收时间\n",
    "    for event in all_events:\n",
    "        if kw[\"collect\"] and re.search(kw[\"collect\"], event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "    if pd.isna(extracted_times[\"揽收时间_zzy\"]):\n",
    "        extracted_times[\"揽收时间_zzy\"] = all_events[0][\"dt\"]\n",
    "\n",
    "    # 2. 派送时间\n",
    "    for event in all_events:\n",
    "        if kw[\"delivery\"] and re.search(kw[\"delivery\"], event[\"line\"]):\n",
    "            extracted_times[\"派送时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "\n",
    "    # 3. 签收时间\n",
    "    for p_key in [\"sign_p1\", \"sign_p2\", \"sign_fallback\"]:\n",
    "        if pd.notna(extracted_times[\"签收时间_zzy\"]):\n",
    "            break\n",
    "        if kw[p_key]:\n",
    "            for event in reversed(all_events):\n",
    "                if kw[\"sign_ignore\"] and re.search(kw[\"sign_ignore\"], event[\"line\"]):\n",
    "                    continue\n",
    "                if re.search(kw[p_key], event[\"line\"]):\n",
    "                    extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "    # 4. 始发地相关时间\n",
    "    if pd.notna(extracted_times[\"揽收时间_zzy\"]) and sender_pattern:\n",
    "        origin_center_events = [\n",
    "            e\n",
    "            for e in all_events\n",
    "            if e[\"dt\"] > extracted_times[\"揽收时间_zzy\"]\n",
    "            and e[\"is_center\"]\n",
    "            and re.search(sender_pattern, e[\"location\"])\n",
    "        ]\n",
    "\n",
    "        for event in reversed(origin_center_events):\n",
    "            if kw[\"leave\"] and re.search(kw[\"leave\"], event[\"line\"]):\n",
    "                match = re.search(r\"发往【(.*?)】\", event[\"line\"])\n",
    "                if match:\n",
    "                    destination_of_leave = match.group(1)\n",
    "                    if not re.search(sender_pattern, destination_of_leave):\n",
    "                        extracted_times[\"离开寄件城市时间_zzy\"] = event[\"dt\"]\n",
    "\n",
    "                        location_match = re.search(r\"【(.*?)】\", event[\"line\"])\n",
    "                        if location_match:\n",
    "                            location_name = re.escape(\n",
    "                                location_match.group(1).split(\"】\")[0]\n",
    "                            )\n",
    "                            for arr_event in origin_center_events:\n",
    "                                if (\n",
    "                                    arr_event[\"dt\"] <= event[\"dt\"]\n",
    "                                    and re.search(location_name, arr_event[\"line\"])\n",
    "                                    and re.search(kw[\"arrive\"], arr_event[\"line\"])\n",
    "                                ):\n",
    "                                    extracted_times[\"到达分拣中心时间_zzy\"] = arr_event[\n",
    "                                        \"dt\"\n",
    "                                    ]\n",
    "                                    break\n",
    "                        break\n",
    "\n",
    "    # --- 5. 目的地相关时间 (逻辑修正) ---\n",
    "    if dest_pattern:\n",
    "        # A. 到达收件城市时间 (采用两步查找法)\n",
    "        # 第一步：优先查找明确的“到达【城市】”记录\n",
    "        for event in all_events:\n",
    "            if (\n",
    "                not event[\"is_center\"]\n",
    "                and kw[\"arrive\"]\n",
    "                and re.search(kw[\"arrive\"], event[\"line\"])\n",
    "            ):\n",
    "                location_name = event[\"location\"].replace(\"市\", \"\")\n",
    "                if dest_city_key and location_name == dest_city_key.replace(\"市\", \"\"):\n",
    "                    extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "        # 第二步：如果没找到，则回退到查找第一个“到达城市内中心”的记录\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zzy\"]):\n",
    "            for event in all_events:\n",
    "                if (\n",
    "                    event[\"is_center\"]\n",
    "                    and kw[\"arrive\"]\n",
    "                    and re.search(kw[\"arrive\"], event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"location\"])\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "        # B. 离开收件城市分拣中心时间\n",
    "        if pd.notna(extracted_times[\"派送时间_zzy\"]):\n",
    "            dest_center_events = [\n",
    "                e\n",
    "                for e in all_events\n",
    "                if e[\"dt\"] < extracted_times[\"派送时间_zzy\"]\n",
    "                and e[\"is_center\"]\n",
    "                and re.search(dest_pattern, e[\"location\"])\n",
    "            ]\n",
    "            for event in dest_center_events:\n",
    "                if (\n",
    "                    pd.notna(extracted_times[\"到达收件城市时间_zzy\"])\n",
    "                    and event[\"dt\"] < extracted_times[\"到达收件城市时间_zzy\"]\n",
    "                ):\n",
    "                    continue  # 必须在到达收件城市之后\n",
    "                if kw[\"leave\"] and re.search(kw[\"leave\"], event[\"line\"]):\n",
    "                    extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 京东 ---\n",
    "def process_jd_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file):\n",
    "    company_name = \"京东\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": \"京东.xlsx\",\n",
    "        \"internal_name\": \"京东\",\n",
    "        \"anjian_map_key\": \"JBD\",\n",
    "        \"parser\": parse_logistics_events_jd,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "        \"province_capital_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            excel_file = pd.ExcelFile(base_data_file)\n",
    "            df_hierarchy = pd.read_excel(excel_file, sheet_name=\"city_hierarchy\")\n",
    "\n",
    "            df_hierarchy_no_na = df_hierarchy.dropna(subset=[\"Province\", \"City\"])\n",
    "            df_hierarchy_no_na[\"District\"] = df_hierarchy_no_na[\"District\"].fillna(\"\")\n",
    "            df_hierarchy_no_na[\"City_clean\"] = df_hierarchy_no_na[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy_no_na[\"District_clean\"] = df_hierarchy_no_na[\n",
    "                \"District\"\n",
    "            ].str.replace(\"市\", \"\", regex=False)\n",
    "            for city, group in df_hierarchy_no_na.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "\n",
    "            city_to_province = df_hierarchy_no_na.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1].copy()\n",
    "                df_capitals = df_capitals.drop_duplicates(subset=[\"Province\"])\n",
    "                location_maps[\"province_capital_map\"] = df_capitals.set_index(\n",
    "                    \"Province\"\n",
    "                )[\"City\"].to_dict()\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "    all_anjian_df = pd.concat(\n",
    "        [pd.read_excel(f, dtype={\"单号\": str}) for f in anjian_dir.glob(\"*.xlsx\")],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "    anjian_to_internal_map = {config[\"anjian_map_key\"]: config[\"internal_name\"]}\n",
    "    all_anjian_df[\"企业\"] = (\n",
    "        all_anjian_df[\"企业\"].map(anjian_to_internal_map).fillna(all_anjian_df[\"企业\"])\n",
    "    )\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "\n",
    "    initial_rows = len(df_merged)\n",
    "    df_merged.dropna(subset=[\"完整物流信息\"], inplace=True)\n",
    "    rows_dropped = initial_rows - len(df_merged)\n",
    "    if rows_dropped > 0:\n",
    "        print(f\"  -> 已删除 {rows_dropped} 行缺少'完整物流信息'的数据。\")\n",
    "\n",
    "    base_profile = COMPANY_PROFILES_JD[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_JD.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "    time_cols_to_map = [\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "    ]\n",
    "    for col in time_cols_to_map:\n",
    "        df_final[col] = df_final.get(f\"{col}_zzy\", pd.NaT)\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 京东 ---\n",
    "process_jd_data(\n",
    "    zhuzhuyun_merge_path, anjian_data_path, pycharm_input_path, base_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fcb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 极兔 ---\n",
      "  -> 使用 'parse_logistics_events_jitu' 解析器通过 .apply() 运行...\n",
      "✅ 极兔 处理完成，耗时 90.55 秒。文件已保存至: 极兔_logistics_data_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.4: 极兔logistics数据提取（定稿版）\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 极兔 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "# 假设这些路径已在之前的单元格中定义\n",
    "# base_path = Path.cwd()\n",
    "# report_path = base_path / \"报告数据\"\n",
    "# input_path = report_path / \"输入\"\n",
    "# anjian_data_path = input_path / \"安监数据\"\n",
    "# base_data_path = input_path / \"basic_data.xlsx\"\n",
    "# temp_path = report_path / \"temp\"\n",
    "# zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "# pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"通用行应用提取器\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 极兔 (规则已完善) ---\n",
    "COMPANY_PROFILES_JITU = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心|分拣中心\",\n",
    "        \"sign_p1\": r\"已暂存至|已投至|已到站|自提柜|智能柜|菜鸟驿站|快递超市|代收点|存放.*(快递柜|驿站|自提点)\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收|已上门\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收|已投递\",\n",
    "    },\n",
    "    \"极兔\": {\n",
    "        \"center\": r\"转运中心|集散中心|集散点\",\n",
    "        \"center_exclude\": r\"公司|分部|服务部|网格仓|揽投部|营业部|经营分部|网点|集货点|营销中心|站点\",\n",
    "        \"dest_arrival_exclude\": r\"服务部|网点\",\n",
    "        \"sign_p1\": r\"已存放至.*?【|已送达.*?【|暂由.*?代为保管\",\n",
    "        \"sign_p2\": r\"已按址投递|已由本人签收|客户签收\",\n",
    "        \"sign_fallback\": r\"快件已签收|快件已按约定投递\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 极兔 (已应用最终修复) ---\n",
    "def parse_logistics_events_jitu(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    解析极兔的物流事件，并根据预设原则提取关键时间点。\n",
    "    \"\"\"\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "\n",
    "    time_cols = [\n",
    "        \"揽收时间_zzy\",\n",
    "        \"离开寄件城市时间_zzy\",\n",
    "        \"到达收件城市时间_zzy\",\n",
    "        \"派送时间_zzy\",\n",
    "        \"签收时间_zzy\",\n",
    "        \"到达分拣中心时间_zzy\",\n",
    "        \"离开收件城市分拣中心时间_zzy\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "\n",
    "    # 从profile中加载关键词\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave, kw_arrive = profile.get(\"leave\"), profile.get(\"arrive\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_all_centers = profile.get(\"center\")\n",
    "    kw_center_exclude = profile.get(\"center_exclude\", \"\")\n",
    "    kw_dest_arrival_exclude = profile.get(\"dest_arrival_exclude\", \"\")\n",
    "\n",
    "    # 预处理物流信息\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    # 辅助函数：生成城市别名匹配模式\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str) or pd.isna(city_key):\n",
    "            return None\n",
    "        aliases = location_maps.get(\"city_alias_map\", {}).get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        clean_aliases = {\n",
    "            str(a)\n",
    "            for a in aliases\n",
    "            if a is not None and pd.notna(a) and str(a).strip() != \"\"\n",
    "        }\n",
    "        if not clean_aliases:\n",
    "            return None\n",
    "        return \"|\".join(map(re.escape, clean_aliases))\n",
    "\n",
    "    # 提取揽收时间\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "\n",
    "    # 提取派送时间\n",
    "    for event in all_events:\n",
    "        if kw_delivery and re.search(kw_delivery, event[\"line\"]):\n",
    "            extracted_times[\"派送时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "\n",
    "    # 提取签收时间 (按优先级)\n",
    "    p1_event, p2_event, p3_event = None, None, None\n",
    "    for event in all_events:\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p1 and re.search(kw_sign_p1, event[\"line\"]):\n",
    "            p1_event = event\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p2 and re.search(kw_sign_p2, event[\"line\"]):\n",
    "            p2_event = event\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_fallback and re.search(kw_sign_fallback, event[\"line\"]):\n",
    "            p3_event = event\n",
    "            break\n",
    "    if p1_event:\n",
    "        extracted_times[\"签收时间_zzy\"] = p1_event[\"dt\"]\n",
    "    elif p2_event:\n",
    "        extracted_times[\"签收时间_zzy\"] = p2_event[\"dt\"]\n",
    "    elif p3_event:\n",
    "        extracted_times[\"签收时间_zzy\"] = p3_event[\"dt\"]\n",
    "\n",
    "    # 提取与城市和分拣中心相关的时间\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "\n",
    "    # 寄件城市相关时间\n",
    "    if pd.notna(extracted_times[\"揽收时间_zzy\"]) and sender_pattern and kw_all_centers:\n",
    "        last_sender_leave_event = None\n",
    "        last_sender_center_arrival = None\n",
    "\n",
    "        for event in all_events:\n",
    "            if event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]:\n",
    "                action_part = re.split(r\"；|已发往|发往\", event[\"line\"])[0]\n",
    "                is_real_center = re.search(kw_all_centers, action_part) and not (\n",
    "                    kw_center_exclude and re.search(kw_center_exclude, action_part)\n",
    "                )\n",
    "                if (\n",
    "                    kw_leave\n",
    "                    and re.search(kw_leave, action_part)\n",
    "                    and re.search(sender_pattern, action_part)\n",
    "                    and is_real_center\n",
    "                ):\n",
    "                    last_sender_leave_event = event\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, action_part)\n",
    "                    and re.search(sender_pattern, action_part)\n",
    "                    and is_real_center\n",
    "                ):\n",
    "                    last_sender_center_arrival = event\n",
    "\n",
    "        if last_sender_leave_event:\n",
    "            extracted_times[\"离开寄件城市时间_zzy\"] = last_sender_leave_event[\"dt\"]\n",
    "        if last_sender_center_arrival:\n",
    "            extracted_times[\"到达分拣中心时间_zzy\"] = last_sender_center_arrival[\"dt\"]\n",
    "\n",
    "    # 收件城市相关时间\n",
    "    if dest_pattern:\n",
    "        # 查找首次到达收件城市\n",
    "        for event in all_events:\n",
    "            action_part = re.split(r\"；|已发往|发往\", event[\"line\"])[0]\n",
    "            if (\n",
    "                kw_arrive\n",
    "                and re.search(kw_arrive, action_part)\n",
    "                and re.search(dest_pattern, action_part)\n",
    "            ):\n",
    "                if not (\n",
    "                    kw_dest_arrival_exclude\n",
    "                    and re.search(kw_dest_arrival_exclude, action_part)\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "        # 查找离开收件城市分拣中心时间\n",
    "        if pd.notna(extracted_times[\"派送时间_zzy\"]):\n",
    "            search_end_time = extracted_times[\"派送时间_zzy\"]\n",
    "            for event in reversed(all_events):\n",
    "                if event[\"dt\"] < search_end_time:\n",
    "                    action_part = re.split(r\"；|已发往|发往\", event[\"line\"])[0]\n",
    "                    is_real_center = re.search(kw_all_centers, action_part) and not (\n",
    "                        kw_center_exclude and re.search(kw_center_exclude, action_part)\n",
    "                    )\n",
    "                    if (\n",
    "                        kw_leave\n",
    "                        and re.search(kw_leave, action_part)\n",
    "                        and re.search(dest_pattern, action_part)\n",
    "                        and is_real_center\n",
    "                    ):\n",
    "                        extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                        break\n",
    "\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 极兔 (逻辑不变) ---\n",
    "def process_jitu_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file):\n",
    "    \"\"\"主流程函数，用于处理极兔快递的数据\"\"\"\n",
    "    company_name = \"极兔\"\n",
    "    company_start_time = perf_counter()\n",
    "\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": \"极兔.xlsx\",\n",
    "        \"internal_name\": \"极兔\",\n",
    "        \"parser\": parse_logistics_events_jitu,\n",
    "    }\n",
    "\n",
    "    COMPANY_CONFIG = {\"极兔\": {\"internal_name\": \"极兔\", \"anjian_map_key\": \"JT\"}}\n",
    "\n",
    "    location_maps = {}\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(\n",
    "                base_data_file,\n",
    "                sheet_name=\"city_hierarchy\",\n",
    "                dtype={\"Province\": str, \"City\": str, \"District\": str},\n",
    "            )\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "\n",
    "            # <<< 最终修正点 >>>: 调整District清洗逻辑，避免错误删除“乌市”中的“市”\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                r\"区|县|自治州\", \"\", regex=True\n",
    "            )\n",
    "\n",
    "            location_maps[\"city_alias_map\"] = {\n",
    "                city: [group.iloc[0][\"City_clean\"]]\n",
    "                + group[\"District_clean\"].unique().tolist()\n",
    "                for city, group in df_hierarchy.groupby(\"City\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    all_anjian_files = [\n",
    "        f for f in anjian_dir.glob(\"*.xlsx\") if not f.name.startswith(\"~$\")\n",
    "    ]\n",
    "    if not all_anjian_files:\n",
    "        print(\"[ERROR] 安监数据文件夹中未找到有效的Excel文件！\")\n",
    "        return\n",
    "    all_anjian_df = pd.concat(\n",
    "        [pd.read_excel(f, dtype={\"单号\": str}) for f in all_anjian_files],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    if \"快递公司\" in all_anjian_df.columns and \"企业\" in all_anjian_df.columns:\n",
    "        all_anjian_df[\"企业\"].fillna(all_anjian_df[\"快递公司\"], inplace=True)\n",
    "        all_anjian_df.drop(columns=[\"快递公司\"], inplace=True)\n",
    "    elif \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    anjian_to_internal_map = {\n",
    "        v[\"anjian_map_key\"]: v[\"internal_name\"]\n",
    "        for k, v in COMPANY_CONFIG.items()\n",
    "        if \"anjian_map_key\" in v\n",
    "    }\n",
    "    if \"企业\" in all_anjian_df.columns:\n",
    "        all_anjian_df[\"企业\"] = (\n",
    "            all_anjian_df[\"企业\"]\n",
    "            .str.strip()\n",
    "            .map(anjian_to_internal_map)\n",
    "            .fillna(all_anjian_df[\"企业\"].str.strip())\n",
    "        )\n",
    "        all_anjian_df[\"单号\"] = all_anjian_df[\"单号\"].str.strip()\n",
    "    else:\n",
    "        print(\"[ERROR] 安监数据中未找到 '企业' 或 '快递公司' 列，无法匹配寄送信息！\")\n",
    "        return\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "\n",
    "    company_file = zhuzhuyun_merge_path / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(columns={\"快递单号\": \"单号\"}, inplace=True)\n",
    "    df_company_zhu[\"企业\"] = config[\"internal_name\"]\n",
    "    df_company_zhu[\"单号\"] = df_company_zhu[\"单号\"].str.strip()\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "\n",
    "    base_profile = COMPANY_PROFILES_JITU[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_JITU.get(config[\"internal_name\"], {})\n",
    "    profile = base_profile.copy()\n",
    "    for key, value in company_specific_profile.items():\n",
    "        if (\n",
    "            key in profile\n",
    "            and value\n",
    "            and key not in [\"center_exclude\", \"dest_arrival_exclude\"]\n",
    "        ):\n",
    "            profile[key] = f\"{value}|{profile[key]}\"\n",
    "        elif value:\n",
    "            profile[key] = value\n",
    "\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    time_cols_to_map = [\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "    ]\n",
    "    for col in time_cols_to_map:\n",
    "        df_final[col] = df_final.get(f\"{col}_zzy\", pd.NaT)\n",
    "\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "\n",
    "    df_final[\"企业\"] = company_name\n",
    "\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "\n",
    "    output_file = pycharm_input_path / f\"{company_name}_logistics_data.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行示例 (请确保路径变量已正确设置) ---\n",
    "process_jitu_data(\n",
    "    zhuzhuyun_merge_path, anjian_data_path, pycharm_input_path, base_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2238bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 韵达 ---\n",
      "  -> 使用 'parse_logistics_events_yunda' 解析器通过 .apply() 运行...\n",
      "✅ 韵达 处理完成，耗时 92.30 秒。文件已保存至: 韵达_logistics_data_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.5: 韵达logistics数据提取(定稿版)\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 韵达 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 韵达 (引入分场景排除标准) ---\n",
    "COMPANY_PROFILES_YUNDA = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心\",\n",
    "        \"sign_p1\": r\"已暂存至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收|已投递\",\n",
    "    },\n",
    "    \"韵达\": {\n",
    "        \"center\": r\"分拨交付中心\",\n",
    "        # <<< 修正点1 >>>：定义两套排除标准\n",
    "        # 严格标准：用于识别真正的“中心”操作\n",
    "        \"center_exclude\": r\"公司|分部|服务部|网格仓|揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        # 宽松标准：用于识别“到达收件城市”这一首个落地动作\n",
    "        \"dest_arrival_exclude\": r\"服务部\",  # 根据案例，仅排除“服务部”\n",
    "        \"sign_p2\": r\"已送货上门签收|已由邮政派送签收\",\n",
    "        \"sign_fallback\": r\"快件已投递|快件已按址投递\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 韵达 (分场景调用不同排除标准) ---\n",
    "def parse_logistics_events_yunda(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    time_cols = [\n",
    "        \"揽收时间_zzy\",\n",
    "        \"离开寄件城市时间_zzy\",\n",
    "        \"到达收件城市时间_zzy\",\n",
    "        \"派送时间_zzy\",\n",
    "        \"签收时间_zzy\",\n",
    "        \"到达分拣中心时间_zzy\",\n",
    "        \"离开收件城市分拣中心时间_zzy\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave, kw_arrive = profile.get(\"leave\"), profile.get(\"arrive\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_all_centers = profile.get(\"center\")\n",
    "    # <<< 修正点2 >>>：准备两套排除关键词\n",
    "    kw_center_exclude = profile.get(\"center_exclude\", \"\")\n",
    "    kw_dest_arrival_exclude = profile.get(\"dest_arrival_exclude\", \"\")\n",
    "\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str) or pd.isna(city_key):\n",
    "            return None\n",
    "        aliases = location_maps[\"city_alias_map\"].get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        clean_aliases = {\n",
    "            str(a)\n",
    "            for a in aliases\n",
    "            if a is not None and pd.notna(a) and str(a).strip() != \"\"\n",
    "        }\n",
    "        if not clean_aliases:\n",
    "            return None\n",
    "        return \"|\".join(map(re.escape, clean_aliases))\n",
    "\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "\n",
    "    for event in all_events:\n",
    "        if kw_delivery and re.search(kw_delivery, event[\"line\"]):\n",
    "            extracted_times[\"派送时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "\n",
    "    p1_event, p2_event, p3_event = None, None, None\n",
    "    for event in all_events:\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p1 and re.search(kw_sign_p1, event[\"line\"]):\n",
    "            p1_event = event\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p2 and re.search(kw_sign_p2, event[\"line\"]):\n",
    "            p2_event = event\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_fallback and re.search(kw_sign_fallback, event[\"line\"]):\n",
    "            p3_event = event\n",
    "            break\n",
    "    if p1_event:\n",
    "        extracted_times[\"签收时间_zzy\"] = p1_event[\"dt\"]\n",
    "    elif p2_event:\n",
    "        extracted_times[\"签收时间_zzy\"] = p2_event[\"dt\"]\n",
    "    elif p3_event:\n",
    "        extracted_times[\"签收时间_zzy\"] = p3_event[\"dt\"]\n",
    "\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "\n",
    "    if pd.notna(extracted_times[\"揽收时间_zzy\"]) and sender_pattern and kw_all_centers:\n",
    "        last_sender_leave_event = None\n",
    "        for event in all_events:\n",
    "            if event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]:\n",
    "                action_part = re.split(r\"；|发往\", event[\"line\"])[0]\n",
    "                # 使用严格的 center_exclude\n",
    "                if (\n",
    "                    kw_leave\n",
    "                    and re.search(kw_leave, action_part)\n",
    "                    and re.search(kw_all_centers, action_part)\n",
    "                    and re.search(sender_pattern, action_part)\n",
    "                    and not (\n",
    "                        kw_center_exclude and re.search(kw_center_exclude, action_part)\n",
    "                    )\n",
    "                ):\n",
    "                    last_sender_leave_event = event\n",
    "\n",
    "        if last_sender_leave_event:\n",
    "            extracted_times[\"离开寄件城市时间_zzy\"] = last_sender_leave_event[\"dt\"]\n",
    "\n",
    "        last_sender_center_arrival = None\n",
    "        search_end_time = extracted_times.get(\n",
    "            \"离开寄件城市时间_zzy\", all_events[-1][\"dt\"] + pd.Timedelta(seconds=1)\n",
    "        )\n",
    "\n",
    "        for event in all_events:\n",
    "            if not (extracted_times[\"揽收时间_zzy\"] < event[\"dt\"] < search_end_time):\n",
    "                continue\n",
    "            action_part = re.split(r\"；|发往\", event[\"line\"])[0]\n",
    "            # 使用严格的 center_exclude\n",
    "            if (\n",
    "                kw_arrive\n",
    "                and re.search(kw_arrive, action_part)\n",
    "                and re.search(kw_all_centers, action_part)\n",
    "                and re.search(sender_pattern, action_part)\n",
    "                and not (\n",
    "                    kw_center_exclude and re.search(kw_center_exclude, action_part)\n",
    "                )\n",
    "            ):\n",
    "                last_sender_center_arrival = event\n",
    "\n",
    "        if last_sender_center_arrival:\n",
    "            extracted_times[\"到达分拣中心时间_zzy\"] = last_sender_center_arrival[\"dt\"]\n",
    "\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "    if dest_pattern:\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zzy\"]):\n",
    "            for event in all_events:\n",
    "                action_part = re.split(r\"；|发往\", event[\"line\"])[0]\n",
    "                # <<< 修正点3 >>>：使用宽松的 dest_arrival_exclude\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, action_part)\n",
    "                    and re.search(dest_pattern, action_part)\n",
    "                ):\n",
    "                    if not (\n",
    "                        kw_dest_arrival_exclude\n",
    "                        and re.search(kw_dest_arrival_exclude, action_part)\n",
    "                    ):\n",
    "                        extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                        break\n",
    "\n",
    "        if pd.notna(extracted_times[\"派送时间_zzy\"]):\n",
    "            for event in reversed(all_events):\n",
    "                if event[\"dt\"] < extracted_times[\"派送时间_zzy\"]:\n",
    "                    action_part = re.split(r\"；|发往\", event[\"line\"])[0]\n",
    "                    # 使用严格的 center_exclude\n",
    "                    if (\n",
    "                        kw_leave\n",
    "                        and re.search(kw_leave, action_part)\n",
    "                        and kw_all_centers\n",
    "                        and re.search(kw_all_centers, action_part)\n",
    "                        and re.search(dest_pattern, action_part)\n",
    "                        and not (\n",
    "                            kw_center_exclude\n",
    "                            and re.search(kw_center_exclude, action_part)\n",
    "                        )\n",
    "                    ):\n",
    "                        extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                        break\n",
    "\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 韵达 (整合所有修正) ---\n",
    "def process_yunda_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file):\n",
    "    company_name = \"韵达\"\n",
    "    company_start_time = perf_counter()\n",
    "    COMPANY_CONFIG = {\n",
    "        \"EMS\": {\"internal_name\": \"EMS\", \"anjian_map_key\": \"EMS\"},\n",
    "        \"邮政\": {\"internal_name\": \"邮政国内小包\", \"anjian_map_key\": \"ZGYZ\"},\n",
    "        \"京东\": {\"internal_name\": \"京东\", \"anjian_map_key\": \"JBD\"},\n",
    "        \"圆通\": {\"internal_name\": \"圆通\", \"anjian_map_key\": \"YTO\"},\n",
    "        \"申通\": {\"internal_name\": \"申通\", \"anjian_map_key\": \"STO\"},\n",
    "        \"韵达\": {\"internal_name\": \"韵达\", \"anjian_map_key\": \"YUNDA\"},\n",
    "        \"极兔\": {\"internal_name\": \"极兔\", \"anjian_map_key\": \"JT\"},\n",
    "        \"德邦\": {\"internal_name\": \"德邦\", \"anjian_map_key\": \"DEPPON\"},\n",
    "    }\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": \"韵达.xlsx\",\n",
    "        \"internal_name\": \"韵达\",\n",
    "        \"parser\": parse_logistics_events_yunda,\n",
    "    }\n",
    "\n",
    "    location_maps = {}\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(\n",
    "                base_data_file,\n",
    "                sheet_name=\"city_hierarchy\",\n",
    "                dtype={\"Province\": str, \"City\": str, \"District\": str},\n",
    "            )\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            location_maps[\"city_alias_map\"] = {\n",
    "                city: [group.iloc[0][\"City_clean\"]]\n",
    "                + group[\"District_clean\"].unique().tolist()\n",
    "                for city, group in df_hierarchy.groupby(\"City\")\n",
    "            }\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    all_anjian_files = [\n",
    "        f for f in anjian_dir.glob(\"*.xlsx\") if not f.name.startswith(\"~$\")\n",
    "    ]\n",
    "    if not all_anjian_files:\n",
    "        print(\"[ERROR] 安监数据文件夹中未找到有效的Excel文件！\")\n",
    "        return\n",
    "    all_anjian_df = pd.concat(\n",
    "        [pd.read_excel(f, dtype={\"单号\": str}) for f in all_anjian_files],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns and \"企业\" in all_anjian_df.columns:\n",
    "        all_anjian_df[\"企业\"].fillna(all_anjian_df[\"快递公司\"], inplace=True)\n",
    "        all_anjian_df.drop(columns=[\"快递公司\"], inplace=True)\n",
    "    elif \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    anjian_to_internal_map = {\n",
    "        v[\"anjian_map_key\"]: v[\"internal_name\"]\n",
    "        for k, v in COMPANY_CONFIG.items()\n",
    "        if \"anjian_map_key\" in v\n",
    "    }\n",
    "    if \"企业\" in all_anjian_df.columns:\n",
    "        all_anjian_df[\"企业\"] = (\n",
    "            all_anjian_df[\"企业\"]\n",
    "            .str.strip()\n",
    "            .map(anjian_to_internal_map)\n",
    "            .fillna(all_anjian_df[\"企业\"].str.strip())\n",
    "        )\n",
    "        all_anjian_df[\"单号\"] = all_anjian_df[\"单号\"].str.strip()\n",
    "    else:\n",
    "        print(\"[ERROR] 安监数据中未找到 '企业' 或 '快递公司' 列，无法匹配寄送信息！\")\n",
    "        return\n",
    "\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "\n",
    "    company_file = zhuzhuyun_merge_path / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(columns={\"快递单号\": \"单号\"}, inplace=True)\n",
    "    df_company_zhu[\"企业\"] = config[\"internal_name\"]\n",
    "    df_company_zhu[\"单号\"] = df_company_zhu[\"单号\"].str.strip()\n",
    "\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "\n",
    "    base_profile = COMPANY_PROFILES_YUNDA[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_YUNDA.get(config[\"internal_name\"], {})\n",
    "\n",
    "    profile = base_profile.copy()\n",
    "    for key, value in company_specific_profile.items():\n",
    "        if (\n",
    "            key in profile\n",
    "            and value\n",
    "            and key not in [\"center_exclude\", \"dest_arrival_exclude\"]\n",
    "        ):  # 这两个key不合并，直接使用公司专属的\n",
    "            profile[key] = f\"{value}|{profile[key]}\"\n",
    "        elif value:\n",
    "            profile[key] = value\n",
    "\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    time_cols_to_map = [\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "    ]\n",
    "    for col in time_cols_to_map:\n",
    "        df_final[col] = df_final.get(f\"{col}_zzy\", pd.NaT)\n",
    "\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "\n",
    "    df_final[\"企业\"] = company_name\n",
    "\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = pycharm_input_path / f\"{company_name}_logistics_data.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 韵达 ---\n",
    "process_yunda_data(\n",
    "    zhuzhuyun_merge_path, anjian_data_path, pycharm_input_path, base_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9339ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 德邦 ---\n",
      "  -> 使用 'parse_logistics_events_deppon_final_logic' 解析器通过 .apply() 运行...\n",
      "✅ 德邦 处理完成，耗时 89.17 秒。文件已保存至: 报告数据/temp/4_logistics数据/德邦_logistics_data_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.6: 德邦logistics数据提取（定稿版）\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 德邦 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保路径正确) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "# --- 全局预编译正则表达式 ---\n",
    "DATETIME_CAPTURE_PATTERN = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\")\n",
    "\n",
    "# --- 公司档案: 德邦 ---\n",
    "COMPANY_PROFILES_DEPPON = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往|航班起飞\",\n",
    "        \"arrive\": r\"到达|抵达|航班到达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|转运中心|运营区|枢纽中心|集散中心|分拨站|空运总调|机场运作部|运作部|集配站\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点|包裹已存放至\",  # P1: 放入驿站/柜子 - 动作\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",  # P2: 人员签收\n",
    "        \"sign_p3\": r\"DONT_MATCH_ANYTHING\",  # P3: 意向/通用确认 (德邦有专用)\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",  # Fallback\n",
    "    },\n",
    "    \"德邦\": {\n",
    "        \"sign_p1\": r\"快件已暂存至|包裹已存放至|集运仓签收|菜鸟驿站签收|水电表箱签收|丰巢柜|妈妈驿站\",\n",
    "        \"sign_p2\": r\"家门口签收|本人签收|代收|已由同事签收|收发室签收|前台签收|亲属签收|其他签收|门卫签收|物业签收|正常签收\",\n",
    "        \"sign_p3\": r\"经收货人同意，此件放置在\",\n",
    "        \"exclude\": r\"经营分部\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器辅助函数 ---\n",
    "def get_location_pattern(city_key: Optional[str], location_maps: Dict) -> Optional[str]:\n",
    "    if not city_key or not isinstance(city_key, str):\n",
    "        return None\n",
    "    aliases = set(\n",
    "        location_maps[\"city_alias_map\"].get(city_key, [city_key.replace(\"市\", \"\")])\n",
    "    )\n",
    "    if city_key in location_maps[\"capital_cities_set\"]:\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        if province:\n",
    "            aliases.add(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "    valid_aliases = {alias for alias in aliases if alias}\n",
    "    return \"|\".join(map(re.escape, valid_aliases))\n",
    "\n",
    "\n",
    "# --- 主解析器: 德邦 (最终修正版) ---\n",
    "def parse_logistics_events_deppon_final_logic(\n",
    "    row: pd.Series, profile: Dict, location_maps: Dict\n",
    ") -> pd.Series:\n",
    "    log_text, sender_city, dest_city = (\n",
    "        row[\"完整物流信息\"],\n",
    "        row[\"寄出城市\"],\n",
    "        row[\"寄达城市\"],\n",
    "    )\n",
    "    time_cols = [\n",
    "        \"揽收时间_zzy\",\n",
    "        \"离开寄件城市时间_zzy\",\n",
    "        \"到达收件城市时间_zzy\",\n",
    "        \"派送时间_zzy\",\n",
    "        \"签收时间_zzy\",\n",
    "        \"到达分拣中心时间_zzy\",\n",
    "        \"离开收件城市分拣中心时间_zzy\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = DATETIME_CAPTURE_PATTERN.search(line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    # --- 基础时间提取 ---\n",
    "    collect_event = next(\n",
    "        (e for e in all_events if re.search(profile.get(\"collect\"), e[\"line\"])), None\n",
    "    )\n",
    "    if collect_event:\n",
    "        extracted_times[\"揽收时间_zzy\"] = collect_event[\"dt\"]\n",
    "\n",
    "    delivery_event = next(\n",
    "        (e for e in all_events if re.search(profile.get(\"delivery\"), e[\"line\"])),\n",
    "        None,\n",
    "    )\n",
    "    if delivery_event:\n",
    "        extracted_times[\"派送时间_zzy\"] = delivery_event[\"dt\"]\n",
    "\n",
    "    # --- 签收时间（多级优先级处理）---\n",
    "    sign_event = None\n",
    "    patterns_to_check = [\"sign_p1\", \"sign_p2\", \"sign_p3\", \"sign_fallback\"]\n",
    "    for pattern_key in patterns_to_check:\n",
    "        pattern = profile.get(pattern_key)\n",
    "        if pattern:\n",
    "            sign_event = next(\n",
    "                (e for e in reversed(all_events) if re.search(pattern, e[\"line\"])),\n",
    "                None,\n",
    "            )\n",
    "        if sign_event:\n",
    "            break\n",
    "\n",
    "    if sign_event:\n",
    "        extracted_times[\"签收时间_zzy\"] = sign_event[\"dt\"]\n",
    "\n",
    "    # --- 准备地理位置和关键字 ---\n",
    "    sender_pattern = get_location_pattern(sender_city, location_maps)\n",
    "    dest_pattern = get_location_pattern(dest_city, location_maps)\n",
    "    kw_center, kw_leave, kw_arrive, kw_exclude = (\n",
    "        profile.get(\"center\"),\n",
    "        profile.get(\"leave\"),\n",
    "        profile.get(\"arrive\"),\n",
    "        profile.get(\"exclude\", \"DONT_MATCH_ANYTHING\"),\n",
    "    )\n",
    "\n",
    "    # --- 寄件城市阶段处理 (Sender City Phase Processing) ---\n",
    "    if pd.notna(extracted_times[\"揽收时间_zzy\"]) and sender_pattern:\n",
    "        true_leave_event = None\n",
    "        for event in reversed(all_events):\n",
    "            if event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]:\n",
    "                if re.search(kw_leave, event[\"line\"]) and re.search(\n",
    "                    sender_pattern, event[\"line\"]\n",
    "                ):\n",
    "                    if not re.search(kw_exclude, event[\"line\"]):\n",
    "                        true_leave_event = event\n",
    "                        break\n",
    "        if true_leave_event:\n",
    "            extracted_times[\"离开寄件城市时间_zzy\"] = true_leave_event[\"dt\"]\n",
    "\n",
    "        window_end = true_leave_event[\"dt\"] if true_leave_event else pd.Timestamp.max\n",
    "\n",
    "        last_arrival_in_origin = None\n",
    "        for event in all_events:\n",
    "            if not (extracted_times[\"揽收时间_zzy\"] < event[\"dt\"] < window_end):\n",
    "                continue\n",
    "            if (\n",
    "                re.search(kw_arrive, event[\"line\"])\n",
    "                and not re.search(kw_leave, event[\"line\"])\n",
    "                and re.search(kw_center, event[\"line\"])\n",
    "                and re.search(sender_pattern, event[\"line\"])\n",
    "                and not re.search(kw_exclude, event[\"line\"])\n",
    "            ):\n",
    "                last_arrival_in_origin = event\n",
    "        if last_arrival_in_origin:\n",
    "            extracted_times[\"到达分拣中心时间_zzy\"] = last_arrival_in_origin[\"dt\"]\n",
    "\n",
    "    # --- 收件城市阶段处理 (Destination City Phase Processing) ---\n",
    "    if dest_pattern:\n",
    "        arrive_dest_event = next(\n",
    "            (\n",
    "                e\n",
    "                for e in all_events\n",
    "                if (\n",
    "                    re.search(kw_arrive, e[\"line\"])\n",
    "                    and re.search(dest_pattern, e[\"line\"])\n",
    "                    and not re.search(kw_leave, e[\"line\"])\n",
    "                )\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if arrive_dest_event:\n",
    "            extracted_times[\"到达收件城市时间_zzy\"] = arrive_dest_event[\"dt\"]\n",
    "\n",
    "    # *** 此处是核心逻辑修正: 查找离开收件城市中心的最后一个事件 ***\n",
    "    last_leave_dest_center = None  # 确保变量在任何情况下都存在\n",
    "    if pd.notna(extracted_times[\"派送时间_zzy\"]) and dest_pattern and kw_center:\n",
    "        leave_dest_center_pattern = re.compile(\n",
    "            f\"({kw_leave}).*?【(?P<location_name>.*?)】\"\n",
    "        )\n",
    "\n",
    "        start_window = extracted_times.get(\"到达收件城市时间_zzy\", pd.Timestamp.min)\n",
    "        end_window = extracted_times[\"派送时间_zzy\"]\n",
    "\n",
    "        for event in all_events:\n",
    "            if not (start_window < event[\"dt\"] < end_window):\n",
    "                continue\n",
    "\n",
    "            match = leave_dest_center_pattern.search(event[\"line\"])\n",
    "            if match:\n",
    "                location_name = match.group(\"location_name\")\n",
    "\n",
    "                if (\n",
    "                    re.search(dest_pattern, location_name)\n",
    "                    and re.search(kw_center, location_name)\n",
    "                    and not re.search(kw_exclude, location_name)\n",
    "                ):\n",
    "                    last_leave_dest_center = event\n",
    "\n",
    "    if last_leave_dest_center:\n",
    "        extracted_times[\"离开收件城市分拣中心时间_zzy\"] = last_leave_dest_center[\"dt\"]\n",
    "\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 主流程: 德邦 ---\n",
    "def process_deppon_data(\n",
    "    zhuzhuyun_dir: Path, anjian_dir: Path, output_dir: Path, base_data_file: Path\n",
    "):\n",
    "    company_name = \"德邦\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": \"德邦.xlsx\",\n",
    "        \"internal_name\": \"德邦\",\n",
    "        \"anjian_map_key\": \"DEPPON\",\n",
    "        \"parser\": parse_logistics_events_deppon_final_logic,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    try:\n",
    "        if base_data_file.exists():\n",
    "            df_hierarchy = pd.read_excel(base_data_file, sheet_name=\"city_hierarchy\")\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\", \"District\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                r\"市|区|县\", \"\", regex=True\n",
    "            )\n",
    "\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\").set_index(\"City\")[\n",
    "                \"Province\"\n",
    "            ]\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province.to_dict()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    try:\n",
    "        if base_data_file.exists():\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "    all_anjian_df = pd.concat(\n",
    "        [pd.read_excel(f, dtype={\"单号\": str}) for f in anjian_dir.glob(\"*.xlsx\")],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "    anjian_to_internal_map = {config[\"anjian_map_key\"]: config[\"internal_name\"]}\n",
    "    all_anjian_df[\"企业\"] = (\n",
    "        all_anjian_df[\"企业\"].map(anjian_to_internal_map).fillna(all_anjian_df[\"企业\"])\n",
    "    )\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "\n",
    "    base_profile = COMPANY_PROFILES_DEPPON[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_DEPPON.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    time_cols_to_map = [\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "    ]\n",
    "    for col in time_cols_to_map:\n",
    "        df_final[col] = df_final.get(f\"{col}_zzy\", pd.NaT)\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.relative_to(base_path)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行主流程 ---\n",
    "if __name__ == \"__main__\":\n",
    "    if all(\n",
    "        p.exists()\n",
    "        for p in [\n",
    "            zhuzhuyun_merge_path,\n",
    "            anjian_data_path,\n",
    "            pycharm_input_path,\n",
    "            base_data_path,\n",
    "        ]\n",
    "    ):\n",
    "        process_deppon_data(\n",
    "            zhuzhuyun_merge_path, anjian_data_path, pycharm_input_path, base_data_path\n",
    "        )\n",
    "    else:\n",
    "        print(\"[WARNING] 一个或多个所需的数据路径不存在，已跳过执行。请检查路径设置。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22793ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.7: 申通logistics数据提取\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eada2d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 申通 ---\n",
      "  -> 使用 'parse_logistics_events_sto' 解析器通过 .apply() 运行...\n",
      "✅ 申通 处理完成，耗时 89.94 秒。文件已保存至: 申通_logistics_data_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell: 申通 (STO) 专用代码 (最终版)\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 申通 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 申通 ---\n",
    "COMPANY_PROFILES_STO = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已经妥投|已送达|作废处理|将提供送货上门|即将为您安排送货上门|快件已到达\\[.*?驿站\\]|已抵达.*?公司|包裹已完成签收|已放入|已存放至|到达退货服务站点|被退回|超市|正在验收中|已送货上门|按地址投递|已放入.*?驿站|已投递|已抵达.*?服务点|已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "    },\n",
    "    \"申通\": {\"sign_p2\": r\"已由【.*?】签收|已签收|代收\"},\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 申通 (使用稳定的Postal版本) ---\n",
    "def parse_logistics_events_sto(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    time_cols = [\n",
    "        \"揽收时间_zzy\",\n",
    "        \"离开寄件城市时间_zzy\",\n",
    "        \"到达收件城市时间_zzy\",\n",
    "        \"派送时间_zzy\",\n",
    "        \"签收时间_zzy\",\n",
    "        \"到达分拣中心时间_zzy\",\n",
    "        \"离开收件城市分拣中心时间_zzy\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave, kw_arrive, kw_exclude = (\n",
    "        profile.get(\"leave\"),\n",
    "        profile.get(\"arrive\"),\n",
    "        profile.get(\"exclude\", \"\"),\n",
    "    )\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_center_p1, kw_center_p2 = profile.get(\"center_p1\"), profile.get(\"center_p2\")\n",
    "    kw_center = profile.get(\"center\")\n",
    "    if not kw_center_p1 and not kw_center_p2 and kw_center:\n",
    "        kw_center_p1 = kw_center\n",
    "    kw_all_centers = ((kw_center_p1 or \"\") + \"|\" + (kw_center_p2 or \"\")).strip(\"|\")\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str):\n",
    "            return None\n",
    "        aliases = location_maps[\"city_alias_map\"].get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        if city_key in location_maps[\"capital_cities_set\"]:\n",
    "            province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "            if province:\n",
    "                aliases.append(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "        return \"|\".join(map(re.escape, set(a for a in aliases if a)))\n",
    "\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "\n",
    "    for event in all_events:\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"派送时间_zzy\"])\n",
    "            and kw_delivery\n",
    "            and re.search(kw_delivery, event[\"line\"])\n",
    "        ):\n",
    "            extracted_times[\"派送时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "\n",
    "    if kw_sign_p1:\n",
    "        for event in reversed(all_events):\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p1, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_p2:\n",
    "        for event in reversed(all_events):\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p2, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_fallback:\n",
    "        for event in reversed(all_events):\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_fallback, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "\n",
    "    # --- 逻辑重构: 增加备用逻辑(Fallback) ---\n",
    "    if pd.notna(extracted_times[\"揽收时间_zzy\"]) and sender_pattern:\n",
    "        # 1a. 到达分拣中心时间 (主逻辑)\n",
    "        for event in all_events:\n",
    "            if (\n",
    "                event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]\n",
    "                and re.search(sender_pattern, event[\"line\"])\n",
    "                and kw_all_centers\n",
    "                and re.search(kw_all_centers, event[\"line\"])\n",
    "                and kw_arrive\n",
    "                and re.search(kw_arrive, event[\"line\"])\n",
    "            ):\n",
    "                extracted_times[\"到达分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "\n",
    "        # 1b. 到达分拣中心时间 (备用逻辑)\n",
    "        if pd.isna(extracted_times[\"到达分拣中心时间_zzy\"]):\n",
    "            for event in all_events:\n",
    "                if (\n",
    "                    event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]\n",
    "                    and re.search(sender_pattern, event[\"line\"])\n",
    "                    and kw_all_centers\n",
    "                    and re.search(kw_all_centers, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"到达分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "        # 2. 离开寄件城市时间 (主逻辑)\n",
    "        for event in reversed(all_events):\n",
    "            if (\n",
    "                event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]\n",
    "                and re.search(sender_pattern, event[\"line\"])\n",
    "                and kw_leave\n",
    "                and re.search(kw_leave, event[\"line\"])\n",
    "            ):\n",
    "                extracted_times[\"离开寄件城市时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "\n",
    "    if dest_pattern:\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zzy\"]):\n",
    "            for event in all_events:\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "    if pd.notna(extracted_times[\"派送时间_zzy\"]) and dest_pattern:\n",
    "        # 3a. 离开收件城市分拣中心时间 (主逻辑)\n",
    "        if kw_center_p1:\n",
    "            for event in reversed(all_events):\n",
    "                if (\n",
    "                    event[\"dt\"] < extracted_times[\"派送时间_zzy\"]\n",
    "                    and re.search(kw_center_p1, event[\"line\"])\n",
    "                    and kw_leave\n",
    "                    and re.search(kw_leave, event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "        if pd.isna(extracted_times[\"离开收件城市分拣中心时间_zzy\"]) and kw_center_p2:\n",
    "            for event in reversed(all_events):\n",
    "                if (\n",
    "                    event[\"dt\"] < extracted_times[\"派送时间_zzy\"]\n",
    "                    and re.search(kw_center_p2, event[\"line\"])\n",
    "                    and kw_leave\n",
    "                    and re.search(kw_leave, event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "        # 3b. 离开收件城市分拣中心时间 (备用逻辑)\n",
    "        if pd.isna(extracted_times[\"离开收件城市分拣中心时间_zzy\"]):\n",
    "            for event in reversed(all_events):\n",
    "                if (\n",
    "                    event[\"dt\"] < extracted_times[\"派送时间_zzy\"]\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                    and kw_all_centers\n",
    "                    and re.search(kw_all_centers, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 申通 ---\n",
    "def process_sto_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file):\n",
    "    company_name = \"申通\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": \"申通.xlsx\",\n",
    "        \"internal_name\": \"申通\",\n",
    "        \"anjian_map_key\": \"STO\",\n",
    "        \"parser\": parse_logistics_events_sto,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(base_data_file, sheet_name=\"city_hierarchy\")\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\", \"District\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "    all_anjian_df = pd.concat(\n",
    "        [pd.read_excel(f, dtype={\"单号\": str}) for f in anjian_dir.glob(\"*.xlsx\")],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "    anjian_to_internal_map = {config[\"anjian_map_key\"]: config[\"internal_name\"]}\n",
    "    all_anjian_df[\"企业\"] = (\n",
    "        all_anjian_df[\"企业\"].map(anjian_to_internal_map).fillna(all_anjian_df[\"企业\"])\n",
    "    )\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_STO[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_STO.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "    time_cols_to_map = [\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "    ]\n",
    "    for col in time_cols_to_map:\n",
    "        df_final[col] = df_final.get(f\"{col}_zzy\", pd.NaT)\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_TEST.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 申通 ---\n",
    "process_sto_data(\n",
    "    zhuzhuyun_merge_path, anjian_data_path, pycharm_input_path, base_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.8: 圆通logistics数据提取\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b302c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 圆通 ---\n",
      "  -> 使用 'parse_logistics_events_yto' 解析器通过 .apply() 运行...\n",
      "✅ 圆通 处理完成，耗时 94.29 秒。文件已保存至: 圆通_logistics_data_FINAL.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell: 圆通 (YTO) 专用代码 (最终版)\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 圆通 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 圆通 ---\n",
    "COMPANY_PROFILES_YTO = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "    },\n",
    "    \"圆通\": {\n",
    "        \"collect\": r\"已揽收\",\n",
    "        \"delivery\": r\"正在派件\",\n",
    "        \"leave\": r\"离开\",\n",
    "        \"arrive\": r\"已经到达\",\n",
    "        \"center\": r\"转运中心\",\n",
    "        \"exclude\": r\"\",\n",
    "        \"sign_p1\": r\"已到达\\[.*?\\]|已到达【.*?(仓|仓库|组|公司)】|已验收成功\",\n",
    "        \"sign_p2\": r\"已签收|签收人|收件人:家门口\",\n",
    "        \"sign_fallback\": r\"快件已投递\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 圆通 (使用稳定的Postal版本) ---\n",
    "def parse_logistics_events_yto(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    time_cols = [\n",
    "        \"揽收时间_zzy\",\n",
    "        \"离开寄件城市时间_zzy\",\n",
    "        \"到达收件城市时间_zzy\",\n",
    "        \"派送时间_zzy\",\n",
    "        \"签收时间_zzy\",\n",
    "        \"到达分拣中心时间_zzy\",\n",
    "        \"离开收件城市分拣中心时间_zzy\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave, kw_arrive, kw_exclude = (\n",
    "        profile.get(\"leave\"),\n",
    "        profile.get(\"arrive\"),\n",
    "        profile.get(\"exclude\", \"\"),\n",
    "    )\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_center_p1, kw_center_p2 = profile.get(\"center_p1\"), profile.get(\"center_p2\")\n",
    "    kw_center = profile.get(\"center\")\n",
    "    if not kw_center_p1 and not kw_center_p2 and kw_center:\n",
    "        kw_center_p1 = kw_center\n",
    "    kw_all_centers = ((kw_center_p1 or \"\") + \"|\" + (kw_center_p2 or \"\")).strip(\"|\")\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str):\n",
    "            return None\n",
    "        aliases = location_maps[\"city_alias_map\"].get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        if city_key in location_maps[\"capital_cities_set\"]:\n",
    "            province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "            if province:\n",
    "                aliases.append(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "        return \"|\".join(map(re.escape, set(a for a in aliases if a)))\n",
    "\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"派送时间_zzy\"])\n",
    "            and kw_delivery\n",
    "            and re.search(kw_delivery, event[\"line\"])\n",
    "        ):\n",
    "            extracted_times[\"派送时间_zzy\"] = event[\"dt\"]\n",
    "            break\n",
    "    if kw_sign_p1:\n",
    "        for event in reversed(all_events):\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p1, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_p2:\n",
    "        for event in reversed(all_events):\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p2, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zzy\"]) and kw_sign_fallback:\n",
    "        for event in reversed(all_events):\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_fallback, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zzy\"] = event[\"dt\"]\n",
    "                break\n",
    "    plan_a_success = False\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    if pd.notna(extracted_times[\"揽收时间_zzy\"]) and sender_pattern:\n",
    "        origin_center_events = [\n",
    "            e\n",
    "            for e in all_events\n",
    "            if e[\"dt\"] > extracted_times[\"揽收时间_zzy\"]\n",
    "            and kw_all_centers\n",
    "            and re.search(kw_all_centers, e[\"line\"])\n",
    "            and re.search(sender_pattern, e[\"line\"])\n",
    "        ]\n",
    "        if origin_center_events:\n",
    "            plan_a_success = True\n",
    "            last_origin_center_event = origin_center_events[-1]\n",
    "            if kw_leave and re.search(kw_leave, last_origin_center_event[\"line\"]):\n",
    "                extracted_times[\"离开寄件城市时间_zzy\"] = last_origin_center_event[\"dt\"]\n",
    "            location_match = re.search(r\"【(.*?)】\", last_origin_center_event[\"line\"])\n",
    "            if location_match:\n",
    "                location_name = re.escape(location_match.group(1))\n",
    "                for event in reversed(origin_center_events):\n",
    "                    if (\n",
    "                        event[\"dt\"] <= last_origin_center_event[\"dt\"]\n",
    "                        and re.search(location_name, event[\"line\"])\n",
    "                        and kw_arrive\n",
    "                        and re.search(kw_arrive, event[\"line\"])\n",
    "                    ):\n",
    "                        extracted_times[\"到达分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                        break\n",
    "    if not plan_a_success and pd.notna(extracted_times[\"揽收时间_zzy\"]):\n",
    "        first_center_arrival_event = None\n",
    "        for event in all_events:\n",
    "            if (\n",
    "                event[\"dt\"] > extracted_times[\"揽收时间_zzy\"]\n",
    "                and kw_all_centers\n",
    "                and re.search(kw_all_centers, event[\"line\"])\n",
    "                and kw_arrive\n",
    "                and re.search(kw_arrive, event[\"line\"])\n",
    "            ):\n",
    "                extracted_times[\"到达分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                first_center_arrival_event = event\n",
    "                break\n",
    "        if first_center_arrival_event:\n",
    "            location_match = re.search(r\"【(.*?)】\", first_center_arrival_event[\"line\"])\n",
    "            if location_match:\n",
    "                location_name = re.escape(location_match.group(1))\n",
    "                for event in all_events:\n",
    "                    if (\n",
    "                        event[\"dt\"] > first_center_arrival_event[\"dt\"]\n",
    "                        and re.search(location_name, event[\"line\"])\n",
    "                        and kw_leave\n",
    "                        and re.search(kw_leave, event[\"line\"])\n",
    "                    ):\n",
    "                        extracted_times[\"离开寄件城市时间_zzy\"] = event[\"dt\"]\n",
    "                        break\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "    if dest_pattern:\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zzy\"]):\n",
    "            for event in all_events:\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "    if pd.notna(extracted_times[\"派送时间_zzy\"]) and dest_pattern:\n",
    "        if kw_center_p1:\n",
    "            for event in reversed(all_events):\n",
    "                if (\n",
    "                    event[\"dt\"] < extracted_times[\"派送时间_zzy\"]\n",
    "                    and re.search(kw_center_p1, event[\"line\"])\n",
    "                    and kw_leave\n",
    "                    and re.search(kw_leave, event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "        if pd.isna(extracted_times[\"离开收件城市分拣中心时间_zzy\"]) and kw_center_p2:\n",
    "            for event in reversed(all_events):\n",
    "                if (\n",
    "                    event[\"dt\"] < extracted_times[\"派送时间_zzy\"]\n",
    "                    and re.search(kw_center_p2, event[\"line\"])\n",
    "                    and kw_leave\n",
    "                    and re.search(kw_leave, event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"离开收件城市分拣中心时间_zzy\"] = event[\"dt\"]\n",
    "                    break\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 圆通 ---\n",
    "def process_yto_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file):\n",
    "    company_name = \"圆通\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": \"圆通.xlsx\",\n",
    "        \"internal_name\": \"圆通\",\n",
    "        \"anjian_map_key\": \"YTO\",\n",
    "        \"parser\": parse_logistics_events_yto,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(base_data_file, sheet_name=\"city_hierarchy\")\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\", \"District\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "    all_anjian_df = pd.concat(\n",
    "        [pd.read_excel(f, dtype={\"单号\": str}) for f in anjian_dir.glob(\"*.xlsx\")],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "    anjian_to_internal_map = {config[\"anjian_map_key\"]: config[\"internal_name\"]}\n",
    "    all_anjian_df[\"企业\"] = (\n",
    "        all_anjian_df[\"企业\"].map(anjian_to_internal_map).fillna(all_anjian_df[\"企业\"])\n",
    "    )\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_YTO[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_YTO.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "    time_cols_to_map = [\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "    ]\n",
    "    for col in time_cols_to_map:\n",
    "        df_final[col] = df_final.get(f\"{col}_zzy\", pd.NaT)\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_FINAL.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 圆通 ---\n",
    "process_yto_data(\n",
    "    zhuzhuyun_merge_path, anjian_data_path, pycharm_input_path, base_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc3a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础数据文件应位于: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输入/basic_data.xlsx\n",
      "安监数据文件夹: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输入/安监数据\n",
      "物流明细文件夹: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/3_猪猪云合并数据\n",
      "计算结果将输出至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据\n",
      "----------------------------------------\n",
      "成功加载 297 个目标城市。\n",
      "\n",
      "正在为物流文件 [圆通.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月圆通抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 圆通 ---\n",
      "在 圆通.xlsx 和 2025年6月圆通抽样.xlsx 中共找到 82733 条匹配的运单。\n",
      "✔ 圆通 的中转数据计算完成，已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据/圆通_transit_data.xlsx\n",
      "\n",
      "正在为物流文件 [京东.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月京东抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 京东 ---\n",
      "在 京东.xlsx 和 2025年6月京东抽样.xlsx 中共找到 79688 条匹配的运单。\n",
      "✔ 京东 的中转数据计算完成，已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据/京东_transit_data.xlsx\n",
      "\n",
      "正在为物流文件 [申通.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月申通抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 申通 ---\n",
      "在 申通.xlsx 和 2025年6月申通抽样.xlsx 中共找到 83029 条匹配的运单。\n",
      "✔ 申通 的中转数据计算完成，已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据/申通_transit_data.xlsx\n",
      "\n",
      "正在为物流文件 [邮政.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月中国邮政抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 邮政 ---\n",
      "在 邮政.xlsx 和 2025年6月中国邮政抽样.xlsx 中共找到 81571 条匹配的运单。\n",
      "✔ 邮政 的中转数据计算完成，已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据/邮政_transit_data.xlsx\n",
      "\n",
      "正在为物流文件 [韵达.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月韵达抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 韵达 ---\n",
      "在 韵达.xlsx 和 2025年6月韵达抽样.xlsx 中共找到 81797 条匹配的运单。\n",
      "✔ 韵达 的中转数据计算完成，已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据/韵达_transit_data.xlsx\n",
      "\n",
      "正在为物流文件 [EMS.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月EMS抽样.xlsx]\n",
      "\n",
      "--- 正在处理: EMS ---\n",
      "在 EMS.xlsx 和 2025年6月EMS抽样.xlsx 中共找到 43390 条匹配的运单。\n",
      "✔ EMS 的中转数据计算完成，已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据/EMS_transit_data.xlsx\n",
      "\n",
      "正在为物流文件 [极兔.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月极兔抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 极兔 ---\n",
      "在 极兔.xlsx 和 2025年6月极兔抽样.xlsx 中共找到 81703 条匹配的运单。\n",
      "✔ 极兔 的中转数据计算完成，已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据/极兔_transit_data.xlsx\n",
      "\n",
      "正在为物流文件 [德邦.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月德邦抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 德邦 ---\n",
      "在 德邦.xlsx 和 2025年6月德邦抽样.xlsx 中共找到 78551 条匹配的运单。\n",
      "✔ 德邦 的中转数据计算完成，已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据/德邦_transit_data.xlsx\n",
      "\n",
      "--- 所有文件处理完毕！ ---\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 5: 中转数据生成\n",
    "# --------------------------------------------------\n",
    "# --- 1. 设置文件路径 ---\n",
    "# 输入路径\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_dir = report_path / \"输入\"\n",
    "anjian_dir = input_dir / \"安监数据\"\n",
    "# 中间过程文件路径\n",
    "temp_path = report_path / \"temp\"\n",
    "logistics_dir = temp_path / \"3_猪猪云合并数据\"\n",
    "output_dir = temp_path / \"5_中转数据\"\n",
    "basic_data_path = input_dir / \"basic_data.xlsx\"\n",
    "\n",
    "anjian_dir.mkdir(parents=True, exist_ok=True)\n",
    "logistics_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"基础数据文件应位于: {basic_data_path}\")\n",
    "print(f\"安监数据文件夹: {anjian_dir}\")\n",
    "print(f\"物流明细文件夹: {logistics_dir}\")\n",
    "print(f\"计算结果将输出至: {output_dir}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 2. 城市加载与匹配逻辑 ---\n",
    "def load_city_pattern(path_to_basic_data):\n",
    "    try:\n",
    "        if not path_to_basic_data.exists():\n",
    "            return None\n",
    "        df_city = pd.read_excel(\n",
    "            path_to_basic_data, sheet_name=\"city_names_complete_2025\", engine=\"openpyxl\"\n",
    "        )\n",
    "        if \"城市\" not in df_city.columns or \"行政级别\" not in df_city.columns:\n",
    "            return None\n",
    "        df_city[\"行政级别\"] = df_city[\"行政级别\"].str.strip()\n",
    "        ####################  !!! 请根据你的需要修改以下变量 !!!   #######################\n",
    "        target_levels = [\"地级市\", \"直辖市\", \"副省级城市\", \"省直辖县级市\"]\n",
    "        ####################  !!! 请根据你的需要修改以下变量 !!!   #######################\n",
    "        target_df = df_city[df_city[\"行政级别\"].isin(target_levels)].copy()\n",
    "        target_df[\"城市_clean\"] = target_df[\"城市\"].str.replace(\n",
    "            r\"(市|省|自治区|特别行政区)$\", \"\", regex=True\n",
    "        )\n",
    "        city_list = target_df[\"城市_clean\"].dropna().unique().tolist()\n",
    "        city_list.sort(key=len, reverse=True)\n",
    "        print(f\"成功加载 {len(city_list)} 个目标城市。\")\n",
    "        return re.compile(\"|\".join(city_list))\n",
    "    except Exception as e:\n",
    "        print(f\"加载和处理城市数据时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "BASE_CITY_PATTERN = load_city_pattern(basic_data_path)\n",
    "####################  !!! 请根据你的需要修改以下变量 !!!   #######################\n",
    "AMBIGUOUS_MAP = {\n",
    "    \"朝阳\": \"北京\",\n",
    "    \"湘潭\": \"天津\",\n",
    "}  # 可能产生歧义的字段硬编码（如：北京朝阳区 vs 朝阳市）\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "def find_all_valid_cities_in_text(text, pattern):\n",
    "    if not isinstance(text, str) or not pattern:\n",
    "        return []\n",
    "    cities_found = pattern.findall(text)\n",
    "    valid_cities = []\n",
    "    for city in cities_found:\n",
    "        is_noise = False\n",
    "        if city in AMBIGUOUS_MAP:\n",
    "            context_word = AMBIGUOUS_MAP[city]\n",
    "            if context_word in text:\n",
    "                is_noise = True\n",
    "        if not is_noise:\n",
    "            valid_cities.append(city + \"市\")\n",
    "    return list(dict.fromkeys(valid_cities))\n",
    "\n",
    "\n",
    "# --- “锚点切割”算法---\n",
    "def extract_transit_log_slice(log_entries, origin_city, dest_city):\n",
    "    \"\"\"\n",
    "    根据你的最终逻辑，对物流信息进行“手术式切割”。\n",
    "    log_entries: 已经按时间正序排列的物流记录列表 (最早的在index 0)。\n",
    "    \"\"\"\n",
    "    last_origin_anchor = -1\n",
    "    first_dest_anchor = -1\n",
    "\n",
    "    origin_city_clean = origin_city.replace(\"市\", \"\")\n",
    "    dest_city_clean = dest_city.replace(\"市\", \"\")\n",
    "\n",
    "    # 从头向后，找到最后一次提及“寄出城市”的记录\n",
    "    for i, entry in enumerate(log_entries):\n",
    "        if origin_city_clean in entry:\n",
    "            last_origin_anchor = i  # 持续更新，直到循环结束\n",
    "\n",
    "    # 从头向后，找到第一次提及“寄达城市”的记录\n",
    "    for i, entry in enumerate(log_entries):\n",
    "        if dest_city_clean in entry:\n",
    "            first_dest_anchor = i\n",
    "            break  # 找到第一个就停止\n",
    "\n",
    "    # 只有当两个锚点都找到，并且起点锚点在终点锚点之前，才算有效\n",
    "    if (\n",
    "        last_origin_anchor != -1\n",
    "        and first_dest_anchor != -1\n",
    "        and last_origin_anchor < first_dest_anchor\n",
    "    ):\n",
    "        # 返回这两个锚点之间的“纯净”中转部分\n",
    "        return log_entries[last_origin_anchor + 1 : first_dest_anchor]\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "# --- ---------------------------------------------------- ---\n",
    "\n",
    "\n",
    "# --- 3. 核心处理逻辑 (以“锚点切割”为核心) ---\n",
    "def process_company_data(logistics_path, anjian_path):\n",
    "    company_name = logistics_path.stem\n",
    "    rename_map = {\"邮政国内小包\": \"邮政\"}\n",
    "    company_name = rename_map.get(company_name, company_name)\n",
    "    print(f\"\\n--- 正在处理: {company_name} ---\")\n",
    "\n",
    "    try:\n",
    "        df_anjian = pd.read_excel(anjian_path)\n",
    "        df_logistics = pd.read_excel(logistics_path)\n",
    "\n",
    "        df_anjian = df_anjian[[\"单号\", \"寄出城市\", \"寄达城市\"]].dropna()\n",
    "        df_anjian[\"寄出城市\"] = (\n",
    "            df_anjian[\"寄出城市\"].str.replace(\"市\", \"\", regex=False) + \"市\"\n",
    "        )\n",
    "        df_anjian[\"寄达城市\"] = (\n",
    "            df_anjian[\"寄达城市\"].str.replace(\"市\", \"\", regex=False) + \"市\"\n",
    "        )\n",
    "        df_anjian[\"单号\"] = df_anjian[\"单号\"].astype(str)\n",
    "\n",
    "        df_logistics = df_logistics[[\"快递单号\", \"完整物流信息\"]].dropna()\n",
    "        df_logistics[\"快递单号\"] = df_logistics[\"快递单号\"].astype(str)\n",
    "\n",
    "        merged_df = pd.merge(\n",
    "            df_logistics, df_anjian, left_on=\"快递单号\", right_on=\"单号\", how=\"inner\"\n",
    "        )\n",
    "        print(\n",
    "            f\"在 {logistics_path.name} 和 {anjian_path.name} 中共找到 {len(merged_df)} 条匹配的运单。\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"处理公司 {company_name} 时发生错误: {e}\")\n",
    "        return\n",
    "\n",
    "    all_routes = []\n",
    "    for _, row in merged_df.iterrows():\n",
    "        origin_city_auth = row[\"寄出城市\"]\n",
    "        dest_city_auth = row[\"寄达城市\"]\n",
    "        full_log = row[\"完整物流信息\"]\n",
    "\n",
    "        log_entries = [entry.strip() for entry in full_log.split(\"\\n\") if entry.strip()]\n",
    "        log_entries.reverse()\n",
    "\n",
    "        transit_slice = extract_transit_log_slice(\n",
    "            log_entries, origin_city_auth, dest_city_auth\n",
    "        )\n",
    "\n",
    "        transit_cities = set()\n",
    "        for entry in transit_slice:\n",
    "            cities_in_entry = find_all_valid_cities_in_text(entry, BASE_CITY_PATTERN)\n",
    "            for city in cities_in_entry:\n",
    "                if city not in [origin_city_auth, dest_city_auth]:\n",
    "                    transit_cities.add(city)\n",
    "\n",
    "        all_routes.append(\n",
    "            {\n",
    "                \"出发城市\": origin_city_auth,\n",
    "                \"到达城市\": dest_city_auth,\n",
    "                \"中转城市列表\": sorted(list(transit_cities)),\n",
    "                \"中转次数\": len(transit_cities),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not all_routes:\n",
    "        print(f\"未能在 {company_name} 的数据中提取出任何有效的中转路径。\")\n",
    "        return\n",
    "\n",
    "    # --- 4 & 5. 聚合计算与保存 ---\n",
    "    routes_df = pd.DataFrame(all_routes)\n",
    "    agg_result = (\n",
    "        routes_df.groupby([\"出发城市\", \"到达城市\"])\n",
    "        .agg(\n",
    "            平均中转次数=(\"中转次数\", \"mean\"),\n",
    "            中转城市=(\n",
    "                \"中转城市列表\",\n",
    "                lambda s: sorted(list(set(c for sub in s for c in sub))),\n",
    "            ),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    agg_result[\"中转城市\"] = agg_result[\"中转城市\"].apply(lambda x: \",\".join(x))\n",
    "    agg_result[\"平均中转次数\"] = agg_result[\"平均中转次数\"].round(2)\n",
    "    agg_result = agg_result.sort_values(by=\"平均中转次数\", ascending=False).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    final_columns = [\"出发城市\", \"到达城市\", \"中转城市\", \"平均中转次数\"]\n",
    "    agg_result = agg_result[final_columns]\n",
    "\n",
    "    # --- 6. 保存结果 ---\n",
    "    output_filename = f\"{company_name}_transit_data.xlsx\"\n",
    "    output_path = output_dir / output_filename\n",
    "    agg_result.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "    print(f\"✔ {company_name} 的中转数据计算完成，已保存至: {output_path}\")\n",
    "\n",
    "\n",
    "# --- 7. 主程序入口 ---\n",
    "def main():\n",
    "    if not BASE_CITY_PATTERN:\n",
    "        print(\"由于城市列表未能加载，无法继续处理文件。\")\n",
    "        return\n",
    "\n",
    "    logistics_files = list(logistics_dir.glob(\"*.xlsx\")) + list(\n",
    "        logistics_dir.glob(\"*.xls\")\n",
    "    )\n",
    "    if not logistics_files:\n",
    "        print(f\"警告：在物流明细文件夹中未找到任何Excel文件！请放入: {logistics_dir}\")\n",
    "        return\n",
    "\n",
    "    for log_path in logistics_files:\n",
    "        company_name_stem = log_path.stem\n",
    "        print(f\"\\n正在为物流文件 [{log_path.name}] 寻找对应的安监数据...\")\n",
    "        search_pattern_xlsx = f\"*{company_name_stem}*.xlsx\"\n",
    "        search_pattern_xls = f\"*{company_name_stem}*.xls\"\n",
    "        matching_anjian_files = list(anjian_dir.glob(search_pattern_xlsx)) + list(\n",
    "            anjian_dir.glob(search_pattern_xls)\n",
    "        )\n",
    "        if not matching_anjian_files:\n",
    "            print(\n",
    "                f\"--> 警告：在安监文件夹中未找到任何包含 '{company_name_stem}' 的文件，跳过该公司。\"\n",
    "            )\n",
    "            continue\n",
    "        anjian_path = matching_anjian_files[0]\n",
    "        print(f\"--> 成功匹配: [{anjian_path.name}]\")\n",
    "        if len(matching_anjian_files) > 1:\n",
    "            print(\n",
    "                f\"    注意：找到多个匹配项，已自动选择第一个。匹配项列表: {[p.name for p in matching_anjian_files]}\"\n",
    "            )\n",
    "        process_company_data(log_path, anjian_path)\n",
    "\n",
    "    print(\"\\n--- 所有文件处理完毕！ ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################——————【Cell 6 补充】中转数据侦查工具——————#############################\n",
    "#  输出的中转数据中，觉得输出的“中转次数”或“中转城市“的输出不合理，使用该模块进行物流信息的筛查，寻找原始的”完整物流轨迹“，需修改下方标注的\n",
    "# 1） 目标公司文件名 2）目标出发城市 3） 目标到达城市 4） 你怀疑的、不合理的中转城市  四个参数\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 准备工作：确保所有函数和变量都已定义 ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "base_dir = Path.cwd()\n",
    "input_dir = base_dir / \"报告数据\" / \"输入\"\n",
    "data_dir = base_dir / \"报告数据\" / \"temp\" / \"3_猪猪云合并数据\"\n",
    "basic_data_path = input_dir / \"basic_data.xlsx\"\n",
    "\n",
    "\n",
    "def load_target_cities(path_to_basic_data):\n",
    "    try:\n",
    "        if not path_to_basic_data.exists():\n",
    "            return None\n",
    "        df_city = pd.read_excel(\n",
    "            path_to_basic_data, sheet_name=\"city_names_complete_2025\", engine=\"openpyxl\"\n",
    "        )\n",
    "        if \"城市\" not in df_city.columns or \"行政级别\" not in df_city.columns:\n",
    "            return None\n",
    "        df_city[\"行政级别\"] = df_city[\"行政级别\"].str.strip()\n",
    "        target_levels = [\"地级市\", \"直辖市\", \"副省级城市\", \"省直辖县级市\"]\n",
    "        target_df = df_city[df_city[\"行政级别\"].isin(target_levels)].copy()\n",
    "        target_df[\"城市_clean\"] = target_df[\"城市\"].str.replace(\n",
    "            r\"(市|省|自治区|特别行政区)$\", \"\", regex=True\n",
    "        )\n",
    "        city_list = target_df[\"城市_clean\"].dropna().unique().tolist()\n",
    "        city_list.sort(key=len, reverse=True)\n",
    "        pattern_list = [city + \"(?![\\\\s]?[区县])\" for city in city_list]\n",
    "        return re.compile(\"|\".join(pattern_list))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "TARGET_CITY_PATTERN = load_target_cities(basic_data_path)\n",
    "\n",
    "\n",
    "def find_all_cities(text, pattern):\n",
    "    if not isinstance(text, str) or not pattern:\n",
    "        return []\n",
    "    return [match + \"市\" for match in pattern.findall(text)]\n",
    "\n",
    "\n",
    "def find_first_city(text, pattern):\n",
    "    if not isinstance(text, str) or not pattern:\n",
    "        return None\n",
    "    match = pattern.search(text)\n",
    "    return match.group() + \"市\" if match else None\n",
    "\n",
    "\n",
    "def find_destination_city(log_entries, pattern):\n",
    "    for i in range(len(log_entries) - 1, 0, -1):\n",
    "        city = find_first_city(log_entries[i], pattern)\n",
    "        if city:\n",
    "            return city\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- 物流侦探主程序 ---\n",
    "def route_tracer():\n",
    "    print(\"--- 物流侦探工具已启动 ---\")\n",
    "\n",
    "    # --- 1. 设置追踪目标 ---\n",
    "    ################################  !!! 请根据你的需要修改以下变量 !!!   ################################################################\n",
    "    TARGET_COMPANY_FILE = \"德邦.xlsx\"  # 目标公司文件名\n",
    "    TARGET_ORIGIN = \"北京市\"  # 目标出发城市\n",
    "    TARGET_DEST = \"漯河市\"  # 目标到达城市\n",
    "    SUSPICIOUS_TRANSIT = \"南阳市\"  # 你怀疑的、不合理的中转城市\n",
    "    ###################################################################################################################################\n",
    "\n",
    "    if not TARGET_CITY_PATTERN:\n",
    "        print(\"错误：无法加载城市列表，追踪中止。\")\n",
    "        return\n",
    "\n",
    "    file_path = data_dir / TARGET_COMPANY_FILE\n",
    "    if not file_path.exists():\n",
    "        print(f\"错误：目标文件 '{TARGET_COMPANY_FILE}' 在 '{data_dir}' 中未找到。\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n正在分析文件: {TARGET_COMPANY_FILE}\")\n",
    "    print(\n",
    "        f\"追踪目标: 从 [{TARGET_ORIGIN}] 到 [{TARGET_DEST}]，且中转了 [{SUSPICIOUS_TRANSIT}] 的包裹\"\n",
    "    )\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "        df.dropna(subset=[\"完整物流信息\"], inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"读取或处理Excel文件时出错: {e}\")\n",
    "        return\n",
    "\n",
    "    found_count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        full_log = row.get(\"完整物流信息\", \"\")\n",
    "        if not isinstance(full_log, str):\n",
    "            continue\n",
    "\n",
    "        log_entries = [entry.strip() for entry in full_log.split(\"\\n\") if entry.strip()]\n",
    "        if len(log_entries) < 2:\n",
    "            continue\n",
    "        log_entries.reverse()\n",
    "\n",
    "        origin_city = find_first_city(log_entries[0], TARGET_CITY_PATTERN)\n",
    "        dest_city = find_destination_city(log_entries, TARGET_CITY_PATTERN)\n",
    "\n",
    "        # 检查是否是我们追踪的线路\n",
    "        if origin_city == TARGET_ORIGIN and dest_city == TARGET_DEST:\n",
    "            # 检查完整的物流信息中是否包含了可疑的中转城市\n",
    "            if SUSPICIOUS_TRANSIT.replace(\"市\", \"\") in full_log:\n",
    "                found_count += 1\n",
    "                waybill_no = row.get(\"快递单号\", \"未找到快递单号列\")\n",
    "                print(f\"\\n--- 找到匹配记录! ---\")\n",
    "                print(f\"  快递单号: {waybill_no}\")\n",
    "                print(f\"  原始表格行号: {index + 2}\")\n",
    "                print(\"【完整的物流信息】:\")\n",
    "                print(full_log)\n",
    "                print(\"-\" * 20)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    if found_count == 0:\n",
    "        print(\n",
    "            f\"在文件中未找到任何从 [{TARGET_ORIGIN}] 到 [{TARGET_DEST}]，且物流信息中包含 [{SUSPICIOUS_TRANSIT}] 的记录。\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"追踪完毕，共找到 {found_count} 条符合条件的记录。\")\n",
    "\n",
    "\n",
    "# --- 运行侦探工具 ---\n",
    "route_tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4099a584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "库导入完成。\n",
      "项目文件夹结构设置/检查完毕。\n",
      "输入数据(Logistics数据)应位于: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/temp/4_logistics数据\n",
      "分析报告将输出至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result\n",
      "\n",
      "正在从 'basic_data.xlsx' 加载 Top 30 城市列表...\n",
      "成功加载 30 个 Top 30 城市。\n",
      "\n",
      "找到 10 个待处理文件，开始分析...\n",
      "\n",
      "--- 正在处理文件: EMS_logistics_data.xlsx ---\n",
      "原始数据量: 41287 条\n",
      "  - 因核心时间戳缺失，删除了 247 行。\n",
      "  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\n",
      "    -> 因时限存在空值，删除了 645 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\n",
      "    -> 因时限值不符合业务范围，又删除了 1847 行。\n",
      "  - [小计] 时效筛选阶段共删除 2492 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/EMS_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 中通_logistics_data.xlsx ---\n",
      "原始数据量: 97785 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "  - 正在为中通/顺丰执行宽松筛选(第1步)：仅检查5个核心时限的空值...\n",
      "    -> 因时限存在空值，删除了 0 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 因时限值不符合业务范围，又删除了 23655 行。\n",
      "  - [小计] 时效筛选阶段共删除 23655 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "   -> 提示: 未找到对应的中转文件: 中通_transit_data.xlsx\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/中通_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 京东_logistics_data.xlsx ---\n",
      "原始数据量: 97370 条\n",
      "  - 因核心时间戳缺失，删除了 18053 行。\n",
      "  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\n",
      "    -> 因时限存在空值，删除了 3651 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\n",
      "    -> 因时限值不符合业务范围，又删除了 1518 行。\n",
      "  - [小计] 时效筛选阶段共删除 5169 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/京东_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 圆通_logistics_data.xlsx ---\n",
      "原始数据量: 90846 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\n",
      "    -> 因时限存在空值，删除了 133 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\n",
      "    -> 因时限值不符合业务范围，又删除了 22048 行。\n",
      "  - [小计] 时效筛选阶段共删除 22181 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/圆通_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 德邦_logistics_data.xlsx ---\n",
      "原始数据量: 91659 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\n",
      "    -> 因时限存在空值，删除了 54 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\n",
      "    -> 因时限值不符合业务范围，又删除了 35686 行。\n",
      "  - [小计] 时效筛选阶段共删除 35740 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/德邦_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 极兔_logistics_data.xlsx ---\n",
      "原始数据量: 94029 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\n",
      "    -> 因时限存在空值，删除了 8 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\n",
      "    -> 因时限值不符合业务范围，又删除了 12922 行。\n",
      "  - [小计] 时效筛选阶段共删除 12930 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/极兔_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 申通_logistics_data.xlsx ---\n",
      "原始数据量: 92418 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\n",
      "    -> 因时限存在空值，删除了 4 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\n",
      "    -> 因时限值不符合业务范围，又删除了 19276 行。\n",
      "  - [小计] 时效筛选阶段共删除 19280 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/申通_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 邮政_logistics_data.xlsx ---\n",
      "原始数据量: 95822 条\n",
      "  - 因核心时间戳缺失，删除了 4907 行。\n",
      "  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\n",
      "    -> 因时限存在空值，删除了 1718 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\n",
      "    -> 因时限值不符合业务范围，又删除了 1477 行。\n",
      "  - [小计] 时效筛选阶段共删除 3195 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/邮政_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 韵达_logistics_data.xlsx ---\n",
      "原始数据量: 88604 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\n",
      "    -> 因时限存在空值，删除了 3 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\n",
      "    -> 因时限值不符合业务范围，又删除了 23644 行。\n",
      "  - [小计] 时效筛选阶段共删除 23647 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/韵达_data_analysis_result.xlsx\n",
      "\n",
      "--- 正在处理文件: 顺丰_logistics_data.xlsx ---\n",
      "原始数据量: 25945 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "  - 正在为中通/顺丰执行宽松筛选(第1步)：仅检查5个核心时限的空值...\n",
      "    -> 因时限存在空值，删除了 0 行。\n",
      "  - 正在筛选(第2步)：应用业务逻辑范围条件...\n",
      "    -> 因时限值不符合业务范围，又删除了 515 行。\n",
      "  - [小计] 时效筛选阶段共删除 515 行。\n",
      "  - 正在计算'线路汇总数据'...\n",
      "  - 正在合并中转次数数据...\n",
      "   -> 提示: 未找到对应的中转文件: 顺丰_transit_data.xlsx\n",
      "分析完成, 结果已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis_202507/报告数据/输出/data_analysis_result/顺丰_data_analysis_result.xlsx\n",
      "\n",
      "\n",
      "所有文件处理完毕！总耗时: 241.49 秒。\n",
      "\n",
      "==================== 数据筛选流程汇总统计 ====================\n",
      "| 公司 (Company)   |   原始数据量 |   因时间戳缺失删除 |   因时限空值删除 |   因范围不符删除 |   **总计删除** |   最终保留 | **保留率**   |\n",
      "|:-----------------|-------------:|-------------------:|-----------------:|-----------------:|---------------:|-----------:|:-------------|\n",
      "| EMS              |        41287 |                247 |              645 |             1847 |           2739 |      38548 | 93.4%        |\n",
      "| 中通             |        97785 |                  0 |                0 |            23655 |          23655 |      74130 | 75.8%        |\n",
      "| 京东             |        97370 |              18053 |             3651 |             1518 |          23222 |      74148 | 76.2%        |\n",
      "| 圆通             |        90846 |                  0 |              133 |            22048 |          22181 |      68665 | 75.6%        |\n",
      "| 德邦             |        91659 |                  0 |               54 |            35686 |          35740 |      55919 | 61.0%        |\n",
      "| 极兔             |        94029 |                  0 |                8 |            12922 |          12930 |      81099 | 86.2%        |\n",
      "| 申通             |        92418 |                  0 |                4 |            19276 |          19280 |      73138 | 79.1%        |\n",
      "| 快包             |        95822 |               4907 |             1718 |             1477 |           8102 |      87720 | 91.5%        |\n",
      "| 韵达             |        88604 |                  0 |                3 |            23644 |          23647 |      64957 | 73.3%        |\n",
      "| 顺丰             |        25945 |                  0 |                0 |              515 |            515 |      25430 | 98.0%        |\n",
      "| **总计**         |       815765 |              23207 |             6216 |           142588 |         172011 |     643754 | 78.9%        |\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 7: 核心数据计算层 (含中转次数和汇总统计)\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy\")\n",
    "print(\"库导入完成。\")\n",
    "\n",
    "\n",
    "# --- 1. 运行模式开关 ---\n",
    "RUN_MODE = \"ALL\"\n",
    "TARGET_COMPANY = \"邮政\"\n",
    "\n",
    "# --- 3. 项目路径设置 ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_data_path = report_path / \"temp\" / \"4_logistics数据\"\n",
    "zhuzhuyun_data_path = report_path / \"temp\" / \"3_猪猪云合并数据\"\n",
    "transit_data_path = report_path / \"temp\" / \"5_中转数据\"\n",
    "output_path = report_path / \"输出\" / \"data_analysis_result\"\n",
    "basic_data_file = report_path / \"输入\" / \"basic_data.xlsx\"\n",
    "\n",
    "# (新增) 公司名到文件名的映射，以及反向映射\n",
    "COMPANY_TO_FILENAME_MAP = {\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"中通\": \"中通\",\n",
    "    \"京东\": \"京东\",\n",
    "    \"圆通\": \"圆通\",\n",
    "    \"德邦\": \"德邦\",\n",
    "    \"极兔\": \"极兔\",\n",
    "    \"申通\": \"申通\",\n",
    "    \"韵达\": \"韵达\",\n",
    "    \"顺丰\": \"顺丰\",\n",
    "    \"快包\": \"邮政\",\n",
    "}\n",
    "FILENAME_TO_COMPANY_MAP = {v: k for k, v in COMPANY_TO_FILENAME_MAP.items()}\n",
    "\n",
    "\n",
    "def load_top_cities(file_path: Path) -> set:\n",
    "    sheet_name = \"30_top_volume_city_2024\"\n",
    "    column_name = \"城市\"\n",
    "    print(f\"\\n正在从 '{file_path.name}' 加载 Top 30 城市列表...\")\n",
    "    try:\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"基础数据文件不存在: {file_path}\")\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"在Sheet '{sheet_name}' 中未找到名为 '{column_name}' 的列。\"\n",
    "            )\n",
    "        cities_set = set(df[column_name].dropna().astype(str).str.strip().tolist())\n",
    "        if not cities_set:\n",
    "            print(f\"警告: 从 '{file_path.name}' 中加载的城市列表为空。\")\n",
    "        else:\n",
    "            print(f\"成功加载 {len(cities_set)} 个 Top 30 城市。\")\n",
    "        return cities_set\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 加载Top 30城市列表失败！错误信息: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def generate_basic_metrics_df(\n",
    "    filtered_data: pd.DataFrame,\n",
    "    original_data_count: int,\n",
    "    top_30_cities: set,\n",
    "    total_dropped_count: int,\n",
    ") -> pd.DataFrame:\n",
    "    metrics_data, distance_metrics = [], []\n",
    "    total_count = filtered_data.shape[0]\n",
    "    top_30_data_all = filtered_data[\n",
    "        (filtered_data[\"寄出城市\"].isin(top_30_cities))\n",
    "        & (filtered_data[\"寄达城市\"].isin(top_30_cities))\n",
    "    ]\n",
    "    total_top_30_count = top_30_data_all.shape[0]\n",
    "    if not filtered_data.empty:\n",
    "        time_cols_map = {\n",
    "            \"全程时限\": \"全程时限\",\n",
    "            \"寄出地处理时限\": \"寄出地处理时限\",\n",
    "            \"运输时限\": \"运输时限\",\n",
    "            \"寄达地处理时限\": \"寄达地处理时限\",\n",
    "            \"投递时限\": \"投递时限\",\n",
    "        }\n",
    "        for name, col in time_cols_map.items():\n",
    "            metrics_data.append(\n",
    "                [\n",
    "                    name,\n",
    "                    filtered_data[col].mean(),\n",
    "                    filtered_data[col].max(),\n",
    "                    filtered_data[col].min(),\n",
    "                ]\n",
    "            )\n",
    "        metrics_data.append(\n",
    "            [\n",
    "                \"72小时准时率\",\n",
    "                (filtered_data[\"全程时限\"] <= 72).mean() if total_count > 0 else 0,\n",
    "                \"\",\n",
    "                \"\",\n",
    "            ]\n",
    "        )\n",
    "        metrics_data.append(\n",
    "            [\n",
    "                \"48小时准时率\",\n",
    "                (top_30_data_all[\"全程时限\"] <= 48).mean()\n",
    "                if total_top_30_count > 0\n",
    "                else 0,\n",
    "                \"\",\n",
    "                \"\",\n",
    "            ]\n",
    "        )\n",
    "        metrics_data.append(\n",
    "            [\n",
    "                f\"总数据量 {original_data_count}\\n总筛选掉数据量 {total_dropped_count}\\n最终有效数据量 {total_count}\\n业务量前30的城市间数据量 {total_top_30_count}\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "            ]\n",
    "        )\n",
    "        buckets = {\n",
    "            \"0-600\": filtered_data[filtered_data[\"公里\"] < 600],\n",
    "            \"600-1500\": filtered_data[\n",
    "                (filtered_data[\"公里\"] >= 600) & (filtered_data[\"公里\"] < 1500)\n",
    "            ],\n",
    "            \"1500-2500\": filtered_data[\n",
    "                (filtered_data[\"公里\"] >= 1500) & (filtered_data[\"公里\"] < 2500)\n",
    "            ],\n",
    "            \"2500以上\": filtered_data[filtered_data[\"公里\"] >= 2500],\n",
    "        }\n",
    "        for _, col_key in time_cols_map.items():\n",
    "            distance_metrics.append([buckets[b][col_key].mean() for b in buckets])\n",
    "        distance_metrics.extend([[\"\"] * 4] * 3)\n",
    "    else:\n",
    "        metrics_data, distance_metrics = (\n",
    "            [[\"无有效数据\", \"\", \"\", \"\"]] * 8,\n",
    "            [[\"\"] * 4] * 8,\n",
    "        )\n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=[\"项目\", \"mean\", \"max\", \"min\"])\n",
    "    distance_df = pd.DataFrame(\n",
    "        distance_metrics, columns=[\"0-600\", \"600-1500\", \"1500-2500\", \"2500以上\"]\n",
    "    )\n",
    "    return pd.concat([metrics_df, distance_df], axis=1).round(4).fillna(\"\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    for p in [\n",
    "        report_path,\n",
    "        input_data_path,\n",
    "        zhuzhuyun_data_path,\n",
    "        transit_data_path,\n",
    "        output_path,\n",
    "    ]:\n",
    "        p.mkdir(exist_ok=True, parents=True)\n",
    "    print(\"项目文件夹结构设置/检查完毕。\")\n",
    "    print(f\"输入数据(Logistics数据)应位于: {input_data_path}\")\n",
    "    print(f\"分析报告将输出至: {output_path}\")\n",
    "\n",
    "    top_30_cities = load_top_cities(basic_data_file)\n",
    "    start_time = perf_counter()\n",
    "    all_available_files = list(input_data_path.glob(\"*.xlsx\")) + list(\n",
    "        input_data_path.glob(\"*.csv\")\n",
    "    )\n",
    "    if RUN_MODE == \"ALL\":\n",
    "        files_to_process = all_available_files\n",
    "    elif RUN_MODE == \"SINGLE\":\n",
    "        files_to_process = [\n",
    "            f for f in all_available_files if f.name.startswith(TARGET_COMPANY)\n",
    "        ]\n",
    "    else:\n",
    "        print(f\"错误: 无效的 RUN_MODE '{RUN_MODE}'。\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"\\n警告: 未找到任何待处理的数据文件，程序即将退出。\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # 【修改点 1】: 准备一个列表来收集每个文件的统计数据\n",
    "    summary_stats = []\n",
    "\n",
    "    files_to_process = sorted(list(set(files_to_process)))\n",
    "    print(f\"\\n找到 {len(files_to_process)} 个待处理文件，开始分析...\")\n",
    "    for file_path in files_to_process:\n",
    "        stats = process_single_file(file_path, top_30_cities)\n",
    "        if stats:\n",
    "            summary_stats.append(stats)\n",
    "\n",
    "    end_time = perf_counter()\n",
    "    print(f\"\\n\\n所有文件处理完毕！总耗时: {end_time - start_time:.2f} 秒。\")\n",
    "\n",
    "    # 【修改点 2】: 在所有文件处理后，生成并打印汇总统计表\n",
    "    if summary_stats:\n",
    "        print(\"\\n\" + \"=\" * 20 + \" 数据筛选流程汇总统计 \" + \"=\" * 20)\n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "\n",
    "        # 计算衍生列\n",
    "        summary_df[\"**总计删除**\"] = (\n",
    "            summary_df[\"因时间戳缺失删除\"]\n",
    "            + summary_df[\"因时限空值删除\"]\n",
    "            + summary_df[\"因范围不符删除\"]\n",
    "        )\n",
    "        summary_df[\"最终保留\"] = summary_df[\"原始数据量\"] - summary_df[\"**总计删除**\"]\n",
    "        summary_df[\"**保留率**\"] = (\n",
    "            summary_df[\"最终保留\"] / summary_df[\"原始数据量\"]\n",
    "        ).apply(lambda x: f\"{x:.1%}\")\n",
    "\n",
    "        # 重新排序列\n",
    "        summary_df = summary_df[\n",
    "            [\n",
    "                \"公司 (Company)\",\n",
    "                \"原始数据量\",\n",
    "                \"因时间戳缺失删除\",\n",
    "                \"因时限空值删除\",\n",
    "                \"因范围不符删除\",\n",
    "                \"**总计删除**\",\n",
    "                \"最终保留\",\n",
    "                \"**保留率**\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        # 计算总计行\n",
    "        total_row = summary_df.select_dtypes(include=np.number).sum()\n",
    "        total_row[\"公司 (Company)\"] = \"**总计**\"\n",
    "        total_row[\"**保留率**\"] = (\n",
    "            f\"{(total_row['最终保留'] / total_row['原始数据量']):.1%}\"\n",
    "        )\n",
    "\n",
    "        # 将总计行添加到DataFrame\n",
    "        summary_df = pd.concat(\n",
    "            [summary_df, pd.DataFrame(total_row).T], ignore_index=True\n",
    "        )\n",
    "\n",
    "        # 打印Markdown格式的表格\n",
    "        print(summary_df.to_markdown(index=False))\n",
    "\n",
    "\n",
    "def process_single_file(file_path: Path, top_30_cities: set):\n",
    "    \"\"\"(最终方案版) 处理单个文件，并为下游脚本准备所有必需的标签列\"\"\"\n",
    "    file_name = file_path.name\n",
    "    print(f\"\\n--- 正在处理文件: {file_name} ---\")\n",
    "\n",
    "    # --- 1. 数据加载 ---\n",
    "    try:\n",
    "        data = (\n",
    "            pd.read_excel(file_path, header=0, dtype={\"单号\": str}, engine=\"openpyxl\")\n",
    "            if file_path.suffix == \".xlsx\"\n",
    "            else pd.read_csv(file_path, header=0, dtype={\"单号\": str})\n",
    "        )\n",
    "        original_data_count = data.shape[0]\n",
    "        print(f\"原始数据量: {original_data_count} 条\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件 {file_name} 失败: {e}。跳过此文件。\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. 列检查与补全 ---\n",
    "    core_required_cols = [\n",
    "        \"单号\",\n",
    "        \"寄出省份\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达省份\",\n",
    "        \"寄达城市\",\n",
    "        \"公里\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "    ]\n",
    "    optional_cols = [\"到达分拣中心时间\", \"离开收件城市分拣中心时间\"]\n",
    "    if not all(c in data.columns for c in core_required_cols):\n",
    "        print(f\"错误: 文件 {file_name} 缺少必要的核心列。跳过此文件。\")\n",
    "        return None\n",
    "    for col in optional_cols:\n",
    "        if col not in data.columns:\n",
    "            data[col] = pd.NaT\n",
    "\n",
    "    # --- 合并猪猪云数据 ---\n",
    "    company_name_from_file = file_name.split(\"_\")[0]\n",
    "    zhuzhuyun_file = zhuzhuyun_data_path / f\"{company_name_from_file}.xlsx\"\n",
    "    if zhuzhuyun_file.exists():\n",
    "        try:\n",
    "            df_zhuzhu = pd.read_excel(\n",
    "                zhuzhuyun_file,\n",
    "                usecols=[\"快递单号\", \"完整物流信息\"],\n",
    "                dtype={\"快递单号\": str},\n",
    "                engine=\"openpyxl\",\n",
    "            ).rename(columns={\"快递单号\": \"单号\"})\n",
    "            data = pd.merge(\n",
    "                data, df_zhuzhu.drop_duplicates(subset=[\"单号\"]), on=\"单号\", how=\"left\"\n",
    "            )\n",
    "            if \"完整物流信息_y\" in data.columns:\n",
    "                # 优先使用 _y (猪猪云) 的数据，如果猪猪云数据为空，则使用 _x (主数据) 的数据\n",
    "                data[\"完整物流信息\"] = data[\"完整物流信息_y\"].fillna(\n",
    "                    data[\"完整物流信息_x\"]\n",
    "                )\n",
    "                # 删除多余的 _x 和 _y 列，保持数据整洁\n",
    "                data.drop(columns=[\"完整物流信息_x\", \"完整物流信息_y\"], inplace=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  - 警告: 合并猪猪云数据失败: {e}\")\n",
    "    if \"完整物流信息\" not in data.columns:\n",
    "        data[\"完整物流信息\"] = \"\"\n",
    "\n",
    "    # --- 3. 时间转换与时限计算 ---\n",
    "    all_time_cols = core_required_cols + optional_cols\n",
    "    all_time_cols = [c for c in all_time_cols if \"时间\" in c]\n",
    "    for col in all_time_cols:\n",
    "        data[col] = pd.to_datetime(data[col], errors=\"coerce\")\n",
    "\n",
    "    count_before_time_dropna = len(data)\n",
    "    data.dropna(subset=[c for c in core_required_cols if \"时间\" in c], inplace=True)\n",
    "    time_dropped_count = count_before_time_dropna - len(data)\n",
    "    print(f\"  - 因核心时间戳缺失，删除了 {time_dropped_count} 行。\")\n",
    "\n",
    "    # 计算所有9个时限\n",
    "    data[\"全程时限\"] = (data[\"签收时间\"] - data[\"揽收时间\"]) / np.timedelta64(1, \"h\")\n",
    "    data[\"寄出地处理时限\"] = (\n",
    "        data[\"离开寄件城市时间\"] - data[\"揽收时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "    data[\"寄达地处理时限\"] = (\n",
    "        data[\"派送时间\"] - data[\"到达收件城市时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "    data[\"投递时限\"] = (data[\"签收时间\"] - data[\"派送时间\"]) / np.timedelta64(1, \"h\")\n",
    "    data[\"运输时限\"] = (\n",
    "        data[\"到达收件城市时间\"] - data[\"离开寄件城市时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "\n",
    "    if \"到达分拣中心时间\" in data.columns:\n",
    "        data[\"揽收-到达寄出地分拣中心时长\"] = (\n",
    "            data[\"到达分拣中心时间\"] - data[\"揽收时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        data[\"到达寄出地分拣中心-离开寄出地城市时长\"] = (\n",
    "            data[\"离开寄件城市时间\"] - data[\"到达分拣中心时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "    else:\n",
    "        data[\"揽收-到达寄出地分拣中心时长\"] = np.nan\n",
    "        data[\"到达寄出地分拣中心-离开寄出地城市时长\"] = np.nan\n",
    "\n",
    "    if \"离开收件城市分拣中心时间\" in data.columns:\n",
    "        data[\"到达寄达地城市-离开寄达地分拣中心时长\"] = (\n",
    "            data[\"离开收件城市分拣中心时间\"] - data[\"到达收件城市时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        data[\"离开寄达地分拣中心-派件\"] = (\n",
    "            data[\"派送时间\"] - data[\"离开收件城市分拣中心时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "    else:\n",
    "        data[\"到达寄达地城市-离开寄达地分拣中心时长\"] = np.nan\n",
    "        data[\"离开寄达地分拣中心-派件\"] = np.nan\n",
    "\n",
    "    # --- 4. 严格且有区分的筛选 ---\n",
    "    company_key = FILENAME_TO_COMPANY_MAP.get(\n",
    "        company_name_from_file, company_name_from_file\n",
    "    )\n",
    "    EXCLUDED_COMPANIES = {\"中通\", \"顺丰\"}\n",
    "\n",
    "    count_before_filter = len(data)\n",
    "\n",
    "    core_duration_cols = [\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"运输时限\",\n",
    "    ]\n",
    "    all_duration_cols = core_duration_cols + [\n",
    "        \"揽收-到达寄出地分拣中心时长\",\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "        \"离开寄达地分拣中心-派件\",\n",
    "    ]\n",
    "\n",
    "    if company_key in EXCLUDED_COMPANIES:\n",
    "        print(\"  - 正在为中通/顺丰执行宽松筛选(第1步)：仅检查5个核心时限的空值...\")\n",
    "        columns_to_check_for_nan = core_duration_cols\n",
    "    else:\n",
    "        print(\"  - 正在为其他公司执行严格筛选(第1步)：检查所有9个时限的空值...\")\n",
    "        columns_to_check_for_nan = [\n",
    "            col for col in all_duration_cols if col in data.columns\n",
    "        ]\n",
    "\n",
    "    data.dropna(subset=columns_to_check_for_nan, inplace=True)\n",
    "    count_after_nan_drop = len(data)\n",
    "    nan_dropped_count = count_before_filter - count_after_nan_drop\n",
    "    print(f\"    -> 因时限存在空值，删除了 {nan_dropped_count} 行。\")\n",
    "\n",
    "    print(\"  - 正在筛选(第2步)：应用业务逻辑范围条件...\")\n",
    "    base_mask = (\n",
    "        data[\"寄出地处理时限\"].between(0.1, 48)\n",
    "        & data[\"运输时限\"].between(0.5, 200)\n",
    "        & data[\"寄达地处理时限\"].between(0.1, 60)\n",
    "        & data[\"投递时限\"].between(0, 36)\n",
    "    )\n",
    "\n",
    "    if company_key not in EXCLUDED_COMPANIES:\n",
    "        print(\"    -> 正在为其他公司增加额外筛选：4个分拣中心时长 > 0\")\n",
    "        new_duration_cols = [\n",
    "            \"揽收-到达寄出地分拣中心时长\",\n",
    "            \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "            \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "            \"离开寄达地分拣中心-派件\",\n",
    "        ]\n",
    "        for col in new_duration_cols:\n",
    "            if col in data.columns:\n",
    "                base_mask &= data[col] > 0\n",
    "\n",
    "    filtered_data = data.loc[base_mask].copy()\n",
    "\n",
    "    count_after_value_filter = len(filtered_data)\n",
    "    value_dropped_count = count_after_nan_drop - count_after_value_filter\n",
    "    total_dropped_this_stage = nan_dropped_count + value_dropped_count\n",
    "    print(f\"    -> 因时限值不符合业务范围，又删除了 {value_dropped_count} 行。\")\n",
    "    print(f\"  - [小计] 时效筛选阶段共删除 {total_dropped_this_stage} 行。\")\n",
    "\n",
    "    total_dropped_count = time_dropped_count + total_dropped_this_stage\n",
    "\n",
    "    if filtered_data.empty:\n",
    "        print(\"筛选后无有效数据，无法生成报告。跳过此文件。\")\n",
    "        return None\n",
    "\n",
    "    # --- 5. 为下游计算准备标签列 ---\n",
    "    filtered_data[\"T+1_achieved\"] = filtered_data[\"签收时间\"].dt.normalize() <= (\n",
    "        filtered_data[\"揽收时间\"].dt.normalize() + pd.Timedelta(days=1)\n",
    "    )\n",
    "    filtered_data[\"T+2_achieved\"] = filtered_data[\"签收时间\"].dt.normalize() <= (\n",
    "        filtered_data[\"揽收时间\"].dt.normalize() + pd.Timedelta(days=2)\n",
    "    )\n",
    "\n",
    "    AIR_KEYWORDS_MAP = {\"EMS\": \"飞往|发往航站准备安检\", \"快包\": \"飞往|发往航站准备安检\"}\n",
    "    air_keyword = AIR_KEYWORDS_MAP.get(company_key, \"\")\n",
    "    if air_keyword:\n",
    "        filtered_data[\"is_air\"] = (\n",
    "            filtered_data[\"完整物流信息\"]\n",
    "            .fillna(\"\")\n",
    "            .str.contains(air_keyword, regex=True)\n",
    "        )\n",
    "    else:\n",
    "        filtered_data[\"is_air\"] = False\n",
    "\n",
    "    # --- 6. 生成报告的三个Sheet ---\n",
    "    basic_metrics_df = generate_basic_metrics_df(\n",
    "        filtered_data, original_data_count, top_30_cities, total_dropped_count\n",
    "    )\n",
    "\n",
    "    # '线路详细数据'\n",
    "    final_detailed_cols = [\n",
    "        \"单号\",\n",
    "        \"寄出省份\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达省份\",\n",
    "        \"寄达城市\",\n",
    "        \"公里\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"运输时限\",\n",
    "        \"揽收-到达寄出地分拣中心时长\",\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "        \"离开寄达地分拣中心-派件\",\n",
    "        \"T+1_achieved\",\n",
    "        \"T+2_achieved\",\n",
    "        \"is_air\",\n",
    "    ]\n",
    "    detailed_routes_df = filtered_data[\n",
    "        [col for col in final_detailed_cols if col in filtered_data.columns]\n",
    "    ].round(3)\n",
    "\n",
    "    # '线路汇总数据'\n",
    "    print(\"  - 正在计算'线路汇总数据'...\")\n",
    "    grouped = filtered_data.groupby([\"寄出城市\", \"寄达城市\"])\n",
    "\n",
    "    agg_dict = {\n",
    "        \"快递数量\": (\"单号\", \"count\"),\n",
    "        \"全程时限\": (\"全程时限\", \"mean\"),\n",
    "        \"寄出地处理时限\": (\"寄出地处理时限\", \"mean\"),\n",
    "        \"运输时限\": (\"运输时限\", \"mean\"),\n",
    "        \"寄达地处理时限\": (\"寄达地处理时限\", \"mean\"),\n",
    "        \"投递时限\": (\"投递时限\", \"mean\"),\n",
    "        \"揽收-到达寄出地分拣中心时长\": (\"揽收-到达寄出地分拣中心时长\", \"mean\"),\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\": (\n",
    "            \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "            \"mean\",\n",
    "        ),\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\": (\n",
    "            \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "            \"mean\",\n",
    "        ),\n",
    "        \"离开寄达地分拣中心-派件\": (\"离开寄达地分拣中心-派件\", \"mean\"),\n",
    "    }\n",
    "    summary_df = grouped.agg(**agg_dict)\n",
    "\n",
    "    total_counts = grouped.size()\n",
    "    summary_df[\"72小时准时率\"] = (\n",
    "        filtered_data[filtered_data[\"全程时限\"] <= 72]\n",
    "        .groupby([\"寄出城市\", \"寄达城市\"])\n",
    "        .size()\n",
    "        / total_counts\n",
    "    ).fillna(0)\n",
    "    summary_df[\"48小时准时率\"] = (\n",
    "        filtered_data[filtered_data[\"全程时限\"] <= 48]\n",
    "        .groupby([\"寄出城市\", \"寄达城市\"])\n",
    "        .size()\n",
    "        / total_counts\n",
    "    ).fillna(0)\n",
    "    filtered_data[\"送达天数\"] = (\n",
    "        filtered_data[\"签收时间\"].dt.normalize()\n",
    "        - filtered_data[\"揽收时间\"].dt.normalize()\n",
    "    ).dt.days\n",
    "    summary_df[\"送达天数_80分位\"] = filtered_data.groupby([\"寄出城市\", \"寄达城市\"])[\n",
    "        \"送达天数\"\n",
    "    ].quantile(0.8, interpolation=\"higher\")\n",
    "    summary_df.reset_index(inplace=True)\n",
    "    summary_df[\"路线\"] = summary_df[\"寄出城市\"] + \"-\" + summary_df[\"寄达城市\"]\n",
    "    summary_df = summary_df.round(4)\n",
    "\n",
    "    print(\"  - 正在合并中转次数数据...\")\n",
    "    transit_filename_key = company_name_from_file\n",
    "    if company_key == \"快包\":\n",
    "        transit_filename_key = \"邮政\"\n",
    "    transfer_file = transit_data_path / f\"{transit_filename_key}_transit_data.xlsx\"\n",
    "    if transfer_file.exists():\n",
    "        try:\n",
    "            df_transfer = pd.read_excel(transfer_file, engine=\"openpyxl\")\n",
    "            if \"出发城市\" in df_transfer.columns and \"到达城市\" in df_transfer.columns:\n",
    "                df_transfer[\"路线\"] = (\n",
    "                    df_transfer[\"出发城市\"] + \"-\" + df_transfer[\"到达城市\"]\n",
    "                )\n",
    "                df_agg = (\n",
    "                    df_transfer.groupby(\"路线\")[[\"平均中转次数\"]]\n",
    "                    .mean()\n",
    "                    .rename(columns={\"平均中转次数\": \"中转次数\"})\n",
    "                )\n",
    "                summary_df = pd.merge(summary_df, df_agg, on=\"路线\", how=\"left\")\n",
    "        except Exception as e:\n",
    "            print(f\"   -> 警告: 处理中转文件 {transfer_file.name} 失败: {e}\")\n",
    "    else:\n",
    "        print(f\"   -> 提示: 未找到对应的中转文件: {transfer_file.name}\")\n",
    "\n",
    "    if \"中转次数\" not in summary_df.columns:\n",
    "        summary_df[\"中转次数\"] = np.nan\n",
    "\n",
    "    final_summary_cols = [\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"路线\",\n",
    "        \"快递数量\",\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"运输时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"揽收-到达寄出地分拣中心时长\",\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "        \"离开寄达地分拣中心-派件\",\n",
    "        \"72小时准时率\",\n",
    "        \"48小时准时率\",\n",
    "        \"送达天数_80分位\",\n",
    "        \"中转次数\",\n",
    "    ]\n",
    "    summary_df = summary_df[\n",
    "        [col for col in final_summary_cols if col in summary_df.columns]\n",
    "    ]\n",
    "\n",
    "    # --- 7. 写入Excel ---\n",
    "    output_file_path = (\n",
    "        output_path / f\"{company_name_from_file}_data_analysis_result.xlsx\"\n",
    "    )\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        basic_metrics_df.to_excel(writer, sheet_name=\"基础指标\", index=False)\n",
    "        detailed_routes_df.to_excel(writer, sheet_name=\"线路详细数据\", index=False)\n",
    "        summary_df.to_excel(writer, sheet_name=\"线路汇总数据\", index=False)\n",
    "    print(f\"分析完成, 结果已保存至: {output_file_path}\")\n",
    "\n",
    "    # 【修改点 3】: 返回统计数据字典\n",
    "    stats_dict = {\n",
    "        \"公司 (Company)\": company_key,\n",
    "        \"原始数据量\": original_data_count,\n",
    "        \"因时间戳缺失删除\": time_dropped_count,\n",
    "        \"因时限空值删除\": nan_dropped_count,\n",
    "        \"因范围不符删除\": value_dropped_count,\n",
    "    }\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
