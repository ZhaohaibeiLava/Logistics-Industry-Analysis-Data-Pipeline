{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65e5b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本期报告期数已设置为: 202506\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 0: 【请在这里手动修改本期，例如 '202506'】\n",
    "# --------------------------------------------------\n",
    "period = \"202506\"\n",
    "\n",
    "print(f\"本期报告期数已设置为: {period}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "754cd750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "库导入完成。\n",
      "项目文件夹结构设置/检查完毕。请按以下结构组织文件：\n",
      "将10个安监数据Excel文件放入: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输入/安监数据\n",
      "⚠️ 请修改“京邦达”的名字为“京东；\n",
      "⚠️ 请修改“中国邮政”的名字为“邮政”\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 1: 导入库并设置项目结构\n",
    "# --------------------------------------------------\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"库导入完成。\")\n",
    "\n",
    "# --- 项目路径设置  ---\n",
    "# .\n",
    "# ├── 1_data_preprocess.ipynb\n",
    "# └── 报告数据/\n",
    "#     ├── 输入/\n",
    "#     │   ├── 安监数据/              (存放老师给的原始Excel文件；10家公司，10个文件)\n",
    "#     │   └── basic_data.xlsx          (城市信息、线路信息)\n",
    "#     ├── 输出/                      (存放所有最终生成的报告)\n",
    "#     ├── temp/\n",
    "#     │   ├── 1_待上传猪猪云数据/        (需要逐个手动上传到猪猪云的文件；8家公司，16个文件，排除顺丰和中通)\n",
    "#     │   ├── 2_猪猪云下载数据/          (【手动放入】存放从猪猪云下载的结果文件；8家公司，16个文件，排除顺丰和中通)\n",
    "#     │   ├── 3_猪猪云合并数据/         （猪猪云下载数据按公司合并后数据；8家公司，8个文件，排除顺丰和中通）\n",
    "#     │   ├── 4_logistics数据         （存放logistics数据——提取完整物流信息的时间戳后的数据；8家公司，8个文件，排除顺丰和中通）\n",
    "#     └── └── 5_中转数据/               (存放中转数据——提取中转城市和平均中转次数后的数据；8家公司，8个文件，排除顺丰和中通)\n",
    "# 根目录\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "# 输入路径\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "# 输出路径\n",
    "output_path = report_path / \"输出\"\n",
    "# 中间过程文件路径（自动创建，用于存放临时文件）\n",
    "temp_path = report_path / \"temp\"\n",
    "upload_split_path = temp_path / \"1_待上传猪猪云文件\"  # 存放拆分后待上传的文件\n",
    "zhuzhuyun_download_path = temp_path / \"2_猪猪云下载数据\"  # 关键：这是手动放置文件的目录\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "transit_data_path = temp_path / \"5_中转数据\"\n",
    "\n",
    "# 创建所有需要的文件夹\n",
    "for p in [\n",
    "    report_path,\n",
    "    input_path,\n",
    "    anjian_data_path,\n",
    "    zhuzhuyun_download_path,\n",
    "    zhuzhuyun_merge_path,\n",
    "    transit_data_path,\n",
    "    output_path,\n",
    "    temp_path,\n",
    "    upload_split_path,\n",
    "    pycharm_input_path,\n",
    "]:\n",
    "    p.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"项目文件夹结构设置/检查完毕。请按以下结构组织文件：\")\n",
    "print(\n",
    "    f\"将10个安监数据Excel文件放入: {anjian_data_path}\\n⚠️ 请修改“京邦达”的名字为“京东；\\n⚠️ 请修改“中国邮政”的名字为“邮政”\"\n",
    ")\n",
    "# 计算口径问题\n",
    "# 1.所有的均值、最优值、排名均不计算快包（和EMS本质是一家公司的不同产品）\n",
    "# 2.线路的送达天数计算使用的是80分位数\n",
    "# 3.线路层的均值不加权（计算全部线路均值，需要用∑（单一线路均值*线路快递数量）/快递总数量，或直接对全部线路求均值不延续适用线路均值；但行业层面可直接对公司层面的均值（不包括快包），直接求简单的算术平均（并非全样本真实均值、只是简单对各公司水平求均值）\n",
    "# 4.“单一数据源原则”：logistics经过\n",
    "### (data[\"寄出地处理时限\"].between(0.1，48))\n",
    "### (data[\"运输时限\"].between(0.5，200))&(data[\"寄达地处理时限\"].between(0.1，60))&\n",
    "### (data[“投递时限\"].between(0，36))]\n",
    "### 筛选后得到data_analysis_result；后续的数据只能够基于data_analysis_result进一步计算，不能再重新回到 logistics 数据\n",
    "# 5.“48小时妥投率”口径：经过筛选后的数据（即data_analysis_result）的基础上，仅使用“Top 30城市互寄”的数据来计算；而“72小时妥投率”直接用data_analysis_result数据，不经过“Top 30城市“筛选\n",
    "# 6.邮政月报.xlsx中的“分城市明细”和“分省份明细”sheet中\n",
    "### 全程时限（双挂）是该城市按照所有以该城市为寄出地和寄达地的邮件进行聚合\n",
    "### 全程时限（寄出地）是对应的城市按照所有以该城市为寄出地的邮件进行聚合\n",
    "### 寄出地处理时限、揽收-到达寄出地分拣中心时长、到达寄出地分拣中心-离开寄出地城市时长是按照所有该城市作为寄出地的邮件聚合\n",
    "### 寄达地处理时限、到达寄达地城市-离开寄达地分拣中心时长、离开寄达地分拣中心-派件是按照所有该城市作为寄达地的邮件聚合\n",
    "### 投递时限是按照所有该城市作为寄达地的邮件聚合\n",
    "### 寄出地处理时限+寄达地处理时限+投递时限是前面的简单加和，即寄出地处理时限（按寄出地聚合）+寄达地处理时限（按寄达地聚合）+投递时限（按寄达地聚合）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22767ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 第1部分：开始准备上传数据...\n",
      "正在处理精确匹配的文件: 2025年6月EMS抽样.xlsx (公司: EMS)\n",
      "  -> EMS 数据已拆分为 1 个文件。\n",
      "公司 '中通' 在排除列表内，跳过准备上传。\n",
      "正在处理精确匹配的文件: 2025年6月京东抽样.xlsx (公司: 京东)\n",
      "  -> 京东 数据已拆分为 2 个文件。\n",
      "正在处理精确匹配的文件: 2025年6月圆通抽样.xlsx (公司: 圆通)\n",
      "  -> 圆通 数据已拆分为 2 个文件。\n",
      "正在处理精确匹配的文件: 2025年6月德邦抽样.xlsx (公司: 德邦)\n",
      "  -> 德邦 数据已拆分为 2 个文件。\n",
      "正在处理精确匹配的文件: 2025年6月极兔抽样.xlsx (公司: 极兔)\n",
      "  -> 极兔 数据已拆分为 2 个文件。\n",
      "正在处理精确匹配的文件: 2025年6月申通抽样.xlsx (公司: 申通)\n",
      "  -> 申通 数据已拆分为 2 个文件。\n",
      "正在处理精确匹配的文件: 2025年6月韵达抽样.xlsx (公司: 韵达)\n",
      "  -> 韵达 数据已拆分为 2 个文件。\n",
      "正在处理精确匹配的文件: 2025年6月邮政抽样.xlsx (公司: 邮政)\n",
      "  -> 邮政 数据已拆分为 2 个文件。\n",
      "公司 '顺丰' 在排除列表内，跳过准备上传。\n",
      "\n",
      "====================================================================================================\n",
      "【第一部分完成】数据准备完毕！\n",
      "请前往文件夹: \n",
      "/Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/1_待上传猪猪云文件\n",
      "将里面的所有Excel文件手动上传到“猪猪快递云”网站，\n",
      "\n",
      "最后将所有下载结果放入“/报告数据/temp/2_猪猪云下载数据/”文件夹中。\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 2: 拆分安监数据，准备用于猪猪云上传的数据\n",
    "# --------------------------------------------------\n",
    "def prepare_data_for_upload(\n",
    "    source_dir: Path,\n",
    "    target_dir: Path,\n",
    "    companies_to_exclude: list,\n",
    "    period: str,\n",
    "    chunk_size: int = 50000,\n",
    "):\n",
    "    \"\"\"\n",
    "    根据指定`period`精确查找对应的安监数据，保留'企业'和'单号'列，\n",
    "    并按chunk_size动态拆分为n个文件，为手动上传做准备。\n",
    "    \"\"\"\n",
    "    import math\n",
    "    from pathlib import Path\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\">>> 第1部分：开始准备上传数据...\")\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for f in target_dir.glob(\"*.xlsx\"):\n",
    "        f.unlink()\n",
    "\n",
    "    # --- 文件查找精准匹配 ---\n",
    "    # 1. 从 period (例如 '202507') 中解析出年份和月份\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])  # 使用 int() 去掉可能存在的前导零\n",
    "\n",
    "    # 2. 定义要处理的公司列表\n",
    "    companies = [\n",
    "        \"EMS\",\n",
    "        \"中通\",\n",
    "        \"京东\",\n",
    "        \"圆通\",\n",
    "        \"德邦\",\n",
    "        \"极兔\",\n",
    "        \"申通\",\n",
    "        \"韵达\",\n",
    "        \"邮政\",\n",
    "        \"顺丰\",\n",
    "    ]\n",
    "\n",
    "    # 3. 不再遍历文件夹里的所有文件，而是遍历公司列表，主动构造文件名进行精确查找\n",
    "    for company_name in companies:\n",
    "        if company_name in companies_to_exclude:\n",
    "            print(f\"公司 '{company_name}' 在排除列表内，跳过准备上传。\")\n",
    "            continue\n",
    "\n",
    "        # 4. 根据 period 和公司名，构建期望的精确文件名\n",
    "        target_anjian_filename = f\"{year}年{month}月{company_name}抽样.xlsx\"\n",
    "        file_path = source_dir / target_anjian_filename\n",
    "\n",
    "        # 为了兼容性，也检查 .xls 后缀\n",
    "        target_anjian_filename_xls = target_anjian_filename.replace(\".xlsx\", \".xls\")\n",
    "        file_path_xls = source_dir / target_anjian_filename_xls\n",
    "\n",
    "        # 5. 精确检查文件是否存在\n",
    "        if not file_path.exists() and not file_path_xls.exists():\n",
    "            print(\n",
    "                f\"未找到 {company_name} 的本期安监文件: '{target_anjian_filename}' 或 '.xls'，已跳过。\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # 如果.xlsx不存在但.xls存在，则使用.xls的路径\n",
    "        if not file_path.exists():\n",
    "            file_path = file_path_xls\n",
    "\n",
    "        print(f\"正在处理精确匹配的文件: {file_path.name} (公司: {company_name})\")\n",
    "        # --- 精准匹配逻辑修改结束 ---\n",
    "\n",
    "        # 读取并校验Excel文件\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, dtype={\"单号\": str})\n",
    "            if \"企业\" not in df.columns or \"单号\" not in df.columns:\n",
    "                print(\n",
    "                    f\"  -> 警告: 文件 {file_path.name} 缺少 '企业' 或 '单号' 列，已跳过。\"\n",
    "                )\n",
    "                continue\n",
    "            df_to_upload = df[[\"企业\", \"单号\"]].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"  -> 错误: 读取文件 {file_path.name} 时出错: {e}，已跳过。\")\n",
    "            continue\n",
    "\n",
    "        # --- 动态拆分文件的逻辑保持不变 ---\n",
    "        total_rows = len(df_to_upload)\n",
    "        if total_rows == 0:\n",
    "            print(f\"  -> {company_name} 数据为空，跳过保存。\")\n",
    "            continue\n",
    "\n",
    "        num_chunks = math.ceil(total_rows / chunk_size)\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_row = i * chunk_size\n",
    "            end_row = start_row + chunk_size\n",
    "            chunk_df = df_to_upload.iloc[start_row:end_row]\n",
    "\n",
    "            # 输出文件名已符合原则1，无需改动\n",
    "            output_filename = f\"{company_name}_{period}_{i + 1}.xlsx\"\n",
    "            output_filepath = target_dir / output_filename\n",
    "            chunk_df.to_excel(output_filepath, index=False)\n",
    "\n",
    "        print(f\"  -> {company_name} 数据已拆分为 {num_chunks} 个文件。\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"【第一部分完成】数据准备完毕！\")\n",
    "    print(f\"请前往文件夹: \\n{target_dir.resolve()}\")\n",
    "    print(\n",
    "        \"将里面的所有Excel文件手动上传到“猪猪快递云”网站，\\n\\n最后将所有下载结果放入“/报告数据/temp/2_猪猪云下载数据/”文件夹中。\"\n",
    "    )\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "\n",
    "# 定义需要排除的公司列表\n",
    "companies_to_exclude_from_upload = [\"顺丰\", \"中通\"]\n",
    "# 调用函数的代码无需修改\n",
    "prepare_data_for_upload(\n",
    "    anjian_data_path, upload_split_path, companies_to_exclude_from_upload, period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40a381f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始从文件夹 </Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/2_猪猪云下载数据> 读取猪猪云下载数据...\n",
      "\n",
      "开始处理各公司文件...\n",
      "\n",
      "正在合并 申通 的文件 (2 个)...\n",
      "  已读取: 申通_202506_1.xlsx\n",
      "  已读取: 申通_202506_2.xlsx\n",
      "  未发现'快递单号'的重复值。\n",
      "  成功保存合并数据到: 申通_202506.xlsx (共 83029 行)\n",
      "\n",
      "正在合并 京东 的文件 (2 个)...\n",
      "  已读取: 京东_202506_2.xlsx\n",
      "  已读取: 京东_202506_1.xlsx\n",
      "  未发现'快递单号'的重复值。\n",
      "  成功保存合并数据到: 京东_202506.xlsx (共 82770 行)\n",
      "\n",
      "正在合并 圆通 的文件 (2 个)...\n",
      "  已读取: 圆通_202506_1.xlsx\n",
      "  已读取: 圆通_202506_2.xlsx\n",
      "  未发现'快递单号'的重复值。\n",
      "  成功保存合并数据到: 圆通_202506.xlsx (共 82733 行)\n",
      "\n",
      "正在合并 德邦 的文件 (2 个)...\n",
      "  已读取: 德邦_202506_2.xlsx\n",
      "  已读取: 德邦_202506_1.xlsx\n",
      "  未发现'快递单号'的重复值。\n",
      "  成功保存合并数据到: 德邦_202506.xlsx (共 78551 行)\n",
      "\n",
      "正在合并 邮政 的文件 (2 个)...\n",
      "  已读取: 邮政_202506_2.xlsx\n",
      "  已读取: 邮政_202506_1.xlsx\n",
      "  未发现'快递单号'的重复值。\n",
      "  成功保存合并数据到: 邮政_202506.xlsx (共 81571 行)\n",
      "\n",
      "正在合并 EMS 的文件 (1 个)...\n",
      "  已读取: EMS_202506_1.xlsx\n",
      "  未发现'快递单号'的重复值。\n",
      "  成功保存合并数据到: EMS_202506.xlsx (共 43390 行)\n",
      "\n",
      "正在合并 极兔 的文件 (2 个)...\n",
      "  已读取: 极兔_202506_2.xlsx\n",
      "  已读取: 极兔_202506_1.xlsx\n",
      "  未发现'快递单号'的重复值。\n",
      "  成功保存合并数据到: 极兔_202506.xlsx (共 81703 行)\n",
      "\n",
      "正在合并 韵达 的文件 (2 个)...\n",
      "  已读取: 韵达_202506_1.xlsx\n",
      "  已读取: 韵达_202506_2.xlsx\n",
      "  未发现'快递单号'的重复值。\n",
      "  成功保存合并数据到: 韵达_202506.xlsx (共 81797 行)\n",
      "\n",
      "所有公司文件合并完成！合并后的文件已存入 </Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/3_猪猪云合并数据>\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 3: 合并猪猪云下载数据，生成猪猪云合并数据\n",
    "# --------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# 定义要处理的公司列表\n",
    "companies = [\n",
    "    \"EMS\",\n",
    "    \"中通\",\n",
    "    \"京东\",\n",
    "    \"圆通\",\n",
    "    \"德邦\",\n",
    "    \"极兔\",\n",
    "    \"申通\",\n",
    "    \"韵达\",\n",
    "    \"邮政\",\n",
    "    \"顺丰\",\n",
    "]\n",
    "\n",
    "companies_lower = [comp.lower() for comp in companies]\n",
    "company_files = {}\n",
    "\n",
    "# 关键：请确保手动从猪猪云下载的文件，放入 \"2_猪猪云下载数据\" 文件夹\n",
    "print(f\"开始从文件夹 <{zhuzhuyun_download_path.resolve()}> 读取猪猪云下载数据...\")\n",
    "for file_path in zhuzhuyun_download_path.iterdir():\n",
    "    if file_path.is_file() and file_path.suffix == \".xlsx\":\n",
    "        filename_stem = file_path.stem\n",
    "        filename_stem_lower = filename_stem.lower()\n",
    "\n",
    "        # 检查文件名是否包含当前处理的 period，如果不包含则跳过\n",
    "        if period not in filename_stem:\n",
    "            print(\n",
    "                f\"  -> 文件 '{file_path.name}' 的周期与当前 ({period}) 不符，已跳过。\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        company_name_found = False\n",
    "        found_company = None\n",
    "        for i, company_lower in enumerate(companies_lower):\n",
    "            if company_lower in filename_stem_lower:\n",
    "                found_company = companies[i]\n",
    "                company_name_found = True\n",
    "                break\n",
    "\n",
    "        if company_name_found:\n",
    "            if found_company not in company_files:\n",
    "                company_files[found_company] = []\n",
    "            company_files[found_company].append(file_path)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  警告: 文件 '{file_path.name}' 未匹配到任何已知公司名，将跳过此文件。\"\n",
    "            )\n",
    "\n",
    "print(\"\\n开始处理各公司文件...\")\n",
    "\n",
    "for company_name, files_list in company_files.items():\n",
    "    print(f\"\\n正在合并 {company_name} 的文件 ({len(files_list)} 个)...\")\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for file_path in files_list:\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"  已读取: {file_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  读取文件失败: {file_path.name}, 错误: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        # 输出文件名已符合要求，无需改动\n",
    "        output_filename = f\"{company_name}_{period}.xlsx\"\n",
    "        output_file_path = zhuzhuyun_merge_path / output_filename\n",
    "\n",
    "        if \"快递单号\" in merged_df.columns:\n",
    "            rows_before_dedup = len(merged_df)\n",
    "            merged_df.drop_duplicates(subset=[\"快递单号\"], keep=\"first\", inplace=True)\n",
    "            rows_after_dedup = len(merged_df)\n",
    "            num_duplicates_removed = rows_before_dedup - rows_after_dedup\n",
    "            if num_duplicates_removed > 0:\n",
    "                print(f\"  已基于'快递单号'移除 {num_duplicates_removed} 个重复值。\")\n",
    "            else:\n",
    "                print(f\"  未发现'快递单号'的重复值。\")\n",
    "        else:\n",
    "            print(f\"  警告: 合并后的数据中未找到 '快递单号' 列，无法执行去重操作。\")\n",
    "\n",
    "        try:\n",
    "            merged_df.to_excel(output_file_path, index=False)\n",
    "            print(\n",
    "                f\"  成功保存合并数据到: {output_file_path.name} (共 {len(merged_df)} 行)\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  保存文件失败: {output_file_path.name}, 错误: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"  没有成功读取 {company_name} 的任何文件，跳过合并。\")\n",
    "\n",
    "print(f\"\\n所有公司文件合并完成！合并后的文件已存入 <{zhuzhuyun_merge_path.resolve()}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36892b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####-------- logistics数据提取原则-------------#####\n",
    "# 1. “到达分拣中心时间”本质是“到达寄件城市分拣中心时间”\n",
    "#### 寄达城市（即收件城市）分拣中心时间以提取到的第一个分拣中心为准\n",
    "#### 寄出城市（即寄件城市）分拣时间以提取到的最后一个分拣中心为准\n",
    "# 2. “签收时间”分为3种情况，\n",
    "#### 1）上门送件，以上门的时间为签收时间；\n",
    "#### 2）放在取件网点，顾客之后自己取走，以放在取件网点的时间为签收时间；\n",
    "#### 3）放在快递柜、丰巢等，顾客之后自己取走，以放在快递柜、丰巢的时间为签收时间。\n",
    "# 3. “转运中心”中只有省份名称，没有城市名称的，认为该转运中心就在该省的省会，如果相应的寄出/寄达城市就是该省的省会，则认为快件仍在该寄出地/寄达地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8892e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通用库导入完成。\n",
      "项目文件夹结构设置/检查完毕。\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.0: logistics数据提取-通用库导入与项目结构设置\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"通用库导入完成。\")\n",
    "\n",
    "# --- 项目路径设置 ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "# 确保文件夹存在\n",
    "for p in [anjian_data_path, zhuzhuyun_merge_path, pycharm_input_path]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"项目文件夹结构设置/检查完毕。\")\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da23e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 全局数据源配置加载成功！\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================================================\n",
    "# CELL 4.0.1: 全局数据源配置中心 (Global Data Source Configuration Hub)\n",
    "# ====================================================================================================\n",
    "#\n",
    "# 在此集中配置所有物流公司5个核心时间戳的数据来源。\n",
    "# 后续所有公司处理单元格都将读取此处的配置。\n",
    "#\n",
    "# 可选值:\n",
    "#   'anjian':    使用安监数据\n",
    "#   'zhuzhuyun': 使用猪猪云合并数据\n",
    "#\n",
    "# 注意:\n",
    "#  - “到达分拣中心时间” 和 “离开收件城市分拣中心时间” 默认从猪猪云数据提取，无需在此配置。\n",
    "#  - 请确保公司名称的键 (e.g., \"EMS\", \"邮政\") 与后续处理单元格中的 `COMPANY_NAME` 变量完全一致。\n",
    "#\n",
    "# ====================================================================================================\n",
    "\n",
    "DATA_SOURCE_CONFIG = {\n",
    "    # --------------------------- EMS ---------------------------\n",
    "    \"EMS\": {\n",
    "        \"collection_time_source\": \"anjian\",  # 揽收时间\n",
    "        \"departure_time_source\": \"zhuzhuyun\",  # 离开寄件城市时间\n",
    "        \"arrival_time_source\": \"zhuzhuyun\",  # 到达收件城市时间\n",
    "        \"dispatch_time_source\": \"zhuzhuyun\",  # 派送时间\n",
    "        \"signed_time_source\": \"zhuzhuyun\",  # 签收时间\n",
    "    },\n",
    "    # --------------------------- 邮政 ---------------------------\n",
    "    \"邮政\": {\n",
    "        \"collection_time_source\": \"anjian\",\n",
    "        \"departure_time_source\": \"zhuzhuyun\",\n",
    "        \"arrival_time_source\": \"zhuzhuyun\",\n",
    "        \"dispatch_time_source\": \"zhuzhuyun\",\n",
    "        \"signed_time_source\": \"zhuzhuyun\",\n",
    "    },\n",
    "    # --------------------------- 京东 ---------------------------\n",
    "    \"京东\": {\n",
    "        \"collection_time_source\": \"anjian\",\n",
    "        \"departure_time_source\": \"zhuzhuyun\",\n",
    "        \"arrival_time_source\": \"zhuzhuyun\",\n",
    "        \"dispatch_time_source\": \"zhuzhuyun\",\n",
    "        \"signed_time_source\": \"zhuzhuyun\",\n",
    "    },\n",
    "    # --------------------------- 圆通 ---------------------------\n",
    "    \"圆通\": {\n",
    "        \"collection_time_source\": \"anjian\",\n",
    "        \"departure_time_source\": \"zhuzhuyun\",\n",
    "        \"arrival_time_source\": \"zhuzhuyun\",\n",
    "        \"dispatch_time_source\": \"zhuzhuyun\",\n",
    "        \"signed_time_source\": \"zhuzhuyun\",\n",
    "    },\n",
    "    # --------------------------- 申通 ---------------------------\n",
    "    \"申通\": {\n",
    "        \"collection_time_source\": \"anjian\",\n",
    "        \"departure_time_source\": \"zhuzhuyun\",\n",
    "        \"arrival_time_source\": \"zhuzhuyun\",\n",
    "        \"dispatch_time_source\": \"zhuzhuyun\",\n",
    "        \"signed_time_source\": \"zhuzhuyun\",\n",
    "    },\n",
    "    # --------------------------- 韵达 ---------------------------\n",
    "    \"韵达\": {\n",
    "        \"collection_time_source\": \"anjian\",\n",
    "        \"departure_time_source\": \"zhuzhuyun\",\n",
    "        \"arrival_time_source\": \"zhuzhuyun\",\n",
    "        \"dispatch_time_source\": \"zhuzhuyun\",\n",
    "        \"signed_time_source\": \"zhuzhuyun\",\n",
    "    },\n",
    "    # --------------------------- 极兔 ---------------------------\n",
    "    \"极兔\": {\n",
    "        \"collection_time_source\": \"anjian\",\n",
    "        \"departure_time_source\": \"zhuzhuyun\",\n",
    "        \"arrival_time_source\": \"zhuzhuyun\",\n",
    "        \"dispatch_time_source\": \"zhuzhuyun\",\n",
    "        \"signed_time_source\": \"zhuzhuyun\",\n",
    "    },\n",
    "    # --------------------------- 德邦 ---------------------------\n",
    "    \"德邦\": {\n",
    "        \"collection_time_source\": \"anjian\",\n",
    "        \"departure_time_source\": \"zhuzhuyun\",\n",
    "        \"arrival_time_source\": \"zhuzhuyun\",\n",
    "        \"dispatch_time_source\": \"zhuzhuyun\",\n",
    "        \"signed_time_source\": \"zhuzhuyun\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"✅ 全局数据源配置加载成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54f8eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 EMS ---\n",
      "  -> 正在读取安监数据: 2025年6月EMS抽样.xlsx\n",
      "  -> 已从安监数据中筛选出 43390 条 EMS 记录。\n",
      "  -> 使用 'parse_logistics_events_ems_ultimate' 解析器通过 .apply() 运行...\n",
      "  -> 已整合安监与猪猪云数据，共 43390 条唯一记录。\n",
      "  -> 根据全局配置选择时间戳来源...\n",
      "     - '揽收时间' 已配置使用 'anjian' 数据源。\n",
      "     - '离开寄件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '到达收件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '派送时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '签收时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "✅ EMS 处理完成，耗时 44.77 秒。文件已保存至: EMS_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.1: EMS logistics数据提取 (已修改)\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 EMS ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 (保持不变) ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: EMS (保持不变) ---\n",
    "COMPANY_PROFILES_EMS = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "    },\n",
    "    \"EMS\": {\n",
    "        \"collect\": r\"已收取快件|已收寄|已收取邮件|揽收|物流项目组\",\n",
    "        \"delivery\": r\"已安排派送|正在派送中|已派送(?!至)|为您派件|为您派送\",\n",
    "        \"leave_p1\": r\"已乘机|已搭乘邮航专机\",\n",
    "        \"leave_hard\": r\"离开\",\n",
    "        \"leave_soft\": r\"准备发出|已发出\",\n",
    "        \"center_p1\": r\"邮区中心|航空枢纽|处理中心|网路中心|航空中心\",\n",
    "        \"center_p2\": r\"包件车间|集散中心|集散点|航站|快件处理车间|快件处理中心\",\n",
    "        \"exclude\": r\"揽投部|邮政支局|直投中心|揽收部|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|到达【拼多多中转仓】|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点|已暂存至\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收|家门口签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "        \"sign_ignore\": r\"完成取件\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: EMS (终极版) (保持不变, 仅为统一命名规范修改后缀) ---\n",
    "def parse_logistics_events_ems_ultimate(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    # 【改动】为统一命名规范，将列后缀从 _zzy 改为 _zhuzhuyun\n",
    "    time_cols = [\n",
    "        \"揽收时间_zhuzhuyun\",\n",
    "        \"离开寄件城市时间_zhuzhuyun\",\n",
    "        \"到达收件城市时间_zhuzhuyun\",\n",
    "        \"派送时间_zhuzhuyun\",\n",
    "        \"签收时间_zhuzhuyun\",\n",
    "        \"到达分拣中心时间_zhuzhuyun\",\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    # --- 您其他的解析器逻辑保持完全不变 ---\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave_p1, kw_leave_hard, kw_leave_soft = (\n",
    "        profile.get(\"leave_p1\"),\n",
    "        profile.get(\"leave_hard\"),\n",
    "        profile.get(\"leave_soft\"),\n",
    "    )\n",
    "    kw_arrive, kw_exclude = profile.get(\"arrive\"), profile.get(\"exclude\", \"\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_center_p1, kw_center_p2 = profile.get(\"center_p1\"), profile.get(\"center_p2\")\n",
    "    kw_all_centers = \"|\".join(filter(None, [kw_center_p1, kw_center_p2]))\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str):\n",
    "            return None\n",
    "        aliases = location_maps[\"city_alias_map\"].get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        if province:\n",
    "            aliases.append(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "        clean_aliases = {str(a) for a in aliases if pd.notna(a) and a != \"\"}\n",
    "        return \"|\".join(map(re.escape, clean_aliases))\n",
    "\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    for event in all_events:\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"派送时间_zhuzhuyun\"])\n",
    "            and kw_delivery\n",
    "            and re.search(kw_delivery, event[\"line\"])\n",
    "        ):\n",
    "            extracted_times[\"派送时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    if kw_sign_p1:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p1, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zhuzhuyun\"]) and kw_sign_p2:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p2, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zhuzhuyun\"]) and kw_sign_fallback:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_fallback, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                break\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    if pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"]) and sender_pattern:\n",
    "        true_leave_kws_parts = [kw_leave_p1, kw_leave_hard]\n",
    "        true_leave_kws = \"|\".join(filter(None, true_leave_kws_parts))\n",
    "        last_true_leave_event = None\n",
    "        if true_leave_kws:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"]:\n",
    "                    if re.search(true_leave_kws, event[\"line\"]) and re.search(\n",
    "                        sender_pattern, event[\"line\"]\n",
    "                    ):\n",
    "                        last_true_leave_event = event\n",
    "        if last_true_leave_event:\n",
    "            extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = last_true_leave_event[\"dt\"]\n",
    "        last_sender_center_arrival = None\n",
    "        for event in all_events:\n",
    "            if event[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"]:\n",
    "                if (\n",
    "                    pd.notna(extracted_times[\"离开寄件城市时间_zhuzhuyun\"])\n",
    "                    and event[\"dt\"] >= extracted_times[\"离开寄件城市时间_zhuzhuyun\"]\n",
    "                ):\n",
    "                    break\n",
    "                if (\n",
    "                    kw_all_centers\n",
    "                    and re.search(kw_all_centers, event[\"line\"])\n",
    "                    and kw_arrive\n",
    "                    and re.search(kw_arrive, event[\"line\"])\n",
    "                    and re.search(sender_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    if not (kw_exclude and re.search(kw_exclude, event[\"line\"])):\n",
    "                        last_sender_center_arrival = event\n",
    "        if last_sender_center_arrival:\n",
    "            extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = last_sender_center_arrival[\n",
    "                \"dt\"\n",
    "            ]\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "    if dest_pattern:\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zhuzhuyun\"]):\n",
    "            for event in all_events:\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"line\"])\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                    break\n",
    "        start_time_for_dest_search = extracted_times.get(\n",
    "            \"到达收件城市时间_zhuzhuyun\", pd.NaT\n",
    "        )\n",
    "        if pd.isna(start_time_for_dest_search) and pd.notna(\n",
    "            extracted_times[\"揽收时间_zhuzhuyun\"]\n",
    "        ):\n",
    "            start_time_for_dest_search = extracted_times[\n",
    "                \"揽收时间_zhuzhuyun\"\n",
    "            ] + pd.Timedelta(hours=12)\n",
    "        if pd.notna(start_time_for_dest_search) and kw_all_centers:\n",
    "            if kw_leave_hard:\n",
    "                for event in all_events:\n",
    "                    if event[\"dt\"] > start_time_for_dest_search:\n",
    "                        if (\n",
    "                            re.search(kw_all_centers, event[\"line\"])\n",
    "                            and re.search(kw_leave_hard, event[\"line\"])\n",
    "                            and re.search(dest_pattern, event[\"line\"])\n",
    "                        ):\n",
    "                            if kw_exclude and re.search(kw_exclude, event[\"line\"]):\n",
    "                                continue\n",
    "                            extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"] = (\n",
    "                                event[\"dt\"]\n",
    "                            )\n",
    "                            break\n",
    "            if (\n",
    "                pd.isna(extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"])\n",
    "                and kw_leave_soft\n",
    "            ):\n",
    "                for event in all_events:\n",
    "                    if event[\"dt\"] > start_time_for_dest_search:\n",
    "                        if (\n",
    "                            re.search(kw_all_centers, event[\"line\"])\n",
    "                            and re.search(kw_leave_soft, event[\"line\"])\n",
    "                            and re.search(dest_pattern, event[\"line\"])\n",
    "                        ):\n",
    "                            if kw_exclude and re.search(kw_exclude, event[\"line\"]):\n",
    "                                continue\n",
    "                            extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"] = (\n",
    "                                event[\"dt\"]\n",
    "                            )\n",
    "                            break\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: EMS (已修改数据读取逻辑) ---\n",
    "def process_ems_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file, period):\n",
    "    company_name = \"EMS\"\n",
    "    config = {\n",
    "        # 【无需改动】猪猪云文件名已按period读取，符合要求\n",
    "        \"zhuzhu_filename\": f\"EMS_{period}.xlsx\",\n",
    "        \"internal_name\": \"EMS\",\n",
    "        \"anjian_map_key\": \"EMS\",\n",
    "        \"parser\": parse_logistics_events_ems_ultimate,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(\n",
    "                base_data_file,\n",
    "                sheet_name=\"city_hierarchy\",\n",
    "                dtype={\"Province\": str, \"City\": str, \"District\": str},\n",
    "            )\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])  # '202506' -> 6, 以匹配 '6月' 而非 '06月'\n",
    "    anjian_filename = f\"{year}年{month}月{company_name}抽样.xlsx\"\n",
    "    anjian_file_path = anjian_dir / anjian_filename\n",
    "\n",
    "    if not anjian_file_path.exists():\n",
    "        print(f\"[ERROR] 安监数据文件未找到，流程中止: {anjian_file_path.name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"  -> 正在读取安监数据: {anjian_file_path.name}\")\n",
    "    try:\n",
    "        all_anjian_df = pd.read_excel(\n",
    "            anjian_file_path, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 读取安监文件 '{anjian_filename}' 时出错: {e}\")\n",
    "        return\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    # 【新增】准备安监数据源，并为时间戳列添加 \"_anjian\" 后缀\n",
    "    df_anjian_ems = all_anjian_df[\n",
    "        all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]\n",
    "    ].copy()\n",
    "    if not df_anjian_ems.empty:\n",
    "        print(\n",
    "            f\"  -> 已从安监数据中筛选出 {len(df_anjian_ems)} 条 {company_name} 记录。\"\n",
    "        )\n",
    "        df_anjian_ems.rename(\n",
    "            columns={\n",
    "                \"揽收时间\": \"揽收时间_anjian\",\n",
    "                \"离开寄件城市时间\": \"离开寄件城市时间_anjian\",\n",
    "                \"到达收件城市时间\": \"到达收件城市时间_anjian\",\n",
    "                \"派送时间\": \"派送时间_anjian\",\n",
    "                \"签收时间\": \"签收时间_anjian\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    # --- 猪猪云数据处理 (保持不变) ---\n",
    "    base_info_df = all_anjian_df[[\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]].copy()\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_EMS[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_EMS.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_zhuzhuyun_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    # 【新增】将安监数据与处理后的猪猪云数据进行外连接，整合所有信息\n",
    "    # 保留猪猪云数据中的基础列和解析出的时间戳列\n",
    "    zhuzhuyun_cols_to_keep = [\n",
    "        \"单号\",\n",
    "        \"企业\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"完整物流信息\",\n",
    "    ] + [col for col in df_zhuzhuyun_final if \"_zhuzhuyun\" in col]\n",
    "    df_zhuzhuyun_subset = df_zhuzhuyun_final[zhuzhuyun_cols_to_keep]\n",
    "\n",
    "    # 保留安监数据中的基础列和时间戳列\n",
    "    anjian_cols_to_keep = [\"单号\", \"企业\"] + [\n",
    "        col for col in df_anjian_ems if \"_anjian\" in col\n",
    "    ]\n",
    "    df_anjian_subset = df_anjian_ems[anjian_cols_to_keep]\n",
    "\n",
    "    # 执行合并\n",
    "    df_final = pd.merge(\n",
    "        df_zhuzhuyun_subset, df_anjian_subset, on=[\"单号\", \"企业\"], how=\"outer\"\n",
    "    )\n",
    "    print(f\"  -> 已整合安监与猪猪云数据，共 {len(df_final)} 条唯一记录。\")\n",
    "\n",
    "    print(\"  -> 根据全局配置选择时间戳来源...\")\n",
    "\n",
    "    source_config = DATA_SOURCE_CONFIG.get(company_name, {})\n",
    "    if not source_config:\n",
    "        raise ValueError(\n",
    "            f\"错误: 在 DATA_SOURCE_CONFIG 中未找到 '{company_name}' 的配置!\"\n",
    "        )\n",
    "\n",
    "    # 1. 定义需要配置的5个时间戳\n",
    "    timestamp_map = {\n",
    "        \"揽收时间\": \"collection_time_source\",\n",
    "        \"离开寄件城市时间\": \"departure_time_source\",\n",
    "        \"到达收件城市时间\": \"arrival_time_source\",\n",
    "        \"派送时间\": \"dispatch_time_source\",\n",
    "        \"签收时间\": \"signed_time_source\",\n",
    "    }\n",
    "\n",
    "    # 2. 循环处理这5个时间戳，根据配置选择主数据源，并用另一数据源填充空值\n",
    "    for final_col, config_key in timestamp_map.items():\n",
    "        source_type = source_config.get(config_key, \"anjian\")  # 若未配置，默认使用安监\n",
    "        primary_col = f\"{final_col}_{source_type}\"\n",
    "        fallback_col = (\n",
    "            f\"{final_col}_{'zhuzhuyun' if source_type == 'anjian' else 'anjian'}\"\n",
    "        )\n",
    "\n",
    "        df_final[final_col] = df_final.get(primary_col)\n",
    "        if fallback_col in df_final:\n",
    "            df_final[final_col] = df_final[final_col].fillna(df_final[fallback_col])\n",
    "        print(f\"     - '{final_col}' 已配置使用 '{source_type}' 数据源。\")\n",
    "\n",
    "    # 3. 处理固定来源的时间戳 (这2个始终来自猪猪云)\n",
    "    df_final[\"到达分拣中心时间\"] = df_final.get(\"到达分拣中心时间_zhuzhuyun\")\n",
    "    df_final[\"离开收件城市分拣中心时间\"] = df_final.get(\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "    )\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    # --- 后续处理与保存 (保持不变) ---\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: EMS ---\n",
    "if \"period\" in locals() and \"DATA_SOURCE_CONFIG\" in locals():\n",
    "    process_ems_data(\n",
    "        zhuzhuyun_merge_path,\n",
    "        anjian_data_path,\n",
    "        pycharm_input_path,\n",
    "        base_data_path,\n",
    "        period,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[警告] 未定义 'period' 或 'DATA_SOURCE_CONFIG' 变量，跳过 process_ems_data 的执行。请在前面的单元格中定义这些变量。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27ddf952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 邮政 ---\n",
      "  -> 正在读取安监数据: 2025年6月邮政抽样.xlsx\n",
      "  -> 已从安监数据中筛选出 81571 条 邮政 (ZGYZ) 记录。\n",
      "  -> 使用 'parse_logistics_events_postal_perfected' 解析器通过 .apply() 运行...\n",
      "  -> 已整合安监与猪猪云数据，共 81571 条唯一记录。\n",
      "  -> 根据全局配置选择时间戳来源...\n",
      "     - '揽收时间' 已配置使用 'anjian' 数据源。\n",
      "     - '离开寄件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '到达收件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '派送时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '签收时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "✅ 邮政 处理完成，耗时 55.18 秒。文件已保存至: 邮政_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.2: 邮政logistics数据提取\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 邮政 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 (保持不变) ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 邮政 (保持不变) ---\n",
    "COMPANY_PROFILES_POSTAL = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "    },\n",
    "    \"邮政国内小包\": {\n",
    "        \"collect\": r\"已收取快件|已收寄|已收取邮件|揽收|物流项目组\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派件|已安排派送\",\n",
    "        \"leave_p1\": r\"已乘机|已搭乘邮航专机\",\n",
    "        \"leave_hard\": r\"离开\",\n",
    "        \"leave_soft\": r\"准备发出|已发出|发往\",\n",
    "        \"leave\": r\"离开|已发出|发往|准备发出\",\n",
    "        \"center_p1\": r\"邮区中心|航空枢纽|处理中心|网路中心|航空中心\",\n",
    "        \"center_p2\": r\"包件车间|集散中心|集散点|航站|快件处理车间|快件处理班\",\n",
    "        \"exclude\": r\"揽投部|邮政支局|直投中心|揽收部|营销中心|直投点\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|到达【拼多多中转仓】|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点|已暂存至\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收|家门口签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "        \"sign_ignore\": r\"完成取件\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 邮政 (终极完美版) (保持不变, 仅为统一命名规范修改后缀) ---\n",
    "def parse_logistics_events_postal_perfected(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    # 【改动】为统一命名规范，将列后缀从 _zzy 改为 _zhuzhuyun\n",
    "    time_cols = [\n",
    "        \"揽收时间_zhuzhuyun\",\n",
    "        \"离开寄件城市时间_zhuzhuyun\",\n",
    "        \"到达收件城市时间_zhuzhuyun\",\n",
    "        \"派送时间_zhuzhuyun\",\n",
    "        \"签收时间_zhuzhuyun\",\n",
    "        \"到达分拣中心时间_zhuzhuyun\",\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    # --- 您其他的解析器逻辑保持完全不变 ---\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave_p1, kw_leave_hard, kw_leave_soft, kw_leave = (\n",
    "        profile.get(\"leave_p1\"),\n",
    "        profile.get(\"leave_hard\"),\n",
    "        profile.get(\"leave_soft\"),\n",
    "        profile.get(\"leave\"),\n",
    "    )\n",
    "    kw_arrive, kw_exclude = profile.get(\"arrive\"), profile.get(\"exclude\", \"\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_center_p1, kw_center_p2 = profile.get(\"center_p1\"), profile.get(\"center_p2\")\n",
    "    kw_all_centers = \"|\".join(filter(None, [kw_center_p1, kw_center_p2]))\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_patterns(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str):\n",
    "            return None, None\n",
    "        aliases = location_maps[\"city_alias_map\"].get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        city_pattern = \"|\".join(\n",
    "            map(re.escape, {str(a) for a in aliases if pd.notna(a) and a != \"\"})\n",
    "        )\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        province_pattern = None\n",
    "        if province:\n",
    "            province_clean = province.replace(\"省\", \"\").replace(\"市\", \"\")\n",
    "            if province_clean and not any(\n",
    "                alias in province_clean\n",
    "                for alias in {\n",
    "                    a.replace(\"市\", \"\") for a in aliases if pd.notna(a) and a != \"\"\n",
    "                }\n",
    "            ):\n",
    "                province_pattern = re.escape(province_clean)\n",
    "        return city_pattern, province_pattern\n",
    "\n",
    "    for event in all_events:\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"揽收时间_zhuzhuyun\"])\n",
    "            and kw_collect\n",
    "            and re.search(kw_collect, event[\"line\"])\n",
    "        ):\n",
    "            extracted_times[\"揽收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "    for event in all_events:\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"派送时间_zhuzhuyun\"])\n",
    "            and kw_delivery\n",
    "            and re.search(kw_delivery, event[\"line\"])\n",
    "        ):\n",
    "            extracted_times[\"派送时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "    if pd.isna(extracted_times[\"签收时间_zhuzhuyun\"]) and kw_sign_p1:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p1, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zhuzhuyun\"]) and kw_sign_p2:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_p2, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"签收时间_zhuzhuyun\"]) and kw_sign_fallback:\n",
    "        for event in all_events:\n",
    "            if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "                continue\n",
    "            if re.search(kw_sign_fallback, event[\"line\"]):\n",
    "                extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                break\n",
    "    sender_city_pattern, sender_province_pattern = get_location_patterns(\n",
    "        sender_city_key, location_maps\n",
    "    )\n",
    "    if pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"]):\n",
    "        sender_candidates = []\n",
    "        if sender_city_pattern:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"] and re.search(\n",
    "                    sender_city_pattern, event[\"line\"]\n",
    "                ):\n",
    "                    sender_candidates.append(event)\n",
    "        if not sender_candidates and sender_province_pattern:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"] and re.search(\n",
    "                    sender_province_pattern, event[\"line\"]\n",
    "                ):\n",
    "                    sender_candidates.append(event)\n",
    "        last_center_arrival, last_center_leave = None, None\n",
    "        true_leave_kws = \"|\".join(filter(None, [kw_leave_p1, kw_leave]))\n",
    "        for event in sender_candidates:\n",
    "            if (\n",
    "                kw_all_centers\n",
    "                and re.search(kw_all_centers, event[\"line\"])\n",
    "                and not (kw_exclude and re.search(kw_exclude, event[\"line\"]))\n",
    "            ):\n",
    "                if kw_arrive and re.search(kw_arrive, event[\"line\"]):\n",
    "                    last_center_arrival = event\n",
    "                if true_leave_kws and re.search(true_leave_kws, event[\"line\"]):\n",
    "                    last_center_leave = event\n",
    "            elif kw_leave_p1 and re.search(kw_leave_p1, event[\"line\"]):\n",
    "                last_center_leave = event\n",
    "        if last_center_arrival:\n",
    "            extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = last_center_arrival[\"dt\"]\n",
    "        if last_center_leave:\n",
    "            extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = last_center_leave[\"dt\"]\n",
    "    dest_city_pattern, dest_province_pattern = get_location_patterns(\n",
    "        dest_city_key, location_maps\n",
    "    )\n",
    "    if dest_city_pattern:\n",
    "        for event in all_events:\n",
    "            if (\n",
    "                pd.isna(extracted_times[\"到达收件城市时间_zhuzhuyun\"])\n",
    "                and kw_all_centers\n",
    "                and re.search(kw_all_centers, event[\"line\"])\n",
    "                and not (kw_exclude and re.search(kw_exclude, event[\"line\"]))\n",
    "                and kw_arrive\n",
    "                and re.search(kw_arrive, event[\"line\"])\n",
    "                and re.search(dest_city_pattern, event[\"line\"])\n",
    "            ):\n",
    "                extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"到达收件城市时间_zhuzhuyun\"]) and dest_province_pattern:\n",
    "        for event in all_events:\n",
    "            if (\n",
    "                pd.isna(extracted_times[\"到达收件城市时间_zhuzhuyun\"])\n",
    "                and kw_all_centers\n",
    "                and re.search(kw_all_centers, event[\"line\"])\n",
    "                and not (kw_exclude and re.search(kw_exclude, event[\"line\"]))\n",
    "                and kw_arrive\n",
    "                and re.search(kw_arrive, event[\"line\"])\n",
    "                and re.search(dest_province_pattern, event[\"line\"])\n",
    "            ):\n",
    "                extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                break\n",
    "    start_time = extracted_times.get(\"到达收件城市时间_zhuzhuyun\", pd.NaT)\n",
    "    if pd.isna(start_time) and pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"]):\n",
    "        start_time = extracted_times[\"揽收时间_zhuzhuyun\"]\n",
    "    if pd.notna(start_time):\n",
    "        if kw_leave_hard:\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > start_time:\n",
    "                    if (\n",
    "                        dest_city_pattern\n",
    "                        and re.search(dest_city_pattern, event[\"line\"])\n",
    "                        or (\n",
    "                            dest_province_pattern\n",
    "                            and re.search(dest_province_pattern, event[\"line\"])\n",
    "                        )\n",
    "                    ):\n",
    "                        if (\n",
    "                            kw_all_centers\n",
    "                            and re.search(kw_all_centers, event[\"line\"])\n",
    "                            and not (\n",
    "                                kw_exclude and re.search(kw_exclude, event[\"line\"])\n",
    "                            )\n",
    "                        ):\n",
    "                            if re.search(kw_leave_hard, event[\"line\"]):\n",
    "                                extracted_times[\n",
    "                                    \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "                                ] = event[\"dt\"]\n",
    "                                break\n",
    "        if (\n",
    "            pd.isna(extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"])\n",
    "            and kw_leave_soft\n",
    "        ):\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > start_time:\n",
    "                    if (\n",
    "                        dest_city_pattern\n",
    "                        and re.search(dest_city_pattern, event[\"line\"])\n",
    "                        or (\n",
    "                            dest_province_pattern\n",
    "                            and re.search(dest_province_pattern, event[\"line\"])\n",
    "                        )\n",
    "                    ):\n",
    "                        if (\n",
    "                            kw_all_centers\n",
    "                            and re.search(kw_all_centers, event[\"line\"])\n",
    "                            and not (\n",
    "                                kw_exclude and re.search(kw_exclude, event[\"line\"])\n",
    "                            )\n",
    "                        ):\n",
    "                            if re.search(kw_leave_soft, event[\"line\"]):\n",
    "                                extracted_times[\n",
    "                                    \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "                                ] = event[\"dt\"]\n",
    "                                break\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 邮政 (已修改数据读取逻辑) ---\n",
    "def process_postal_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file, period):\n",
    "    company_name = \"邮政\"\n",
    "    company_start_time = perf_counter()\n",
    "    # 简化config，只保留当前公司需要的信息\n",
    "    config = {\n",
    "        # 【无需改动】猪猪云文件名已按period读取，符合要求\n",
    "        \"zhuzhu_filename\": f\"邮政_{period}.xlsx\",\n",
    "        \"internal_name\": \"邮政国内小包\",  # 用于匹配profile\n",
    "        \"anjian_map_key\": \"ZGYZ\",  # 用于匹配安监数据中的公司名\n",
    "        \"parser\": parse_logistics_events_postal_perfected,\n",
    "    }\n",
    "\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(\n",
    "                base_data_file,\n",
    "                sheet_name=\"city_hierarchy\",\n",
    "                dtype={\"Province\": str, \"City\": str, \"District\": str},\n",
    "            )\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [\n",
    "                    a for a in aliases if pd.notna(a) and a != \"\"\n",
    "                ]\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])  # '202506' -> 6, 以匹配 '6月' 而非 '06月'\n",
    "    anjian_filename = f\"{year}年{month}月{company_name}抽样.xlsx\"\n",
    "    anjian_file_path = anjian_dir / anjian_filename\n",
    "\n",
    "    if not anjian_file_path.exists():\n",
    "        print(f\"[ERROR] 安监数据文件未找到，流程中止: {anjian_file_path.name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"  -> 正在读取安监数据: {anjian_file_path.name}\")\n",
    "    try:\n",
    "        all_anjian_df = pd.read_excel(\n",
    "            anjian_file_path, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 读取安监文件 '{anjian_filename}' 时出错: {e}\")\n",
    "        return\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    # 【新增】准备安监数据源，并为时间戳列添加 \"_anjian\" 后缀\n",
    "    df_anjian_postal = all_anjian_df[\n",
    "        all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]\n",
    "    ].copy()\n",
    "    if not df_anjian_postal.empty:\n",
    "        print(\n",
    "            f\"  -> 已从安监数据中筛选出 {len(df_anjian_postal)} 条 {company_name} (ZGYZ) 记录。\"\n",
    "        )\n",
    "        df_anjian_postal.rename(\n",
    "            columns={\n",
    "                \"揽收时间\": \"揽收时间_anjian\",\n",
    "                \"离开寄件城市时间\": \"离开寄件城市时间_anjian\",\n",
    "                \"到达收件城市时间\": \"到达收件城市时间_anjian\",\n",
    "                \"派送时间\": \"派送时间_anjian\",\n",
    "                \"签收时间\": \"签收时间_anjian\",\n",
    "                \"企业\": \"企业_anjian_raw\",  # 临时重命名以避免合并冲突\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    # --- 猪猪云数据处理 (保持不变) ---\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_company_zhu[\"企业\"] = company_name  # 将企业名统一为最终要显示的名称\n",
    "\n",
    "    # 为了让解析器获得城市信息，需要从安监数据中合并\n",
    "    # 仅合并基础信息，不合并时间戳\n",
    "    base_info_df = (\n",
    "        all_anjian_df[all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]][\n",
    "            [\"单号\", \"寄出城市\", \"寄达城市\"]\n",
    "        ]\n",
    "        .copy()\n",
    "        .drop_duplicates(subset=[\"单号\"])\n",
    "    )\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=\"单号\", how=\"left\")\n",
    "\n",
    "    base_profile = COMPANY_PROFILES_POSTAL[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_POSTAL.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_zhuzhuyun_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    # 【新增】整合两大数据源\n",
    "    zhuzhuyun_cols_to_keep = [\n",
    "        \"单号\",\n",
    "        \"企业\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"完整物流信息\",\n",
    "    ] + [col for col in df_zhuzhuyun_final if \"_zhuzhuyun\" in col]\n",
    "    df_zhuzhuyun_subset = df_zhuzhuyun_final[zhuzhuyun_cols_to_keep]\n",
    "\n",
    "    anjian_cols_to_keep = [\"单号\"] + [\n",
    "        col for col in df_anjian_postal if \"_anjian\" in col\n",
    "    ]\n",
    "    df_anjian_subset = df_anjian_postal[anjian_cols_to_keep]\n",
    "\n",
    "    # 执行合并\n",
    "    df_final = pd.merge(df_zhuzhuyun_subset, df_anjian_subset, on=\"单号\", how=\"outer\")\n",
    "    # 合并后，企业名称可能来自猪猪云（已统一）或为空（安监独有），统一设置为最终名称\n",
    "    df_final[\"企业\"] = company_name\n",
    "    print(f\"  -> 已整合安监与猪猪云数据，共 {len(df_final)} 条唯一记录。\")\n",
    "    print(\"  -> 根据全局配置选择时间戳来源...\")\n",
    "    source_config = DATA_SOURCE_CONFIG.get(company_name, {})\n",
    "    if not source_config:\n",
    "        raise ValueError(\n",
    "            f\"错误: 在 DATA_SOURCE_CONFIG 中未找到 '{company_name}' 的配置!\"\n",
    "        )\n",
    "\n",
    "    timestamp_map = {\n",
    "        \"揽收时间\": \"collection_time_source\",\n",
    "        \"离开寄件城市时间\": \"departure_time_source\",\n",
    "        \"到达收件城市时间\": \"arrival_time_source\",\n",
    "        \"派送时间\": \"dispatch_time_source\",\n",
    "        \"签收时间\": \"signed_time_source\",\n",
    "    }\n",
    "\n",
    "    for final_col, config_key in timestamp_map.items():\n",
    "        source_type = source_config.get(config_key, \"anjian\")\n",
    "        primary_col = f\"{final_col}_{source_type}\"\n",
    "        fallback_col = (\n",
    "            f\"{final_col}_{'zhuzhuyun' if source_type == 'anjian' else 'anjian'}\"\n",
    "        )\n",
    "\n",
    "        df_final[final_col] = df_final.get(primary_col)\n",
    "        if fallback_col in df_final:\n",
    "            df_final[final_col] = df_final[final_col].fillna(df_final[fallback_col])\n",
    "        print(f\"     - '{final_col}' 已配置使用 '{source_type}' 数据源。\")\n",
    "\n",
    "    df_final[\"到达分拣中心时间\"] = df_final.get(\"到达分拣中心时间_zhuzhuyun\")\n",
    "    df_final[\"离开收件城市分拣中心时间\"] = df_final.get(\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "    )\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    # --- 后续处理与保存 (保持不变) ---\n",
    "    if df_base_subset is not None:\n",
    "        # 填充可能因 outer merge 产生的缺失城市信息\n",
    "        if \"寄出城市_x\" in df_final.columns and \"寄出城市_y\" in df_final.columns:\n",
    "            df_final[\"寄出城市\"] = df_final[\"寄出城市_x\"].fillna(df_final[\"寄出城市_y\"])\n",
    "            df_final[\"寄达城市\"] = df_final[\"寄达城市_x\"].fillna(df_final[\"寄达城市_y\"])\n",
    "\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 邮政 (保持不变) ---\n",
    "if \"period\" in locals() and \"DATA_SOURCE_CONFIG\" in locals():\n",
    "    process_postal_data(\n",
    "        zhuzhuyun_merge_path,\n",
    "        anjian_data_path,\n",
    "        pycharm_input_path,\n",
    "        base_data_path,\n",
    "        period,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[警告] 未定义 'period' 或 'DATA_SOURCE_CONFIG' 变量，跳过 process_postal_data 的执行。请在前面的单元格中定义这些变量。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fbe4632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 京东 ---\n",
      "  -> 正在安监数据目录中搜索匹配 '2025年6月*.xlsx' 的文件...\n",
      "  -> 找到 10 个匹配的安监文件，准备合并。\n",
      "  -> 使用 'parse_logistics_events_jd' 解析器通过 .apply() 运行...\n",
      "  -> 已整合安监与猪猪云数据，共 82770 条唯一记录。\n",
      "  -> 已从最终结果中删除 3082 行缺少'完整物流信息'的数据。\n",
      "  -> 根据全局配置选择时间戳来源...\n",
      "     - '揽收时间' 已配置使用 'anjian' 数据源。\n",
      "     - '离开寄件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '到达收件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '派送时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "     - '签收时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "✅ 京东 处理完成，耗时 92.07 秒。文件已保存至: 京东_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.3: 京东logistics数据提取\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 京东 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 (保持不变) ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 京东 (保持不变) ---\n",
    "COMPANY_PROFILES_JD = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件|揽收完成\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心|分拣中心|接货仓\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心|项目营业点|校园服务站|接驳点\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点|已送达至|已送至|校园服务站|快递柜|云柜|丰巢柜|便民驿站\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收|已送达至|已由.*?代收|已由.*?签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收|已送达\",\n",
    "        \"sign_ignore\": r\"完成取件\",\n",
    "    },\n",
    "    \"京东\": {},\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 京东 (保持不变) ---\n",
    "def parse_logistics_events_jd(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    time_cols = [\n",
    "        \"揽收时间_zhuzhuyun\",\n",
    "        \"离开寄件城市时间_zhuzhuyun\",\n",
    "        \"到达收件城市时间_zhuzhuyun\",\n",
    "        \"派送时间_zhuzhuyun\",\n",
    "        \"签收时间_zhuzhuyun\",\n",
    "        \"到达分拣中心时间_zhuzhuyun\",\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    kw = {\n",
    "        key: profile.get(key)\n",
    "        for key in [\n",
    "            \"collect\",\n",
    "            \"delivery\",\n",
    "            \"leave\",\n",
    "            \"arrive\",\n",
    "            \"center\",\n",
    "            \"exclude\",\n",
    "            \"sign_p1\",\n",
    "            \"sign_p2\",\n",
    "            \"sign_fallback\",\n",
    "            \"sign_ignore\",\n",
    "        ]\n",
    "    }\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            location_in_brackets = re.search(r\"【(.*?)】\", line)\n",
    "            is_center = False\n",
    "            location_name = \"\"\n",
    "            if location_in_brackets:\n",
    "                location_name = location_in_brackets.group(1)\n",
    "                if kw[\"center\"] and re.search(kw[\"center\"], location_name):\n",
    "                    if not (kw[\"exclude\"] and re.search(kw[\"exclude\"], location_name)):\n",
    "                        is_center = True\n",
    "            all_events.append(\n",
    "                {\n",
    "                    \"dt\": pd.to_datetime(match.group(1), errors=\"coerce\"),\n",
    "                    \"line\": line,\n",
    "                    \"is_center\": is_center,\n",
    "                    \"location\": location_name,\n",
    "                }\n",
    "            )\n",
    "    all_events = sorted(\n",
    "        [e for e in all_events if pd.notna(e[\"dt\"])], key=lambda x: x[\"dt\"]\n",
    "    )\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str):\n",
    "            return None\n",
    "        aliases = set(\n",
    "            location_maps[\"city_alias_map\"].get(city_key, [city_key.replace(\"市\", \"\")])\n",
    "        )\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        if province:\n",
    "            aliases.add(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "            if location_maps.get(\"province_capital_map\", {}).get(province) == city_key:\n",
    "                aliases.add(province.replace(\"省\", \"\"))\n",
    "        return \"|\".join(map(re.escape, {a for a in aliases if a}))\n",
    "\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "    for event in all_events:\n",
    "        if kw[\"collect\"] and re.search(kw[\"collect\"], event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    if pd.isna(extracted_times[\"揽收时间_zhuzhuyun\"]):\n",
    "        extracted_times[\"揽收时间_zhuzhuyun\"] = all_events[0][\"dt\"]\n",
    "    for event in all_events:\n",
    "        if kw[\"delivery\"] and re.search(kw[\"delivery\"], event[\"line\"]):\n",
    "            extracted_times[\"派送时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    for p_key in [\"sign_p1\", \"sign_p2\", \"sign_fallback\"]:\n",
    "        if pd.notna(extracted_times[\"签收时间_zhuzhuyun\"]):\n",
    "            break\n",
    "        if kw[p_key]:\n",
    "            for event in reversed(all_events):\n",
    "                if kw[\"sign_ignore\"] and re.search(kw[\"sign_ignore\"], event[\"line\"]):\n",
    "                    continue\n",
    "                if re.search(kw[p_key], event[\"line\"]):\n",
    "                    extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                    break\n",
    "    if pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"]) and sender_pattern:\n",
    "        origin_center_events = [\n",
    "            e\n",
    "            for e in all_events\n",
    "            if e[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"]\n",
    "            and e[\"is_center\"]\n",
    "            and re.search(sender_pattern, e[\"location\"])\n",
    "        ]\n",
    "        for event in reversed(origin_center_events):\n",
    "            if kw[\"leave\"] and re.search(kw[\"leave\"], event[\"line\"]):\n",
    "                match = re.search(r\"发往【(.*?)】\", event[\"line\"])\n",
    "                if match:\n",
    "                    destination_of_leave = match.group(1)\n",
    "                    if not re.search(sender_pattern, destination_of_leave):\n",
    "                        extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                        location_match = re.search(r\"【(.*?)】\", event[\"line\"])\n",
    "                        if location_match:\n",
    "                            location_name = re.escape(\n",
    "                                location_match.group(1).split(\"】\")[0]\n",
    "                            )\n",
    "                            for arr_event in origin_center_events:\n",
    "                                if (\n",
    "                                    arr_event[\"dt\"] <= event[\"dt\"]\n",
    "                                    and re.search(location_name, arr_event[\"line\"])\n",
    "                                    and re.search(kw[\"arrive\"], arr_event[\"line\"])\n",
    "                                ):\n",
    "                                    extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = (\n",
    "                                        arr_event[\"dt\"]\n",
    "                                    )\n",
    "                                    break\n",
    "                        break\n",
    "    if dest_pattern:\n",
    "        for event in all_events:\n",
    "            if (\n",
    "                not event[\"is_center\"]\n",
    "                and kw[\"arrive\"]\n",
    "                and re.search(kw[\"arrive\"], event[\"line\"])\n",
    "            ):\n",
    "                location_name = event[\"location\"].replace(\"市\", \"\")\n",
    "                if dest_city_key and location_name == dest_city_key.replace(\"市\", \"\"):\n",
    "                    extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                    break\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zhuzhuyun\"]):\n",
    "            for event in all_events:\n",
    "                if (\n",
    "                    event[\"is_center\"]\n",
    "                    and kw[\"arrive\"]\n",
    "                    and re.search(kw[\"arrive\"], event[\"line\"])\n",
    "                    and re.search(dest_pattern, event[\"location\"])\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                    break\n",
    "        if pd.notna(extracted_times[\"派送时间_zhuzhuyun\"]):\n",
    "            dest_center_events = [\n",
    "                e\n",
    "                for e in all_events\n",
    "                if e[\"dt\"] < extracted_times[\"派送时间_zhuzhuyun\"]\n",
    "                and e[\"is_center\"]\n",
    "                and re.search(dest_pattern, e[\"location\"])\n",
    "            ]\n",
    "            for event in dest_center_events:\n",
    "                if (\n",
    "                    pd.notna(extracted_times[\"到达收件城市时间_zhuzhuyun\"])\n",
    "                    and event[\"dt\"] < extracted_times[\"到达收件城市时间_zhuzhuyun\"]\n",
    "                ):\n",
    "                    continue\n",
    "                if kw[\"leave\"] and re.search(kw[\"leave\"], event[\"line\"]):\n",
    "                    extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                    break\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 京东 ---\n",
    "def process_jd_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file, period):\n",
    "    company_name = \"京东\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": f\"京东_{period}.xlsx\",\n",
    "        \"internal_name\": \"京东\",\n",
    "        \"anjian_map_key\": \"JBD\",\n",
    "        \"parser\": parse_logistics_events_jd,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "        \"province_capital_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            excel_file = pd.ExcelFile(base_data_file)\n",
    "            df_hierarchy = pd.read_excel(excel_file, sheet_name=\"city_hierarchy\")\n",
    "            df_hierarchy_no_na = df_hierarchy.dropna(subset=[\"Province\", \"City\"])\n",
    "            df_hierarchy_no_na[\"District\"] = df_hierarchy_no_na[\"District\"].fillna(\"\")\n",
    "            df_hierarchy_no_na[\"City_clean\"] = df_hierarchy_no_na[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy_no_na[\"District_clean\"] = df_hierarchy_no_na[\n",
    "                \"District\"\n",
    "            ].str.replace(\"市\", \"\", regex=False)\n",
    "            for city, group in df_hierarchy_no_na.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "            city_to_province = df_hierarchy_no_na.drop_duplicates(\"City\")[\n",
    "                [\"City\", \"Province\"]\n",
    "            ].set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = (\n",
    "                    df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                    .copy()\n",
    "                    .drop_duplicates(subset=[\"Province\"])\n",
    "                )\n",
    "                location_maps[\"province_capital_map\"] = df_capitals.set_index(\n",
    "                    \"Province\"\n",
    "                )[\"City\"].to_dict()\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    year = period[:4]\n",
    "    month = period[4:].lstrip(\"0\")\n",
    "    anjian_search_pattern = f\"{year}年{month}月*.xlsx\"\n",
    "    print(f\"  -> 正在安监数据目录中搜索匹配 '{anjian_search_pattern}' 的文件...\")\n",
    "    all_anjian_files = [\n",
    "        f for f in anjian_dir.glob(anjian_search_pattern) if not f.name.startswith(\"~$\")\n",
    "    ]\n",
    "    if not all_anjian_files:\n",
    "        print(\n",
    "            f\"[ERROR] 在目录 {anjian_dir} 中未找到与 '{anjian_search_pattern}' 匹配的安监数据Excel文件，处理中断。\"\n",
    "        )\n",
    "        return\n",
    "    print(f\"  -> 找到 {len(all_anjian_files)} 个匹配的安监文件，准备合并。\")\n",
    "    all_anjian_df = pd.concat(\n",
    "        [pd.read_excel(f, dtype={\"单号\": str}) for f in all_anjian_files],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    base_info_df = all_anjian_df[all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]][\n",
    "        [\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]\n",
    "    ].drop_duplicates(\"单号\")\n",
    "    base_info_df[\"企业\"] = config[\"internal_name\"]\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "\n",
    "    base_profile = COMPANY_PROFILES_JD[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_JD.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_zhuzhuyun_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    df_anjian_jd = all_anjian_df[\n",
    "        all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]\n",
    "    ].copy()\n",
    "    df_anjian_jd.rename(\n",
    "        columns={\n",
    "            \"揽收时间\": \"揽收时间_anjian\",\n",
    "            \"离开寄件城市时间\": \"离开寄件城市时间_anjian\",\n",
    "            \"到达收件城市时间\": \"到达收件城市时间_anjian\",\n",
    "            \"派送时间\": \"派送时间_anjian\",\n",
    "            \"签收时间\": \"签收时间_anjian\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df_anjian_jd[\"企业\"] = config[\"internal_name\"]\n",
    "\n",
    "    zhuzhuyun_cols_to_keep = [\"单号\", \"企业\", \"完整物流信息\"] + [\n",
    "        col for col in df_zhuzhuyun_final if \"_zhuzhuyun\" in col\n",
    "    ]\n",
    "    anjian_cols_to_keep = [\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"] + [\n",
    "        col for col in df_anjian_jd if \"_anjian\" in col\n",
    "    ]\n",
    "    df_final = pd.merge(\n",
    "        df_zhuzhuyun_final[zhuzhuyun_cols_to_keep],\n",
    "        df_anjian_jd[anjian_cols_to_keep],\n",
    "        on=[\"单号\", \"企业\"],\n",
    "        how=\"outer\",\n",
    "    )\n",
    "    print(f\"  -> 已整合安监与猪猪云数据，共 {len(df_final)} 条唯一记录。\")\n",
    "\n",
    "    # --- 【新增】在最终合并后，删除缺少“完整物流信息”的行 ---\n",
    "    initial_final_rows = len(df_final)\n",
    "    df_final.dropna(subset=[\"完整物流信息\"], inplace=True)\n",
    "    rows_dropped_final = initial_final_rows - len(df_final)\n",
    "    if rows_dropped_final > 0:\n",
    "        print(\n",
    "            f\"  -> 已从最终结果中删除 {rows_dropped_final} 行缺少'完整物流信息'的数据。\"\n",
    "        )\n",
    "    # --- 新增结束 ---\n",
    "\n",
    "    if \"寄出城市_x\" in df_final.columns:\n",
    "        df_final[\"寄出城市\"] = df_final[\"寄出城市_y\"].fillna(df_final[\"寄出城市_x\"])\n",
    "        df_final[\"寄达城市\"] = df_final[\"寄达城市_y\"].fillna(df_final[\"寄达城市_x\"])\n",
    "        df_final.drop(\n",
    "            columns=[\"寄出城市_x\", \"寄出城市_y\", \"寄达城市_x\", \"寄达城市_y\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "        print(\"  -> 已合并城市信息，安监数据优先。\")\n",
    "\n",
    "    print(\"  -> 根据全局配置选择时间戳来源...\")\n",
    "    source_config = DATA_SOURCE_CONFIG.get(company_name, {})\n",
    "    if not source_config:\n",
    "        raise ValueError(\n",
    "            f\"错误: 在 DATA_SOURCE_CONFIG 中未找到 '{company_name}' 的配置!\"\n",
    "        )\n",
    "    timestamp_map = {\n",
    "        \"揽收时间\": \"collection_time_source\",\n",
    "        \"离开寄件城市时间\": \"departure_time_source\",\n",
    "        \"到达收件城市时间\": \"arrival_time_source\",\n",
    "        \"派送时间\": \"dispatch_time_source\",\n",
    "        \"签收时间\": \"signed_time_source\",\n",
    "    }\n",
    "    for final_col, config_key in timestamp_map.items():\n",
    "        source_type = source_config.get(config_key, \"anjian\")\n",
    "        primary_col = f\"{final_col}_{source_type}\"\n",
    "        fallback_col = (\n",
    "            f\"{final_col}_{'zhuzhuyun' if source_type == 'anjian' else 'anjian'}\"\n",
    "        )\n",
    "        df_final[final_col] = df_final.get(primary_col)\n",
    "        if fallback_col in df_final:\n",
    "            df_final[final_col] = df_final[final_col].fillna(df_final[fallback_col])\n",
    "        print(f\"     - '{final_col}' 已配置使用 '{source_type}' 数据源。\")\n",
    "    df_final[\"到达分拣中心时间\"] = df_final.get(\"到达分拣中心时间_zhuzhuyun\")\n",
    "    df_final[\"离开收件城市分拣中心时间\"] = df_final.get(\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "    )\n",
    "\n",
    "    if df_base_subset is not None:\n",
    "        df_final[\"寄出城市\"] = df_final[\"寄出城市\"].astype(str)\n",
    "        df_final[\"寄达城市\"] = df_final[\"寄达城市\"].astype(str)\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 京东 ---\n",
    "if \"period\" in locals() and \"DATA_SOURCE_CONFIG\" in locals():\n",
    "    process_jd_data(\n",
    "        zhuzhuyun_merge_path,\n",
    "        anjian_data_path,\n",
    "        pycharm_input_path,\n",
    "        base_data_path,\n",
    "        period,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[警告] 未定义 'period' 或 'DATA_SOURCE_CONFIG' 变量，跳过 process_jd_data 的执行。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "167fcb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 极兔 ---\n",
      "  -> 正在读取安监数据: 2025年6月极兔抽样.xlsx\n",
      "  -> 已从安监数据中筛选出 81703 条 极兔 (JT) 记录。\n",
      "  -> 使用 'parse_logistics_events_jitu' 解析器通过 .apply() 运行...\n",
      "  -> 已整合安监与猪猪云数据，共 81703 条唯一记录。\n",
      "  -> 根据全局配置选择时间戳来源...\n",
      "    - '揽收时间' 已配置使用 'anjian' 数据源。\n",
      "    - '离开寄件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '到达收件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '派送时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '签收时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "✅ 极兔 处理完成，耗时 52.73 秒。文件已保存至: 极兔_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.4: 极兔logistics数据提取 (已修改)\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 极兔 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 (保持不变) ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"通用行应用提取器\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 极兔 (规则已完善) (保持不变) ---\n",
    "COMPANY_PROFILES_JITU = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心|分拣中心\",\n",
    "        \"sign_p1\": r\"已暂存至|已投至|已到站|自提柜|智能柜|菜鸟驿站|快递超市|代收点|存放.*(快递柜|驿站|自提点)\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收|已上门\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收|已投递\",\n",
    "    },\n",
    "    \"极兔\": {\n",
    "        \"center\": r\"转运中心|集散中心|集散点\",\n",
    "        \"center_exclude\": r\"公司|分部|服务部|网格仓|揽投部|营业部|经营分部|网点|集货点|营销中心|站点\",\n",
    "        \"dest_arrival_exclude\": r\"服务部|网点\",\n",
    "        \"sign_p1\": r\"已存放至.*?【|已送达.*?【|暂由.*?代为保管\",\n",
    "        \"sign_p2\": r\"已按址投递|已由本人签收|客户签收\",\n",
    "        \"sign_fallback\": r\"快件已签收|快件已按约定投递\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 极兔 (已应用最终修复) (保持不变, 仅为统一命名规范修改后缀) ---\n",
    "def parse_logistics_events_jitu(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    解析极兔的物流事件，并根据预设原则提取关键时间点。\n",
    "    \"\"\"\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    # 【改动】为统一命名规范，将列后缀从 _zzy 改为 _zhuzhuyun\n",
    "    time_cols = [\n",
    "        \"揽收时间_zhuzhuyun\",\n",
    "        \"离开寄件城市时间_zhuzhuyun\",\n",
    "        \"到达收件城市时间_zhuzhuyun\",\n",
    "        \"派送时间_zhuzhuyun\",\n",
    "        \"签收时间_zhuzhuyun\",\n",
    "        \"到达分拣中心时间_zhuzhuyun\",\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    # --- 您其他的解析器逻辑保持完全不变 ---\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave, kw_arrive = profile.get(\"leave\"), profile.get(\"arrive\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_all_centers = profile.get(\"center\")\n",
    "    kw_center_exclude = profile.get(\"center_exclude\", \"\")\n",
    "    kw_dest_arrival_exclude = profile.get(\"dest_arrival_exclude\", \"\")\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str) or pd.isna(city_key):\n",
    "            return None\n",
    "        aliases = location_maps.get(\"city_alias_map\", {}).get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        clean_aliases = {\n",
    "            str(a)\n",
    "            for a in aliases\n",
    "            if a is not None and pd.notna(a) and str(a).strip() != \"\"\n",
    "        }\n",
    "        if not clean_aliases:\n",
    "            return None\n",
    "        return \"|\".join(map(re.escape, clean_aliases))\n",
    "\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    for event in all_events:\n",
    "        if kw_delivery and re.search(kw_delivery, event[\"line\"]):\n",
    "            extracted_times[\"派送时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    p1_event, p2_event, p3_event = None, None, None\n",
    "    for event in all_events:\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p1 and re.search(kw_sign_p1, event[\"line\"]):\n",
    "            p1_event = event\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p2 and re.search(kw_sign_p2, event[\"line\"]):\n",
    "            p2_event = event\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_fallback and re.search(kw_sign_fallback, event[\"line\"]):\n",
    "            p3_event = event\n",
    "            break\n",
    "    if p1_event:\n",
    "        extracted_times[\"签收时间_zhuzhuyun\"] = p1_event[\"dt\"]\n",
    "    elif p2_event:\n",
    "        extracted_times[\"签收时间_zhuzhuyun\"] = p2_event[\"dt\"]\n",
    "    elif p3_event:\n",
    "        extracted_times[\"签收时间_zhuzhuyun\"] = p3_event[\"dt\"]\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "    if (\n",
    "        pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"])\n",
    "        and sender_pattern\n",
    "        and kw_all_centers\n",
    "    ):\n",
    "        last_sender_leave_event = None\n",
    "        last_sender_center_arrival = None\n",
    "        for event in all_events:\n",
    "            if event[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"]:\n",
    "                action_part = re.split(r\"；|已发往|发往\", event[\"line\"])[0]\n",
    "                is_real_center = re.search(kw_all_centers, action_part) and not (\n",
    "                    kw_center_exclude and re.search(kw_center_exclude, action_part)\n",
    "                )\n",
    "                if (\n",
    "                    kw_leave\n",
    "                    and re.search(kw_leave, action_part)\n",
    "                    and re.search(sender_pattern, action_part)\n",
    "                    and is_real_center\n",
    "                ):\n",
    "                    last_sender_leave_event = event\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, action_part)\n",
    "                    and re.search(sender_pattern, action_part)\n",
    "                    and is_real_center\n",
    "                ):\n",
    "                    last_sender_center_arrival = event\n",
    "        if last_sender_leave_event:\n",
    "            extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = last_sender_leave_event[\n",
    "                \"dt\"\n",
    "            ]\n",
    "        if last_sender_center_arrival:\n",
    "            extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = last_sender_center_arrival[\n",
    "                \"dt\"\n",
    "            ]\n",
    "    if dest_pattern:\n",
    "        for event in all_events:\n",
    "            action_part = re.split(r\"；|已发往|发往\", event[\"line\"])[0]\n",
    "            if (\n",
    "                kw_arrive\n",
    "                and re.search(kw_arrive, action_part)\n",
    "                and re.search(dest_pattern, action_part)\n",
    "            ):\n",
    "                if not (\n",
    "                    kw_dest_arrival_exclude\n",
    "                    and re.search(kw_dest_arrival_exclude, action_part)\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                    break\n",
    "        if pd.notna(extracted_times[\"派送时间_zhuzhuyun\"]):\n",
    "            search_end_time = extracted_times[\"派送时间_zhuzhuyun\"]\n",
    "            for event in reversed(all_events):\n",
    "                if event[\"dt\"] < search_end_time:\n",
    "                    action_part = re.split(r\"；|已发往|发往\", event[\"line\"])[0]\n",
    "                    is_real_center = re.search(kw_all_centers, action_part) and not (\n",
    "                        kw_center_exclude and re.search(kw_center_exclude, action_part)\n",
    "                    )\n",
    "                    if (\n",
    "                        kw_leave\n",
    "                        and re.search(kw_leave, action_part)\n",
    "                        and re.search(dest_pattern, action_part)\n",
    "                        and is_real_center\n",
    "                    ):\n",
    "                        extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"] = event[\n",
    "                            \"dt\"\n",
    "                        ]\n",
    "                        break\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 极兔 (已修改数据读取逻辑) ---\n",
    "def process_jitu_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file, period):\n",
    "    \"\"\"主流程函数，用于处理极兔快递的数据\"\"\"\n",
    "    company_name = \"极兔\"\n",
    "    company_start_time = perf_counter()\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": f\"极兔_{period}.xlsx\",\n",
    "        \"internal_name\": \"极兔\",\n",
    "        \"anjian_map_key\": \"JT\",\n",
    "        \"parser\": parse_logistics_events_jitu,\n",
    "    }\n",
    "    location_maps = {}\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(\n",
    "                base_data_file,\n",
    "                sheet_name=\"city_hierarchy\",\n",
    "                dtype={\"Province\": str, \"City\": str, \"District\": str},\n",
    "            )\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                r\"区|县|自治州\", \"\", regex=True\n",
    "            )\n",
    "            location_maps[\"city_alias_map\"] = {\n",
    "                city: [group.iloc[0][\"City_clean\"]]\n",
    "                + group[\"District_clean\"].unique().tolist()\n",
    "                for city, group in df_hierarchy.groupby(\"City\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])  # '202506' -> 6, 以匹配 '6月' 而非 '06月'\n",
    "    anjian_filename = f\"{year}年{month}月{company_name}抽样.xlsx\"\n",
    "    anjian_file_path = anjian_dir / anjian_filename\n",
    "\n",
    "    if not anjian_file_path.exists():\n",
    "        print(f\"[ERROR] 安监数据文件未找到，流程中止: {anjian_file_path.name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"  -> 正在读取安监数据: {anjian_file_path.name}\")\n",
    "    try:\n",
    "        all_anjian_df = pd.read_excel(\n",
    "            anjian_file_path, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 读取安监文件 '{anjian_filename}' 时出错: {e}\")\n",
    "        return\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    # 【新增】准备安监数据源\n",
    "    df_anjian_jitu = all_anjian_df[\n",
    "        all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]\n",
    "    ].copy()\n",
    "    if not df_anjian_jitu.empty:\n",
    "        print(\n",
    "            f\"  -> 已从安监数据中筛选出 {len(df_anjian_jitu)} 条 {company_name} (JT) 记录。\"\n",
    "        )\n",
    "        df_anjian_jitu.rename(\n",
    "            columns={\n",
    "                \"揽收时间\": \"揽收时间_anjian\",\n",
    "                \"离开寄件城市时间\": \"离开寄件城市时间_anjian\",\n",
    "                \"到达收件城市时间\": \"到达收件城市时间_anjian\",\n",
    "                \"派送时间\": \"派送时间_anjian\",\n",
    "                \"签收时间\": \"签收时间_anjian\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        # 统一安监数据中的企业名称，以便后续合并\n",
    "        df_anjian_jitu[\"企业\"] = config[\"internal_name\"]\n",
    "\n",
    "    # --- 猪猪云数据处理 (保持不变) ---\n",
    "    company_file = zhuzhuyun_merge_path / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(columns={\"快递单号\": \"单号\"}, inplace=True)\n",
    "    df_company_zhu[\"企业\"] = config[\"internal_name\"]\n",
    "    df_company_zhu[\"单号\"] = df_company_zhu[\"单号\"].str.strip()\n",
    "    base_info_df = all_anjian_df[all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]][\n",
    "        [\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]\n",
    "    ].drop_duplicates(\"单号\")\n",
    "    base_info_df[\"企业\"] = config[\"internal_name\"]  # 统一企业名\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_JITU[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_JITU.get(config[\"internal_name\"], {})\n",
    "    profile = base_profile.copy()\n",
    "    for key, value in company_specific_profile.items():\n",
    "        if (\n",
    "            key in profile\n",
    "            and value\n",
    "            and key not in [\"center_exclude\", \"dest_arrival_exclude\"]\n",
    "        ):\n",
    "            profile[key] = f\"{value}|{profile[key]}\"\n",
    "        elif value:\n",
    "            profile[key] = value\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_zhuzhuyun_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    # 【新增】整合两大数据源\n",
    "    zhuzhuyun_cols_to_keep = [\n",
    "        \"单号\",\n",
    "        \"企业\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"完整物流信息\",\n",
    "    ] + [col for col in df_zhuzhuyun_final if \"_zhuzhuyun\" in col]\n",
    "    df_zhuzhuyun_subset = df_zhuzhuyun_final[zhuzhuyun_cols_to_keep]\n",
    "    anjian_cols_to_keep = [\"单号\", \"企业\"] + [\n",
    "        col for col in df_anjian_jitu if \"_anjian\" in col\n",
    "    ]\n",
    "    df_anjian_subset = df_anjian_jitu[anjian_cols_to_keep]\n",
    "    df_final = pd.merge(\n",
    "        df_zhuzhuyun_subset, df_anjian_subset, on=[\"单号\", \"企业\"], how=\"outer\"\n",
    "    )\n",
    "    print(f\"  -> 已整合安监与猪猪云数据，共 {len(df_final)} 条唯一记录。\")\n",
    "\n",
    "    print(\"  -> 根据全局配置选择时间戳来源...\")\n",
    "    source_config = DATA_SOURCE_CONFIG.get(company_name, {})\n",
    "    if not source_config:\n",
    "        raise ValueError(\n",
    "            f\"错误: 在 DATA_SOURCE_CONFIG 中未找到 '{company_name}' 的配置!\"\n",
    "        )\n",
    "    timestamp_map = {\n",
    "        \"揽收时间\": \"collection_time_source\",\n",
    "        \"离开寄件城市时间\": \"departure_time_source\",\n",
    "        \"到达收件城市时间\": \"arrival_time_source\",\n",
    "        \"派送时间\": \"dispatch_time_source\",\n",
    "        \"签收时间\": \"signed_time_source\",\n",
    "    }\n",
    "    for final_col, config_key in timestamp_map.items():\n",
    "        source_type = source_config.get(config_key, \"anjian\")\n",
    "        primary_col = f\"{final_col}_{source_type}\"\n",
    "        fallback_col = (\n",
    "            f\"{final_col}_{'zhuzhuyun' if source_type == 'anjian' else 'anjian'}\"\n",
    "        )\n",
    "        df_final[final_col] = df_final.get(primary_col)\n",
    "        if fallback_col in df_final:\n",
    "            df_final[final_col] = df_final[final_col].fillna(df_final[fallback_col])\n",
    "        print(f\"    - '{final_col}' 已配置使用 '{source_type}' 数据源。\")\n",
    "    df_final[\"到达分拣中心时间\"] = df_final.get(\"到达分拣中心时间_zhuzhuyun\")\n",
    "    df_final[\"离开收件城市分拣中心时间\"] = df_final.get(\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "    )\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    # --- 后续处理与保存 (保持不变) ---\n",
    "    if df_base_subset is not None:\n",
    "        # 填充可能因 outer merge 产生的缺失城市信息\n",
    "        if \"寄出城市_x\" in df_final.columns and \"寄出城市_y\" in df_final.columns:\n",
    "            df_final[\"寄出城市\"] = df_final[\"寄出城市_x\"].fillna(df_final[\"寄出城市_y\"])\n",
    "            df_final[\"寄达城市\"] = df_final[\"寄达城市_x\"].fillna(df_final[\"寄达城市_y\"])\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 极兔 (保持不变) ---\n",
    "if \"period\" in locals() and \"DATA_SOURCE_CONFIG\" in locals():\n",
    "    process_jitu_data(\n",
    "        zhuzhuyun_merge_path,\n",
    "        anjian_data_path,\n",
    "        pycharm_input_path,\n",
    "        base_data_path,\n",
    "        period,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[警告] 未定义 'period' 或 'DATA_SOURCE_CONFIG' 变量，跳过 process_jitu_data 的执行。请在前面的单元格中定义这些变量。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2238bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 韵达 ---\n",
      "  -> 正在读取安监数据: 2025年6月韵达抽样.xlsx\n",
      "  -> 已从安监数据中筛选出 81797 条 韵达 (YUNDA) 记录。\n",
      "  -> 使用 'parse_logistics_events_yunda' 解析器通过 .apply() 运行...\n",
      "  -> 已整合安监与猪猪云数据，共 81797 条唯一记录。\n",
      "  -> 根据全局配置选择时间戳来源...\n",
      "    - '揽收时间' 已配置使用 'anjian' 数据源。\n",
      "    - '离开寄件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '到达收件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '派送时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '签收时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "✅ 韵达 处理完成，耗时 51.89 秒。文件已保存至: 韵达_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.5: 韵达logistics数据提取(已修改)\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 韵达 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 (保持不变) ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 韵达 (修正“网格仓”定义) ---\n",
    "COMPANY_PROFILES_YUNDA = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|派件员|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        # <<< 修正点1 >>>：将“网格仓”提升为一种通用的中心类型\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心|网格仓\",\n",
    "        \"sign_p1\": r\"已暂存至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收|已投递\",\n",
    "    },\n",
    "    \"韵达\": {\n",
    "        \"center\": r\"分拨交付中心\",\n",
    "        # <<< 修正点2 >>>：从严格排除列表中移除“网格仓”，因为它现在是中心了\n",
    "        \"center_exclude\": r\"公司|分部|服务部|揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        # 宽松标准：用于识别“到达收件城市”这一首个落地动作\n",
    "        \"dest_arrival_exclude\": r\"服务部\",  # 根据案例，仅排除“服务部”\n",
    "        \"sign_p2\": r\"已送货上门签收|已由邮政派送签收\",\n",
    "        \"sign_fallback\": r\"快件已投递|快件已按址投递\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 韵达 (分场景调用不同排除标准) (保持不变, 仅为统一命名规范修改后缀) ---\n",
    "def parse_logistics_events_yunda(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    # 【改动】为统一命名规范，将列后缀从 _zzy 改为 _zhuzhuyun\n",
    "    time_cols = [\n",
    "        \"揽收时间_zhuzhuyun\",\n",
    "        \"离开寄件城市时间_zhuzhuyun\",\n",
    "        \"到达收件城市时间_zhuzhuyun\",\n",
    "        \"派送时间_zhuzhuyun\",\n",
    "        \"签收时间_zhuzhuyun\",\n",
    "        \"到达分拣中心时间_zhuzhuyun\",\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    # --- 您其他的解析器逻辑保持完全不变 ---\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave, kw_arrive = profile.get(\"leave\"), profile.get(\"arrive\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback, kw_sign_ignore = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "        profile.get(\"sign_ignore\"),\n",
    "    )\n",
    "    kw_all_centers = profile.get(\"center\")\n",
    "    kw_center_exclude = profile.get(\"center_exclude\", \"\")\n",
    "    kw_dest_arrival_exclude = profile.get(\"dest_arrival_exclude\", \"\")\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps):\n",
    "        if not city_key or not isinstance(city_key, str) or pd.isna(city_key):\n",
    "            return None\n",
    "        aliases = location_maps[\"city_alias_map\"].get(\n",
    "            city_key, [city_key.replace(\"市\", \"\")]\n",
    "        )\n",
    "        clean_aliases = {\n",
    "            str(a)\n",
    "            for a in aliases\n",
    "            if a is not None and pd.notna(a) and str(a).strip() != \"\"\n",
    "        }\n",
    "        if not clean_aliases:\n",
    "            return None\n",
    "        return \"|\".join(map(re.escape, clean_aliases))\n",
    "\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    for event in all_events:\n",
    "        if kw_delivery and re.search(kw_delivery, event[\"line\"]):\n",
    "            extracted_times[\"派送时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    p1_event, p2_event, p3_event = None, None, None\n",
    "    for event in all_events:\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p1 and re.search(kw_sign_p1, event[\"line\"]):\n",
    "            p1_event = event\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p2 and re.search(kw_sign_p2, event[\"line\"]):\n",
    "            p2_event = event\n",
    "            break\n",
    "    for event in reversed(all_events):\n",
    "        if kw_sign_ignore and re.search(kw_sign_ignore, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_fallback and re.search(kw_sign_fallback, event[\"line\"]):\n",
    "            p3_event = event\n",
    "            break\n",
    "    if p1_event:\n",
    "        extracted_times[\"签收时间_zhuzhuyun\"] = p1_event[\"dt\"]\n",
    "    elif p2_event:\n",
    "        extracted_times[\"签收时间_zhuzhuyun\"] = p2_event[\"dt\"]\n",
    "    elif p3_event:\n",
    "        extracted_times[\"签收时间_zhuzhuyun\"] = p3_event[\"dt\"]\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    if (\n",
    "        pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"])\n",
    "        and sender_pattern\n",
    "        and kw_all_centers\n",
    "    ):\n",
    "        last_sender_leave_event = None\n",
    "        for event in all_events:\n",
    "            if event[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"]:\n",
    "                action_part = re.split(r\"；|发往\", event[\"line\"])[0]\n",
    "                if (\n",
    "                    kw_leave\n",
    "                    and re.search(kw_leave, action_part)\n",
    "                    and re.search(kw_all_centers, action_part)\n",
    "                    and re.search(sender_pattern, action_part)\n",
    "                    and not (\n",
    "                        kw_center_exclude and re.search(kw_center_exclude, action_part)\n",
    "                    )\n",
    "                ):\n",
    "                    last_sender_leave_event = event\n",
    "        if last_sender_leave_event:\n",
    "            extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = last_sender_leave_event[\n",
    "                \"dt\"\n",
    "            ]\n",
    "        last_sender_center_arrival = None\n",
    "        search_end_time = extracted_times.get(\n",
    "            \"离开寄件城市时间_zhuzhuyun\", all_events[-1][\"dt\"] + pd.Timedelta(seconds=1)\n",
    "        )\n",
    "        for event in all_events:\n",
    "            if not (\n",
    "                extracted_times[\"揽收时间_zhuzhuyun\"] < event[\"dt\"] < search_end_time\n",
    "            ):\n",
    "                continue\n",
    "            action_part = re.split(r\"；|发往\", event[\"line\"])[0]\n",
    "            if (\n",
    "                kw_arrive\n",
    "                and re.search(kw_arrive, action_part)\n",
    "                and re.search(kw_all_centers, action_part)\n",
    "                and re.search(sender_pattern, action_part)\n",
    "                and not (\n",
    "                    kw_center_exclude and re.search(kw_center_exclude, action_part)\n",
    "                )\n",
    "            ):\n",
    "                last_sender_center_arrival = event\n",
    "        if last_sender_center_arrival:\n",
    "            extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = last_sender_center_arrival[\n",
    "                \"dt\"\n",
    "            ]\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "    if dest_pattern:\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zhuzhuyun\"]):\n",
    "            for event in all_events:\n",
    "                action_part = re.split(r\"；|发往\", event[\"line\"])[0]\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, action_part)\n",
    "                    and re.search(dest_pattern, action_part)\n",
    "                ):\n",
    "                    if not (\n",
    "                        kw_dest_arrival_exclude\n",
    "                        and re.search(kw_dest_arrival_exclude, action_part)\n",
    "                    ):\n",
    "                        extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                        break\n",
    "        if pd.notna(extracted_times[\"派送时间_zhuzhuyun\"]):\n",
    "            for event in reversed(all_events):\n",
    "                if event[\"dt\"] < extracted_times[\"派送时间_zhuzhuyun\"]:\n",
    "                    action_part = re.split(r\"；|发往\", event[\"line\"])[0]\n",
    "                    if (\n",
    "                        kw_leave\n",
    "                        and re.search(kw_leave, action_part)\n",
    "                        and kw_all_centers\n",
    "                        and re.search(kw_all_centers, action_part)\n",
    "                        and re.search(dest_pattern, action_part)\n",
    "                        and not (\n",
    "                            kw_center_exclude\n",
    "                            and re.search(kw_center_exclude, action_part)\n",
    "                        )\n",
    "                    ):\n",
    "                        extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"] = event[\n",
    "                            \"dt\"\n",
    "                        ]\n",
    "                        break\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 韵达 (已修改数据读取逻辑) ---\n",
    "def process_yunda_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file, period):\n",
    "    company_name = \"韵达\"\n",
    "    company_start_time = perf_counter()\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": f\"韵达_{period}.xlsx\",\n",
    "        \"internal_name\": \"韵达\",\n",
    "        \"anjian_map_key\": \"YUNDA\",\n",
    "        \"parser\": parse_logistics_events_yunda,\n",
    "    }\n",
    "\n",
    "    location_maps = {}\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(\n",
    "                base_data_file,\n",
    "                sheet_name=\"city_hierarchy\",\n",
    "                dtype={\"Province\": str, \"City\": str, \"District\": str},\n",
    "            )\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            location_maps[\"city_alias_map\"] = {\n",
    "                city: [group.iloc[0][\"City_clean\"]]\n",
    "                + group[\"District_clean\"].unique().tolist()\n",
    "                for city, group in df_hierarchy.groupby(\"City\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])  # '202506' -> 6, 以匹配 '6月' 而非 '06月'\n",
    "    anjian_filename = f\"{year}年{month}月{company_name}抽样.xlsx\"\n",
    "    anjian_file_path = anjian_dir / anjian_filename\n",
    "\n",
    "    if not anjian_file_path.exists():\n",
    "        print(f\"[ERROR] 安监数据文件未找到，流程中止: {anjian_file_path.name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"  -> 正在读取安监数据: {anjian_file_path.name}\")\n",
    "    try:\n",
    "        all_anjian_df = pd.read_excel(\n",
    "            anjian_file_path, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 读取安监文件 '{anjian_filename}' 时出错: {e}\")\n",
    "        return\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    # 【新增】准备安监数据源\n",
    "    df_anjian_yunda = all_anjian_df[\n",
    "        all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]\n",
    "    ].copy()\n",
    "    if not df_anjian_yunda.empty:\n",
    "        print(\n",
    "            f\"  -> 已从安监数据中筛选出 {len(df_anjian_yunda)} 条 {company_name} (YUNDA) 记录。\"\n",
    "        )\n",
    "        df_anjian_yunda.rename(\n",
    "            columns={\n",
    "                \"揽收时间\": \"揽收时间_anjian\",\n",
    "                \"离开寄件城市时间\": \"离开寄件城市时间_anjian\",\n",
    "                \"到达收件城市时间\": \"到达收件城市时间_anjian\",\n",
    "                \"派送时间\": \"派送时间_anjian\",\n",
    "                \"签收时间\": \"签收时间_anjian\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        # 统一安监数据中的企业名称\n",
    "        df_anjian_yunda[\"企业\"] = config[\"internal_name\"]\n",
    "\n",
    "    # --- 猪猪云数据处理 (保持不变) ---\n",
    "    company_file = zhuzhuyun_merge_path / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(columns={\"快递单号\": \"单号\"}, inplace=True)\n",
    "    df_company_zhu[\"企业\"] = config[\"internal_name\"]\n",
    "    df_company_zhu[\"单号\"] = df_company_zhu[\"单号\"].str.strip()\n",
    "    base_info_df = all_anjian_df[all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]][\n",
    "        [\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]\n",
    "    ].drop_duplicates(\"单号\")\n",
    "    base_info_df[\"企业\"] = config[\"internal_name\"]\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_YUNDA[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_YUNDA.get(config[\"internal_name\"], {})\n",
    "    profile = base_profile.copy()\n",
    "    for key, value in company_specific_profile.items():\n",
    "        if (\n",
    "            key in profile\n",
    "            and value\n",
    "            and key not in [\"center_exclude\", \"dest_arrival_exclude\"]\n",
    "        ):\n",
    "            profile[key] = f\"{value}|{profile[key]}\"\n",
    "        elif value:\n",
    "            profile[key] = value\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_zhuzhuyun_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    # 【新增】整合两大数据源\n",
    "    zhuzhuyun_cols_to_keep = [\n",
    "        \"单号\",\n",
    "        \"企业\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"完整物流信息\",\n",
    "    ] + [col for col in df_zhuzhuyun_final if \"_zhuzhuyun\" in col]\n",
    "    df_zhuzhuyun_subset = df_zhuzhuyun_final[zhuzhuyun_cols_to_keep]\n",
    "    anjian_cols_to_keep = [\"单号\", \"企业\"] + [\n",
    "        col for col in df_anjian_yunda if \"_anjian\" in col\n",
    "    ]\n",
    "    df_anjian_subset = df_anjian_yunda[anjian_cols_to_keep]\n",
    "    df_final = pd.merge(\n",
    "        df_zhuzhuyun_subset, df_anjian_subset, on=[\"单号\", \"企业\"], how=\"outer\"\n",
    "    )\n",
    "    print(f\"  -> 已整合安监与猪猪云数据，共 {len(df_final)} 条唯一记录。\")\n",
    "\n",
    "    print(\"  -> 根据全局配置选择时间戳来源...\")\n",
    "    source_config = DATA_SOURCE_CONFIG.get(company_name, {})\n",
    "    if not source_config:\n",
    "        raise ValueError(\n",
    "            f\"错误: 在 DATA_SOURCE_CONFIG 中未找到 '{company_name}' 的配置!\"\n",
    "        )\n",
    "    timestamp_map = {\n",
    "        \"揽收时间\": \"collection_time_source\",\n",
    "        \"离开寄件城市时间\": \"departure_time_source\",\n",
    "        \"到达收件城市时间\": \"arrival_time_source\",\n",
    "        \"派送时间\": \"dispatch_time_source\",\n",
    "        \"签收时间\": \"signed_time_source\",\n",
    "    }\n",
    "    for final_col, config_key in timestamp_map.items():\n",
    "        source_type = source_config.get(config_key, \"anjian\")\n",
    "        primary_col = f\"{final_col}_{source_type}\"\n",
    "        fallback_col = (\n",
    "            f\"{final_col}_{'zhuzhuyun' if source_type == 'anjian' else 'anjian'}\"\n",
    "        )\n",
    "        df_final[final_col] = df_final.get(primary_col)\n",
    "        if fallback_col in df_final:\n",
    "            df_final[final_col] = df_final[final_col].fillna(df_final[fallback_col])\n",
    "        print(f\"    - '{final_col}' 已配置使用 '{source_type}' 数据源。\")\n",
    "    df_final[\"到达分拣中心时间\"] = df_final.get(\"到达分拣中心时间_zhuzhuyun\")\n",
    "    df_final[\"离开收件城市分拣中心时间\"] = df_final.get(\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "    )\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    # --- 后续处理与保存 (保持不变) ---\n",
    "    if df_base_subset is not None:\n",
    "        if \"寄出城市_x\" in df_final.columns and \"寄出城市_y\" in df_final.columns:\n",
    "            df_final[\"寄出城市\"] = df_final[\"寄出城市_x\"].fillna(df_final[\"寄出城市_y\"])\n",
    "            df_final[\"寄达城市\"] = df_final[\"寄达城市_x\"].fillna(df_final[\"寄达城市_y\"])\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 韵达 (保持不变) ---\n",
    "if \"period\" in locals() and \"DATA_SOURCE_CONFIG\" in locals():\n",
    "    process_yunda_data(\n",
    "        zhuzhuyun_merge_path,\n",
    "        anjian_data_path,\n",
    "        pycharm_input_path,\n",
    "        base_data_path,\n",
    "        period,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[警告] 未定义 'period' 或 'DATA_SOURCE_CONFIG' 变量，跳过 process_yunda_data 的执行。请在前面的单元格中定义这些变量。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b9339ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 德邦 ---\n",
      "  -> 正在读取安监数据: 2025年6月德邦抽样.xlsx\n",
      "  -> 已从安监数据中筛选出 78551 条 德邦 (DEPPON) 记录。\n",
      "  -> 使用 'parse_logistics_events_deppon_final_logic' 解析器通过 .apply() 运行...\n",
      "  -> 已整合安监与猪猪云数据，共 78551 条唯一记录。\n",
      "  -> 根据全局配置选择时间戳来源...\n",
      "    - '揽收时间' 已配置使用 'anjian' 数据源。\n",
      "    - '离开寄件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '到达收件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '派送时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '签收时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "✅ 德邦 处理完成，耗时 55.42 秒。文件已保存至: 德邦_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.6: 德邦logistics数据提取\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 德邦 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保路径正确) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "# --- 全局预编译正则表达式 (保持不变) ---\n",
    "DATETIME_CAPTURE_PATTERN = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\")\n",
    "\n",
    "# --- 公司档案: 德邦 (保持不变) ---\n",
    "COMPANY_PROFILES_DEPPON = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|为您派送|正在为您派件|已安排派送\",\n",
    "        \"leave\": r\"离开|已发出|发往|航班起飞\",\n",
    "        \"arrive\": r\"到达|抵达|航班到达\",\n",
    "        \"center\": r\"处理中心|分拨中心|转运场|转运中心|运营区|枢纽中心|集散中心|分拨站|空运总调|机场运作部|运作部|集配站\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点|包裹已存放至\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_p3\": r\"DONT_MATCH_ANYTHING\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "    },\n",
    "    \"德邦\": {\n",
    "        \"sign_p1\": r\"快件已暂存至|包裹已存放至|集运仓签收|菜鸟驿站签收|水电表箱签收|丰巢柜|妈妈驿站\",\n",
    "        \"sign_p2\": r\"家门口签收|本人签收|代收|已由同事签收|收发室签收|前台签收|亲属签收|其他签收|门卫签收|物业签收|正常签收\",\n",
    "        \"sign_p3\": r\"经收货人同意，此件放置在\",\n",
    "        \"exclude\": r\"经营分部\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器辅助函数 (保持不变) ---\n",
    "def get_location_pattern(city_key: Optional[str], location_maps: Dict) -> Optional[str]:\n",
    "    if not city_key or not isinstance(city_key, str):\n",
    "        return None\n",
    "    aliases = set(\n",
    "        location_maps[\"city_alias_map\"].get(city_key, [city_key.replace(\"市\", \"\")])\n",
    "    )\n",
    "    if city_key in location_maps[\"capital_cities_set\"]:\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        if province:\n",
    "            aliases.add(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "    valid_aliases = {alias for alias in aliases if alias}\n",
    "    return \"|\".join(map(re.escape, valid_aliases))\n",
    "\n",
    "\n",
    "# --- 主解析器: 德邦 (最终修正版) (保持不变, 仅为统一命名规范修改后缀) ---\n",
    "def parse_logistics_events_deppon_final_logic(\n",
    "    row: pd.Series, profile: Dict, location_maps: Dict\n",
    ") -> pd.Series:\n",
    "    log_text, sender_city, dest_city = (\n",
    "        row[\"完整物流信息\"],\n",
    "        row[\"寄出城市\"],\n",
    "        row[\"寄达城市\"],\n",
    "    )\n",
    "    # 【改动】为统一命名规范，将列后缀从 _zzy 改为 _zhuzhuyun\n",
    "    time_cols = [\n",
    "        \"揽收时间_zhuzhuyun\",\n",
    "        \"离开寄件城市时间_zhuzhuyun\",\n",
    "        \"到达收件城市时间_zhuzhuyun\",\n",
    "        \"派送时间_zhuzhuyun\",\n",
    "        \"签收时间_zhuzhuyun\",\n",
    "        \"到达分拣中心时间_zhuzhuyun\",\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    # --- 您其他的解析器逻辑保持完全不变 ---\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = DATETIME_CAPTURE_PATTERN.search(line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                all_events.append({\"dt\": dt, \"line\": line})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "    collect_event = next(\n",
    "        (e for e in all_events if re.search(profile.get(\"collect\"), e[\"line\"])), None\n",
    "    )\n",
    "    if collect_event:\n",
    "        extracted_times[\"揽收时间_zhuzhuyun\"] = collect_event[\"dt\"]\n",
    "    delivery_event = next(\n",
    "        (e for e in all_events if re.search(profile.get(\"delivery\"), e[\"line\"])), None\n",
    "    )\n",
    "    if delivery_event:\n",
    "        extracted_times[\"派送时间_zhuzhuyun\"] = delivery_event[\"dt\"]\n",
    "    sign_event = None\n",
    "    patterns_to_check = [\"sign_p1\", \"sign_p2\", \"sign_p3\", \"sign_fallback\"]\n",
    "    for pattern_key in patterns_to_check:\n",
    "        pattern = profile.get(pattern_key)\n",
    "        if pattern:\n",
    "            sign_event = next(\n",
    "                (e for e in reversed(all_events) if re.search(pattern, e[\"line\"])), None\n",
    "            )\n",
    "        if sign_event:\n",
    "            break\n",
    "    if sign_event:\n",
    "        extracted_times[\"签收时间_zhuzhuyun\"] = sign_event[\"dt\"]\n",
    "    sender_pattern = get_location_pattern(sender_city, location_maps)\n",
    "    dest_pattern = get_location_pattern(dest_city, location_maps)\n",
    "    kw_center, kw_leave, kw_arrive, kw_exclude = (\n",
    "        profile.get(\"center\"),\n",
    "        profile.get(\"leave\"),\n",
    "        profile.get(\"arrive\"),\n",
    "        profile.get(\"exclude\", \"DONT_MATCH_ANYTHING\"),\n",
    "    )\n",
    "    if pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"]) and sender_pattern:\n",
    "        true_leave_event = None\n",
    "        for event in reversed(all_events):\n",
    "            if event[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"]:\n",
    "                if re.search(kw_leave, event[\"line\"]) and re.search(\n",
    "                    sender_pattern, event[\"line\"]\n",
    "                ):\n",
    "                    if not re.search(kw_exclude, event[\"line\"]):\n",
    "                        true_leave_event = event\n",
    "                        break\n",
    "        if true_leave_event:\n",
    "            extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = true_leave_event[\"dt\"]\n",
    "        window_end = true_leave_event[\"dt\"] if true_leave_event else pd.Timestamp.max\n",
    "        last_arrival_in_origin = None\n",
    "        for event in all_events:\n",
    "            if not (extracted_times[\"揽收时间_zhuzhuyun\"] < event[\"dt\"] < window_end):\n",
    "                continue\n",
    "            if (\n",
    "                re.search(kw_arrive, event[\"line\"])\n",
    "                and not re.search(kw_leave, event[\"line\"])\n",
    "                and re.search(kw_center, event[\"line\"])\n",
    "                and re.search(sender_pattern, event[\"line\"])\n",
    "                and not re.search(kw_exclude, event[\"line\"])\n",
    "            ):\n",
    "                last_arrival_in_origin = event\n",
    "        if last_arrival_in_origin:\n",
    "            extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = last_arrival_in_origin[\"dt\"]\n",
    "    if dest_pattern:\n",
    "        arrive_dest_event = next(\n",
    "            (\n",
    "                e\n",
    "                for e in all_events\n",
    "                if (\n",
    "                    re.search(kw_arrive, e[\"line\"])\n",
    "                    and re.search(dest_pattern, e[\"line\"])\n",
    "                    and not re.search(kw_leave, e[\"line\"])\n",
    "                )\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if arrive_dest_event:\n",
    "            extracted_times[\"到达收件城市时间_zhuzhuyun\"] = arrive_dest_event[\"dt\"]\n",
    "    last_leave_dest_center = None\n",
    "    if pd.notna(extracted_times[\"派送时间_zhuzhuyun\"]) and dest_pattern and kw_center:\n",
    "        leave_dest_center_pattern = re.compile(\n",
    "            f\"({kw_leave}).*?【(?P<location_name>.*?)】\"\n",
    "        )\n",
    "        start_window = extracted_times.get(\n",
    "            \"到达收件城市时间_zhuzhuyun\", pd.Timestamp.min\n",
    "        )\n",
    "        end_window = extracted_times[\"派送时间_zhuzhuyun\"]\n",
    "        for event in all_events:\n",
    "            if not (start_window < event[\"dt\"] < end_window):\n",
    "                continue\n",
    "            match = leave_dest_center_pattern.search(event[\"line\"])\n",
    "            if match:\n",
    "                location_name = match.group(\"location_name\")\n",
    "                if (\n",
    "                    re.search(dest_pattern, location_name)\n",
    "                    and re.search(kw_center, location_name)\n",
    "                    and not re.search(kw_exclude, location_name)\n",
    "                ):\n",
    "                    last_leave_dest_center = event\n",
    "    if last_leave_dest_center:\n",
    "        extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"] = last_leave_dest_center[\n",
    "            \"dt\"\n",
    "        ]\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 (保持不变) ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 主流程: 德邦 (已修改数据读取逻辑) ---\n",
    "def process_deppon_data(\n",
    "    zhuzhuyun_dir: Path,\n",
    "    anjian_dir: Path,\n",
    "    output_dir: Path,\n",
    "    base_data_file: Path,\n",
    "    period: str,\n",
    "):\n",
    "    company_name = \"德邦\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": f\"德邦_{period}.xlsx\",\n",
    "        \"internal_name\": \"德邦\",\n",
    "        \"anjian_map_key\": \"DEPPON\",\n",
    "        \"parser\": parse_logistics_events_deppon_final_logic,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    try:\n",
    "        if base_data_file.exists():\n",
    "            df_hierarchy = pd.read_excel(base_data_file, sheet_name=\"city_hierarchy\")\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\", \"District\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                r\"市|区|县\", \"\", regex=True\n",
    "            )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\").set_index(\"City\")[\n",
    "                \"Province\"\n",
    "            ]\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province.to_dict()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    try:\n",
    "        if base_data_file.exists():\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])  # '202506' -> 6, 以匹配 '6月' 而非 '06月'\n",
    "    anjian_filename = f\"{year}年{month}月{company_name}抽样.xlsx\"\n",
    "    anjian_file_path = anjian_dir / anjian_filename\n",
    "\n",
    "    if not anjian_file_path.exists():\n",
    "        print(f\"[ERROR] 安监数据文件未找到，流程中止: {anjian_file_path.name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"  -> 正在读取安监数据: {anjian_file_path.name}\")\n",
    "    try:\n",
    "        all_anjian_df = pd.read_excel(\n",
    "            anjian_file_path, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 读取安监文件 '{anjian_filename}' 时出错: {e}\")\n",
    "        return\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    # 【新增】准备安监数据源\n",
    "    df_anjian_deppon = all_anjian_df[\n",
    "        all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]\n",
    "    ].copy()\n",
    "    if not df_anjian_deppon.empty:\n",
    "        print(\n",
    "            f\"  -> 已从安监数据中筛选出 {len(df_anjian_deppon)} 条 {company_name} (DEPPON) 记录。\"\n",
    "        )\n",
    "        df_anjian_deppon.rename(\n",
    "            columns={\n",
    "                \"揽收时间\": \"揽收时间_anjian\",\n",
    "                \"离开寄件城市时间\": \"离开寄件城市时间_anjian\",\n",
    "                \"到达收件城市时间\": \"到达收件城市时间_anjian\",\n",
    "                \"派送时间\": \"派送时间_anjian\",\n",
    "                \"签收时间\": \"签收时间_anjian\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        df_anjian_deppon[\"企业\"] = config[\"internal_name\"]  # 统一企业名\n",
    "\n",
    "    # --- 猪猪云数据处理 ---\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    base_info_df = all_anjian_df[all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]][\n",
    "        [\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]\n",
    "    ].drop_duplicates(\"单号\")\n",
    "    base_info_df[\"企业\"] = config[\"internal_name\"]  # 统一企业名，以匹配猪猪云\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_DEPPON[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_DEPPON.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_zhuzhuyun_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    # 【新增】整合两大数据源\n",
    "    zhuzhuyun_cols_to_keep = [\n",
    "        \"单号\",\n",
    "        \"企业\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"完整物流信息\",\n",
    "    ] + [col for col in df_zhuzhuyun_final if \"_zhuzhuyun\" in col]\n",
    "    df_zhuzhuyun_subset = df_zhuzhuyun_final[zhuzhuyun_cols_to_keep]\n",
    "    anjian_cols_to_keep = [\"单号\", \"企业\"] + [\n",
    "        col for col in df_anjian_deppon if \"_anjian\" in col\n",
    "    ]\n",
    "    df_anjian_subset = df_anjian_deppon[anjian_cols_to_keep]\n",
    "    df_final = pd.merge(\n",
    "        df_zhuzhuyun_subset, df_anjian_subset, on=[\"单号\", \"企业\"], how=\"outer\"\n",
    "    )\n",
    "    print(f\"  -> 已整合安监与猪猪云数据，共 {len(df_final)} 条唯一记录。\")\n",
    "    print(\"  -> 根据全局配置选择时间戳来源...\")\n",
    "    source_config = DATA_SOURCE_CONFIG.get(company_name, {})\n",
    "    if not source_config:\n",
    "        raise ValueError(\n",
    "            f\"错误: 在 DATA_SOURCE_CONFIG 中未找到 '{company_name}' 的配置!\"\n",
    "        )\n",
    "    timestamp_map = {\n",
    "        \"揽收时间\": \"collection_time_source\",\n",
    "        \"离开寄件城市时间\": \"departure_time_source\",\n",
    "        \"到达收件城市时间\": \"arrival_time_source\",\n",
    "        \"派送时间\": \"dispatch_time_source\",\n",
    "        \"签收时间\": \"signed_time_source\",\n",
    "    }\n",
    "    for final_col, config_key in timestamp_map.items():\n",
    "        source_type = source_config.get(config_key, \"anjian\")\n",
    "        primary_col = f\"{final_col}_{source_type}\"\n",
    "        fallback_col = (\n",
    "            f\"{final_col}_{'zhuzhuyun' if source_type == 'anjian' else 'anjian'}\"\n",
    "        )\n",
    "        df_final[final_col] = df_final.get(primary_col)\n",
    "        if fallback_col in df_final:\n",
    "            df_final[final_col] = df_final[final_col].fillna(df_final[fallback_col])\n",
    "        print(f\"    - '{final_col}' 已配置使用 '{source_type}' 数据源。\")\n",
    "    df_final[\"到达分拣中心时间\"] = df_final.get(\"到达分拣中心时间_zhuzhuyun\")\n",
    "    df_final[\"离开收件城市分拣中心时间\"] = df_final.get(\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "    )\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    # --- 后续处理与保存 (保持不变) ---\n",
    "    if df_base_subset is not None:\n",
    "        if \"寄出城市_x\" in df_final.columns and \"寄出城市_y\" in df_final.columns:\n",
    "            df_final[\"寄出城市\"] = df_final[\"寄出城市_x\"].fillna(df_final[\"寄出城市_y\"])\n",
    "            df_final[\"寄达城市\"] = df_final[\"寄达城市_x\"].fillna(df_final[\"寄达城市_y\"])\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行主流程 (保持不变) ---\n",
    "if \"period\" in locals() and \"DATA_SOURCE_CONFIG\" in locals():\n",
    "    process_deppon_data(\n",
    "        zhuzhuyun_merge_path,\n",
    "        anjian_data_path,\n",
    "        pycharm_input_path,\n",
    "        base_data_path,\n",
    "        period,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[警告] 未定义 'period' 或 'DATA_SOURCE_CONFIG' 变量，跳过 process_deppon_data 的执行。请在前面的单元格中定义这些变量。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1deac852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 申通 ---\n",
      "  -> 正在读取安监数据: 2025年6月申通抽样.xlsx\n",
      "  -> 已从安监数据中筛选出 83029 条 申通 (STO) 记录。\n",
      "  -> 使用 'parse_logistics_events_sto' 解析器通过 .apply() 运行...\n",
      "  -> 已整合安监与猪猪云数据，共 83029 条唯一记录。\n",
      "  -> 根据全局配置选择时间戳来源...\n",
      "    - '揽收时间' 已配置使用 'anjian' 数据源。\n",
      "    - '离开寄件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '到达收件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '派送时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '签收时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "✅ 申通 处理完成，耗时 51.72 秒。文件已保存至: 申通_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.7: 申通logistics数据提取 (已修改)\n",
    "# -------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 (仅在独立运行此单元格时需要) ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 申通 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 通用辅助函数 (保持不变) ---\n",
    "def extractor_apply_based(\n",
    "    df: pd.DataFrame, profile: dict, location_maps: dict, parser_func\n",
    ") -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"  -> 使用 '{parser_func.__name__}' 解析器通过 .apply() 运行...\")\n",
    "    return df.apply(lambda row: parser_func(row, profile, location_maps), axis=1)\n",
    "\n",
    "\n",
    "# --- 公司档案: 申通 (保持不变) ---\n",
    "COMPANY_PROFILES_STO = {\n",
    "    \"default\": {\n",
    "        \"collect\": r\"揽收|已收取|揽件|已取件\",\n",
    "        \"delivery\": r\"派送中|为您派送|正在为您派件|已安排派送|正在为您派送|正在派送途中\",\n",
    "        \"leave\": r\"离开|已发出|发往\",\n",
    "        \"arrive\": r\"到达|抵达\",\n",
    "        \"center\": r\"转运中心|处理中心|分拨中心|转运场|运营区|枢纽中心|集散中心\",\n",
    "        \"exclude\": r\"揽投部|营业部|经营分部|网点|集货点|营销中心\",\n",
    "        \"sign_p1\": r\"已被.*?接收|已被.*?代收|已经妥投|已送达|作废处理|快件已到达\\[.*?驿站\\]|已抵达.*?公司|包裹已完成签收|已放入|已存放至|到达退货服务站点|被退回|超市|正在验收中|已送货上門|按地址投递|已放入.*?驿站|已投递|已抵达.*?服务点|已派送至|已投至|已到站|自提柜|智能柜|菜鸟驿站|邮政营业网点\",\n",
    "        \"sign_p2\": r\"本人签收|他人代签收|已代收|放至家门口|指定位置签收\",\n",
    "        \"sign_fallback\": r\"已签收|已妥投|已妥收\",\n",
    "    },\n",
    "    \"申通\": {\"sign_p2\": r\"已由【.*?】签收|已签收|代收\"},\n",
    "}\n",
    "\n",
    "\n",
    "# --- 解析器: 申通 (最终定稿版) (保持不变, 仅为统一命名规范修改后缀) ---\n",
    "def parse_logistics_events_sto(\n",
    "    row: pd.Series, profile: dict, location_maps: dict\n",
    ") -> pd.Series:\n",
    "    log_text = row[\"完整物流信息\"]\n",
    "    sender_city_key, dest_city_key = row[\"寄出城市\"], row[\"寄达城市\"]\n",
    "    # 【改动】为统一命名规范，将列后缀从 _zzy 改为 _zhuzhuyun\n",
    "    time_cols = [\n",
    "        \"揽收时间_zhuzhuyun\",\n",
    "        \"离开寄件城市时间_zhuzhuyun\",\n",
    "        \"到达收件城市时间_zhuzhuyun\",\n",
    "        \"派送时间_zhuzhuyun\",\n",
    "        \"签收时间_zhuzhuyun\",\n",
    "        \"到达分拣中心时间_zhuzhuyun\",\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    # --- 您其他的解析器逻辑保持完全不变 ---\n",
    "    kw_collect, kw_delivery = profile.get(\"collect\"), profile.get(\"delivery\")\n",
    "    kw_leave, kw_arrive = profile.get(\"leave\"), profile.get(\"arrive\")\n",
    "    kw_sign_p1, kw_sign_p2, kw_sign_fallback = (\n",
    "        profile.get(\"sign_p1\"),\n",
    "        profile.get(\"sign_p2\"),\n",
    "        profile.get(\"sign_fallback\"),\n",
    "    )\n",
    "    datetime_capture_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    all_events = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_capture_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                location_match = re.search(r\"【(.*?)】\", line)\n",
    "                location = location_match.group(1) if location_match else None\n",
    "                all_events.append({\"dt\": dt, \"line\": line, \"location\": location})\n",
    "    if not all_events:\n",
    "        return pd.Series(extracted_times)\n",
    "    all_events.sort(key=lambda x: x[\"dt\"])\n",
    "\n",
    "    def get_location_pattern(city_key, location_maps, include_province=True):\n",
    "        if not city_key or not isinstance(city_key, str) or not city_key.strip():\n",
    "            return None\n",
    "        aliases = {city_key.replace(\"市\", \"\")}\n",
    "        aliases.update(location_maps[\"city_alias_map\"].get(city_key, []))\n",
    "        if include_province:\n",
    "            province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "            if province:\n",
    "                aliases.add(province.replace(\"省\", \"\").replace(\"市\", \"\"))\n",
    "        return \"|\".join(map(re.escape, {a for a in aliases if a}))\n",
    "\n",
    "    def event_matches_location(event, pattern):\n",
    "        if not pattern:\n",
    "            return False\n",
    "        if event[\"location\"]:\n",
    "            return bool(re.search(pattern, event[\"location\"]))\n",
    "        else:\n",
    "            return bool(re.search(pattern, event[\"line\"]))\n",
    "\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    sender_pattern_strict = get_location_pattern(\n",
    "        sender_city_key, location_maps, include_province=False\n",
    "    )\n",
    "    dest_pattern_strict = get_location_pattern(\n",
    "        dest_city_key, location_maps, include_province=False\n",
    "    )\n",
    "    dest_pattern_broad = get_location_pattern(\n",
    "        dest_city_key, location_maps, include_province=True\n",
    "    )\n",
    "    for event in all_events:\n",
    "        if kw_collect and re.search(kw_collect, event[\"line\"]):\n",
    "            extracted_times[\"揽收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    for event in all_events:\n",
    "        if kw_delivery and re.search(kw_delivery, event[\"line\"]):\n",
    "            extracted_times[\"派送时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    for event in all_events:\n",
    "        if (\n",
    "            pd.notna(extracted_times[\"派送时间_zhuzhuyun\"])\n",
    "            and event[\"dt\"] < extracted_times[\"派送时间_zhuzhuyun\"]\n",
    "        ):\n",
    "            continue\n",
    "        if kw_delivery and re.search(kw_delivery, event[\"line\"]):\n",
    "            continue\n",
    "        if kw_sign_p1 and re.search(kw_sign_p1, event[\"line\"]):\n",
    "            extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "        if kw_sign_p2 and re.search(kw_sign_p2, event[\"line\"]):\n",
    "            extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "        if kw_sign_fallback and re.search(kw_sign_fallback, event[\"line\"]):\n",
    "            extracted_times[\"签收时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "            break\n",
    "    if sender_city_key in [\"西安市\", \"贵阳市\"]:\n",
    "        hub_pattern_map = {\"西安市\": r\"陕西西安转运中心\", \"贵阳市\": r\"贵州贵阳转运中心\"}\n",
    "        hub_pattern = hub_pattern_map.get(sender_city_key)\n",
    "        last_hub_arrival_event = None\n",
    "        for event in reversed(all_events):\n",
    "            if (\n",
    "                kw_arrive\n",
    "                and re.search(kw_arrive, event[\"line\"])\n",
    "                and hub_pattern\n",
    "                and re.search(hub_pattern, event[\"line\"])\n",
    "            ):\n",
    "                last_hub_arrival_event = event\n",
    "                break\n",
    "        if last_hub_arrival_event:\n",
    "            extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = last_hub_arrival_event[\"dt\"]\n",
    "            for event in all_events:\n",
    "                if event[\"dt\"] > last_hub_arrival_event[\"dt\"]:\n",
    "                    if kw_leave and re.search(kw_leave, event[\"line\"]):\n",
    "                        extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                        break\n",
    "    sender_events = [e for e in all_events if event_matches_location(e, sender_pattern)]\n",
    "    dest_events = []\n",
    "    if dest_city_key and isinstance(dest_city_key, str) and dest_city_key.strip():\n",
    "        dest_events = (\n",
    "            sender_events\n",
    "            if sender_city_key == dest_city_key\n",
    "            else [\n",
    "                e for e in all_events if event_matches_location(e, dest_pattern_broad)\n",
    "            ]\n",
    "        )\n",
    "    if pd.isna(extracted_times[\"离开寄件城市时间_zhuzhuyun\"]):\n",
    "        if sender_events:\n",
    "            true_intercity_departure = pd.NaT\n",
    "            for event in reversed(sender_events):\n",
    "                if kw_leave and re.search(kw_leave, event[\"line\"]):\n",
    "                    destination_match = re.search(rf\"{kw_leave}\\s*(.*)\", event[\"line\"])\n",
    "                    if destination_match:\n",
    "                        destination_text_raw = destination_match.group(1)\n",
    "                        if destination_text_raw:\n",
    "                            destination_text = destination_text_raw.strip()\n",
    "                            if sender_pattern_strict and not re.search(\n",
    "                                sender_pattern_strict, destination_text\n",
    "                            ):\n",
    "                                true_intercity_departure = event[\"dt\"]\n",
    "                                break\n",
    "            if pd.isna(true_intercity_departure):\n",
    "                for event in reversed(sender_events):\n",
    "                    if kw_leave and re.search(kw_leave, event[\"line\"]):\n",
    "                        if (\n",
    "                            pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"])\n",
    "                            and event[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"]\n",
    "                        ):\n",
    "                            true_intercity_departure = event[\"dt\"]\n",
    "                            break\n",
    "            extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = true_intercity_departure\n",
    "    if pd.isna(extracted_times[\"到达收件城市时间_zhuzhuyun\"]):\n",
    "        if dest_pattern_strict:\n",
    "            for event in all_events:\n",
    "                if \"预计\" in event[\"line\"]:\n",
    "                    continue\n",
    "                if (\n",
    "                    kw_arrive\n",
    "                    and re.search(kw_arrive, event[\"line\"])\n",
    "                    and event_matches_location(event, dest_pattern_strict)\n",
    "                ):\n",
    "                    extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                    break\n",
    "        if pd.isna(extracted_times[\"到达收件城市时间_zhuzhuyun\"]) and dest_events:\n",
    "            for event in dest_events:\n",
    "                if \"预计\" in event[\"line\"]:\n",
    "                    continue\n",
    "                if kw_arrive and re.search(kw_arrive, event[\"line\"]):\n",
    "                    extracted_times[\"到达收件城市时间_zhuzhuyun\"] = event[\"dt\"]\n",
    "                    break\n",
    "    if pd.isna(extracted_times[\"到达分拣中心时间_zhuzhuyun\"]) and pd.notna(\n",
    "        extracted_times[\"离开寄件城市时间_zhuzhuyun\"]\n",
    "    ):\n",
    "        for i, event in enumerate(all_events):\n",
    "            if event[\"dt\"] == extracted_times[\"离开寄件城市时间_zhuzhuyun\"]:\n",
    "                if i > 0:\n",
    "                    previous_event = all_events[i - 1]\n",
    "                    if (\n",
    "                        kw_arrive\n",
    "                        and re.search(kw_arrive, previous_event[\"line\"])\n",
    "                        and event_matches_location(previous_event, sender_pattern)\n",
    "                    ):\n",
    "                        extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = previous_event[\n",
    "                            \"dt\"\n",
    "                        ]\n",
    "                break\n",
    "    if pd.isna(extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"]):\n",
    "        if pd.notna(extracted_times[\"到达收件城市时间_zhuzhuyun\"]) and pd.notna(\n",
    "            extracted_times[\"派送时间_zhuzhuyun\"]\n",
    "        ):\n",
    "            start_time = extracted_times[\"到达收件城市时间_zhuzhuyun\"]\n",
    "            end_time = extracted_times[\"派送时间_zhuzhuyun\"]\n",
    "            for event in all_events:\n",
    "                if start_time < event[\"dt\"] < end_time:\n",
    "                    if (\n",
    "                        kw_leave\n",
    "                        and re.search(kw_leave, event[\"line\"])\n",
    "                        and event_matches_location(event, dest_pattern_broad)\n",
    "                    ):\n",
    "                        extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"] = event[\n",
    "                            \"dt\"\n",
    "                        ]\n",
    "                        break\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 申通 (已修改数据读取逻辑) ---\n",
    "def process_sto_data(zhuzhuyun_dir, anjian_dir, output_dir, base_data_file, period):\n",
    "    company_name = \"申通\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": f\"申通_{period}.xlsx\",\n",
    "        \"internal_name\": \"申通\",\n",
    "        \"anjian_map_key\": \"STO\",\n",
    "        \"parser\": parse_logistics_events_sto,\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(base_data_file, sheet_name=\"city_hierarchy\")\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\", \"District\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            location_maps[\"city_to_province_map\"] = (\n",
    "                df_hierarchy.drop_duplicates(\"City\")\n",
    "                .set_index(\"City\")[\"Province\"]\n",
    "                .to_dict()\n",
    "            )\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                location_maps[\"capital_cities_set\"] = set(\n",
    "                    df_hierarchy[df_hierarchy[\"IsCapital\"] == 1][\"City\"].unique()\n",
    "                )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    df_base_subset = None\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            df_base_subset = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] 处理 'basic_data.xlsx' 出错: {e}。\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 【核心改动】根据period动态读取本期的安监数据文件\n",
    "    # -----------------------------------------------------------------\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])  # '202506' -> 6, 以匹配 '6月' 而非 '06月'\n",
    "    anjian_filename = f\"{year}年{month}月{company_name}抽样.xlsx\"\n",
    "    anjian_file_path = anjian_dir / anjian_filename\n",
    "\n",
    "    if not anjian_file_path.exists():\n",
    "        print(f\"[ERROR] 安监数据文件未找到，流程中止: {anjian_file_path.name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"  -> 正在读取安监数据: {anjian_file_path.name}\")\n",
    "    try:\n",
    "        all_anjian_df = pd.read_excel(\n",
    "            anjian_file_path, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 读取安监文件 '{anjian_filename}' 时出错: {e}\")\n",
    "        return\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    # 【新增】准备安监数据源\n",
    "    df_anjian_sto = all_anjian_df[\n",
    "        all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]\n",
    "    ].copy()\n",
    "    if not df_anjian_sto.empty:\n",
    "        print(\n",
    "            f\"  -> 已从安监数据中筛选出 {len(df_anjian_sto)} 条 {company_name} (STO) 记录。\"\n",
    "        )\n",
    "        df_anjian_sto.rename(\n",
    "            columns={\n",
    "                \"揽收时间\": \"揽收时间_anjian\",\n",
    "                \"离开寄件城市时间\": \"离开寄件城市时间_anjian\",\n",
    "                \"到达收件城市时间\": \"到达收件城市时间_anjian\",\n",
    "                \"派送时间\": \"派送时间_anjian\",\n",
    "                \"签收时间\": \"签收时间_anjian\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        df_anjian_sto[\"企业\"] = config[\"internal_name\"]\n",
    "\n",
    "    # --- 猪猪云数据处理 ---\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[WARNING] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    base_info_df = all_anjian_df[all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]][\n",
    "        [\"单号\", \"企业\", \"寄出城市\", \"寄达城市\"]\n",
    "    ].drop_duplicates(\"单号\")\n",
    "    base_info_df[\"企业\"] = config[\"internal_name\"]\n",
    "    df_merged = pd.merge(df_company_zhu, base_info_df, on=[\"单号\", \"企业\"], how=\"left\")\n",
    "    base_profile = COMPANY_PROFILES_STO[\"default\"].copy()\n",
    "    company_specific_profile = COMPANY_PROFILES_STO.get(config[\"internal_name\"], {})\n",
    "    profile = {**base_profile, **company_specific_profile}\n",
    "    extracted_df = extractor_apply_based(\n",
    "        df_merged, profile, location_maps, config[\"parser\"]\n",
    "    )\n",
    "    df_zhuzhuyun_final = pd.concat([df_merged, extracted_df], axis=1)\n",
    "\n",
    "    # 【新增】整合两大数据源\n",
    "    zhuzhuyun_cols_to_keep = [\n",
    "        \"单号\",\n",
    "        \"企业\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"完整物流信息\",\n",
    "    ] + [col for col in df_zhuzhuyun_final if \"_zhuzhuyun\" in col]\n",
    "    df_zhuzhuyun_subset = df_zhuzhuyun_final[zhuzhuyun_cols_to_keep]\n",
    "    anjian_cols_to_keep = [\"单号\", \"企业\"] + [\n",
    "        col for col in df_anjian_sto if \"_anjian\" in col\n",
    "    ]\n",
    "    df_anjian_subset = df_anjian_sto[anjian_cols_to_keep]\n",
    "    df_final = pd.merge(\n",
    "        df_zhuzhuyun_subset, df_anjian_subset, on=[\"单号\", \"企业\"], how=\"outer\"\n",
    "    )\n",
    "    print(f\"  -> 已整合安监与猪猪云数据，共 {len(df_final)} 条唯一记录。\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # 【核心改动】替换原有的时间戳赋值逻辑，改为根据全局配置选择\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    print(\"  -> 根据全局配置选择时间戳来源...\")\n",
    "    # 假设 DATA_SOURCE_CONFIG 已在别处定义\n",
    "    # e.g. DATA_SOURCE_CONFIG = {\"申通\": {\"collection_time_source\": \"anjian\", ...}}\n",
    "    source_config = DATA_SOURCE_CONFIG.get(company_name, {})\n",
    "    if not source_config:\n",
    "        raise ValueError(\n",
    "            f\"错误: 在 DATA_SOURCE_CONFIG 中未找到 '{company_name}' 的配置!\"\n",
    "        )\n",
    "    timestamp_map = {\n",
    "        \"揽收时间\": \"collection_time_source\",\n",
    "        \"离开寄件城市时间\": \"departure_time_source\",\n",
    "        \"到达收件城市时间\": \"arrival_time_source\",\n",
    "        \"派送时间\": \"dispatch_time_source\",\n",
    "        \"签收时间\": \"signed_time_source\",\n",
    "    }\n",
    "    for final_col, config_key in timestamp_map.items():\n",
    "        source_type = source_config.get(config_key, \"anjian\")\n",
    "        primary_col = f\"{final_col}_{source_type}\"\n",
    "        fallback_col = (\n",
    "            f\"{final_col}_{'zhuzhuyun' if source_type == 'anjian' else 'anjian'}\"\n",
    "        )\n",
    "        df_final[final_col] = df_final.get(primary_col)\n",
    "        if fallback_col in df_final:\n",
    "            df_final[final_col] = df_final[final_col].fillna(df_final[fallback_col])\n",
    "        print(f\"    - '{final_col}' 已配置使用 '{source_type}' 数据源。\")\n",
    "    df_final[\"到达分拣中心时间\"] = df_final.get(\"到达分拣中心时间_zhuzhuyun\")\n",
    "    df_final[\"离开收件城市分拣中心时间\"] = df_final.get(\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "    )\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    # --- 后续处理与保存 (保持不变) ---\n",
    "    if df_base_subset is not None:\n",
    "        if \"寄出城市_x\" in df_final.columns and \"寄出城市_y\" in df_final.columns:\n",
    "            df_final[\"寄出城市\"] = df_final[\"寄出城市_x\"].fillna(df_final[\"寄出城市_y\"])\n",
    "            df_final[\"寄达城市\"] = df_final[\"寄达城市_x\"].fillna(df_final[\"寄达城市_y\"])\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, df_base_subset, on=\"路线_std\", how=\"left\")\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行: 申通 (保持不变) ---\n",
    "if \"period\" in locals() and \"DATA_SOURCE_CONFIG\" in locals():\n",
    "    process_sto_data(\n",
    "        zhuzhuyun_merge_path,\n",
    "        anjian_data_path,\n",
    "        pycharm_input_path,\n",
    "        base_data_path,\n",
    "        period,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[警告] 未定义 'period' 或 'DATA_SOURCE_CONFIG' 变量，跳过 process_sto_data 的执行。请在前面的单元格中定义这些变量。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b302c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [开始] 处理 圆通 ---\n",
      "  -> 正在读取安监数据: 2025年6月圆通抽样.xlsx\n",
      "  -> 已从安监数据中筛选出 82733 条 圆通 (YTO) 记录。\n",
      "  -> 已整合安监与猪猪云数据，共 82733 条唯一记录。\n",
      "  -> 使用 'parse_logistics_events_yto_test' 解析器运行...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/7fywcbxd7t52ftqjt6sy8cs40000gn/T/ipykernel_49637/244163722.py:111: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  events_df[\"line\"].str.contains(kw[sign_kw_key], na=False, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 根据全局配置选择时间戳来源...\n",
      "    - '揽收时间' 已配置使用 'anjian' 数据源。\n",
      "    - '离开寄件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '到达收件城市时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '派送时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "    - '签收时间' 已配置使用 'zhuzhuyun' 数据源。\n",
      "✅ 圆通 处理完成，耗时 206.70 秒。文件已保存至: 圆通_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 4.8: 圆通logistics数据提取\n",
    "# --------------------------------------------------\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 初始化 ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "print(\"--- [开始] 处理 圆通 ---\")\n",
    "\n",
    "# --- 项目路径设置 (请确保在Cell 0中已正确设置) ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "temp_path = report_path / \"temp\"\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "\n",
    "\n",
    "# --- 公司档案: 圆通 (YTO) (保持不变) ---\n",
    "COMPANY_PROFILES_YTO = {\n",
    "    \"圆通\": {\n",
    "        \"keywords\": {\n",
    "            \"collect\": r\"已揽收\",\n",
    "            \"delivery\": r\"正在派件\",\n",
    "            \"leave\": r\"离开|准备飞往\",\n",
    "            \"center\": r\"转运中心|接驳点\",\n",
    "            \"airport\": r\"机场\",\n",
    "            \"sign_p1\": r\"已到达.*?驿站|已到达.*?代收点|已到达.*?智能柜|已到达.*?自提柜|已到达.*?云柜|已到达.*?智柜|已到达.*?快递柜|已到达【.*?(仓|仓库|组|公司)】|已到达.*?包裹自提|已到达.*?丰巢|已到达.*?兔喜生活|已到达.*?腾云宝|已到达.*?韵达超市|已到达.*?驿收发|已到达.*?溪鸟|已到达.*?格格货栈|已到达.*?近邻宝|已到达.*?富友|已到达.*?熊猫快收|已到达.*?巧目|已到达.*?和驿|已到达.*?喵站|已到达.*?蜜罐|已到达.*?峰栈|已到达.*?腾云小站|已验收成功|本人签收|收件人:家门口\",\n",
    "            \"sign_p2\": r\"已签收\",\n",
    "            \"sign_fallback\": r\"快件已投递\",\n",
    "        },\n",
    "        \"parser\": \"parse_logistics_events_yto_test\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# --- 辅助函数 (保持不变) ---\n",
    "def get_location_pattern(\n",
    "    city_key: Optional[str], location_maps: Dict[str, Any]\n",
    ") -> Optional[str]:\n",
    "    if not city_key or not isinstance(city_key, str):\n",
    "        return None\n",
    "    city_clean = city_key.replace(\"市\", \"\")\n",
    "    aliases = {city_clean}\n",
    "    aliases.update(location_maps[\"city_alias_map\"].get(city_key, []))\n",
    "    if city_key in location_maps[\"capital_cities_set\"]:\n",
    "        province = location_maps[\"city_to_province_map\"].get(city_key)\n",
    "        if province:\n",
    "            province_clean = re.sub(r\"省|市|自治区|维吾尔|壮族\", \"\", province)\n",
    "            aliases.add(province_clean)\n",
    "    unique_aliases = set(filter(None, aliases))\n",
    "    return \"|\".join(map(re.escape, unique_aliases)) if unique_aliases else None\n",
    "\n",
    "\n",
    "# --- 核心解析器: 圆通 (TEST版) (保持不变, 仅为统一命名规范修改后缀) ---\n",
    "def parse_logistics_events_yto_test(\n",
    "    row: pd.Series, profile: Dict[str, Any], location_maps: Dict[str, Any]\n",
    ") -> pd.Series:\n",
    "    log_text = row.get(\"完整物流信息\")\n",
    "    kw = profile[\"keywords\"]\n",
    "    # 【改动】为统一命名规范，将列后缀从 _zzy 改为 _zhuzhuyun\n",
    "    time_cols = [\n",
    "        \"揽收时间_zhuzhuyun\",\n",
    "        \"离开寄件城市时间_zhuzhuyun\",\n",
    "        \"到达收件城市时间_zhuzhuyun\",\n",
    "        \"派送时间_zhuzhuyun\",\n",
    "        \"签收时间_zhuzhuyun\",\n",
    "        \"到达分拣中心时间_zhuzhuyun\",\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\",\n",
    "    ]\n",
    "    extracted_times = {col: pd.NaT for col in time_cols}\n",
    "    if not isinstance(log_text, str) or not log_text.strip():\n",
    "        return pd.Series(extracted_times)\n",
    "    # --- 您其他的解析器逻辑保持完全不变 ---\n",
    "    datetime_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\"\n",
    "    events_data = []\n",
    "    for line in log_text.strip().split(\"\\n\"):\n",
    "        match = re.search(datetime_pattern, line)\n",
    "        if match:\n",
    "            dt = pd.to_datetime(match.group(1), errors=\"coerce\")\n",
    "            if pd.notna(dt):\n",
    "                location_match = re.search(r\"【(.*?)】\", line)\n",
    "                location = location_match.group(1) if location_match else None\n",
    "                events_data.append({\"dt\": dt, \"line\": line, \"location\": location})\n",
    "    if not events_data:\n",
    "        return pd.Series(extracted_times)\n",
    "    events_df = (\n",
    "        pd.DataFrame(events_data)\n",
    "        .sort_values(by=\"dt\", ascending=True)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    collect_events = events_df[events_df[\"line\"].str.contains(kw[\"collect\"], na=False)]\n",
    "    if not collect_events.empty:\n",
    "        extracted_times[\"揽收时间_zhuzhuyun\"] = collect_events.iloc[0][\"dt\"]\n",
    "    delivery_events = events_df[\n",
    "        events_df[\"line\"].str.contains(kw[\"delivery\"], na=False)\n",
    "    ]\n",
    "    if not delivery_events.empty:\n",
    "        extracted_times[\"派送时间_zhuzhuyun\"] = delivery_events.iloc[0][\"dt\"]\n",
    "    for sign_kw_key in [\"sign_p1\", \"sign_p2\", \"sign_fallback\"]:\n",
    "        if pd.isna(extracted_times[\"签收时间_zhuzhuyun\"]) and kw.get(sign_kw_key):\n",
    "            sign_events = events_df[\n",
    "                events_df[\"line\"].str.contains(kw[sign_kw_key], na=False, regex=True)\n",
    "            ]\n",
    "            if not sign_events.empty:\n",
    "                extracted_times[\"签收时间_zhuzhuyun\"] = sign_events.iloc[-1][\"dt\"]\n",
    "    sender_city_key = row.get(\"寄出城市\")\n",
    "    sender_pattern = get_location_pattern(sender_city_key, location_maps)\n",
    "    if pd.notna(extracted_times[\"揽收时间_zhuzhuyun\"]) and sender_pattern:\n",
    "        post_collect_events = events_df[\n",
    "            events_df[\"dt\"] > extracted_times[\"揽收时间_zhuzhuyun\"]\n",
    "        ]\n",
    "        all_hub_keywords = kw[\"center\"] + \"|\" + kw.get(\"airport\", \"\")\n",
    "        sender_hub_events = post_collect_events[\n",
    "            post_collect_events[\"location\"].str.contains(\n",
    "                all_hub_keywords, na=False, regex=True\n",
    "            )\n",
    "            & post_collect_events[\"location\"].str.contains(\n",
    "                sender_pattern, na=False, regex=True\n",
    "            )\n",
    "        ]\n",
    "        if not sender_hub_events.empty:\n",
    "            sender_arrive = sender_hub_events[\n",
    "                sender_hub_events[\"line\"].str.contains(\"已经到达\", na=False)\n",
    "            ]\n",
    "            if not sender_arrive.empty:\n",
    "                extracted_times[\"到达分拣中心时间_zhuzhuyun\"] = sender_arrive.iloc[-1][\n",
    "                    \"dt\"\n",
    "                ]\n",
    "            sender_leave = sender_hub_events[\n",
    "                sender_hub_events[\"line\"].str.contains(\n",
    "                    kw[\"leave\"], na=False, regex=True\n",
    "                )\n",
    "            ]\n",
    "            if not sender_leave.empty:\n",
    "                extracted_times[\"离开寄件城市时间_zhuzhuyun\"] = sender_leave.iloc[-1][\n",
    "                    \"dt\"\n",
    "                ]\n",
    "    dest_city_key = row.get(\"寄达城市\")\n",
    "    dest_pattern = get_location_pattern(dest_city_key, location_maps)\n",
    "    if dest_pattern:\n",
    "        dest_hub_arrive_events = events_df[\n",
    "            events_df[\"location\"].str.contains(\n",
    "                kw[\"center\"] + \"|\" + kw.get(\"airport\", \"\"), na=False, regex=True\n",
    "            )\n",
    "            & events_df[\"location\"].str.contains(dest_pattern, na=False, regex=True)\n",
    "            & events_df[\"line\"].str.contains(\"已经到达\", na=False)\n",
    "        ]\n",
    "        if not dest_hub_arrive_events.empty:\n",
    "            extracted_times[\"到达收件城市时间_zhuzhuyun\"] = dest_hub_arrive_events.iloc[\n",
    "                0\n",
    "            ][\"dt\"]\n",
    "        else:\n",
    "            dest_arrive_events = events_df[\n",
    "                events_df[\"line\"].str.contains(\"已经到达\", na=False)\n",
    "                & events_df[\"location\"].str.contains(dest_pattern, na=False, regex=True)\n",
    "            ]\n",
    "            if not dest_arrive_events.empty:\n",
    "                extracted_times[\"到达收件城市时间_zhuzhuyun\"] = dest_arrive_events.iloc[\n",
    "                    0\n",
    "                ][\"dt\"]\n",
    "        if pd.notna(extracted_times[\"派送时间_zhuzhuyun\"]):\n",
    "            pre_delivery_events = events_df[\n",
    "                events_df[\"dt\"] < extracted_times[\"派送时间_zhuzhuyun\"]\n",
    "            ]\n",
    "            dest_location_events = pre_delivery_events[\n",
    "                pre_delivery_events[\"location\"].str.contains(\n",
    "                    dest_pattern, na=False, case=False, regex=True\n",
    "                )\n",
    "            ]\n",
    "            if not dest_location_events.empty:\n",
    "                dest_leave_events = dest_location_events[\n",
    "                    dest_location_events[\"line\"].str.contains(\"离开\", na=False)\n",
    "                    & dest_location_events[\"location\"].str.contains(\n",
    "                        kw[\"center\"], na=False, regex=True\n",
    "                    )\n",
    "                ]\n",
    "                if not dest_leave_events.empty:\n",
    "                    extracted_times[\"离开收件城市分拣中心时间_zhuzhuyun\"] = (\n",
    "                        dest_leave_events.iloc[-1][\"dt\"]\n",
    "                    )\n",
    "    return pd.Series(extracted_times)\n",
    "\n",
    "\n",
    "# --- 主流程: 圆通 (已修改) ---\n",
    "def process_yto_data(\n",
    "    zhuzhuyun_dir: Path,\n",
    "    anjian_dir: Path,\n",
    "    output_dir: Path,\n",
    "    base_data_file: Path,\n",
    "    period: str,\n",
    "):\n",
    "    company_name = \"圆通\"\n",
    "    config = {\n",
    "        \"zhuzhu_filename\": f\"圆通_{period}.xlsx\",\n",
    "        \"parser\": \"parse_logistics_events_yto_test\",\n",
    "        \"anjian_map_key\": \"YTO\",\n",
    "    }\n",
    "    company_start_time = perf_counter()\n",
    "\n",
    "    # 1. 加载通用数据\n",
    "    location_maps = {\n",
    "        \"city_alias_map\": {},\n",
    "        \"capital_cities_set\": set(),\n",
    "        \"city_to_province_map\": {},\n",
    "    }\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_hierarchy = pd.read_excel(base_data_file, sheet_name=\"city_hierarchy\")\n",
    "            df_hierarchy.dropna(subset=[\"Province\", \"City\", \"District\"], inplace=True)\n",
    "            df_hierarchy[\"City_clean\"] = df_hierarchy[\"City\"].str.replace(\n",
    "                \"市\", \"\", regex=False\n",
    "            )\n",
    "            df_hierarchy[\"District_clean\"] = df_hierarchy[\"District\"].str.replace(\n",
    "                r\"市|区|县\", \"\", regex=True\n",
    "            )\n",
    "            for city, group in df_hierarchy.groupby(\"City\"):\n",
    "                aliases = [group.iloc[0][\"City_clean\"]] + group[\n",
    "                    \"District_clean\"\n",
    "                ].unique().tolist()\n",
    "                location_maps[\"city_alias_map\"][city] = [a for a in aliases if a]\n",
    "            if \"IsCapital\" in df_hierarchy.columns:\n",
    "                df_capitals = df_hierarchy[df_hierarchy[\"IsCapital\"] == 1]\n",
    "                location_maps[\"capital_cities_set\"] = set(df_capitals[\"City\"].unique())\n",
    "            city_to_province = df_hierarchy.drop_duplicates(\"City\").set_index(\"City\")\n",
    "            location_maps[\"city_to_province_map\"] = city_to_province[\n",
    "                \"Province\"\n",
    "            ].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"[错误] 构建地理位置地图失败: {e}。\")\n",
    "\n",
    "    base_routes_df = pd.DataFrame()\n",
    "    if base_data_file.exists():\n",
    "        try:\n",
    "            df_base = pd.read_excel(base_data_file, sheet_name=\"inter-city_routes\")\n",
    "            df_base[\"路线_std\"] = (\n",
    "                df_base[\"寄出城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "                + \"-\"\n",
    "                + df_base[\"寄达城市\"].str.replace(\"市\", \"\", regex=False)\n",
    "            )\n",
    "            base_routes_df = df_base[\n",
    "                [\"路线_std\", \"公里\", \"寄出省份\", \"寄达省份\"]\n",
    "            ].drop_duplicates(\"路线_std\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"[警告] 处理 'basic_data.xlsx' 的 'inter-city_routes' sheet 出错: {e}。\"\n",
    "            )\n",
    "\n",
    "    # 2. 加载安监数据 (核心修改点)\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])\n",
    "    anjian_filename = f\"{year}年{month}月{company_name}抽样.xlsx\"\n",
    "    anjian_file_path = anjian_dir / anjian_filename\n",
    "\n",
    "    if not anjian_file_path.exists():\n",
    "        print(f\"[警告] 安监数据文件 {anjian_filename} 未找到，将仅使用猪猪云数据。\")\n",
    "        all_anjian_df = pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"  -> 正在读取安监数据: {anjian_filename}\")\n",
    "        try:\n",
    "            all_anjian_df = pd.read_excel(\n",
    "                anjian_file_path, dtype={\"单号\": str, \"寄出城市\": str, \"寄达城市\": str}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[错误] 读取安监文件 '{anjian_filename}' 时出错: {e}\")\n",
    "            all_anjian_df = pd.DataFrame()\n",
    "\n",
    "    if \"快递公司\" in all_anjian_df.columns:\n",
    "        all_anjian_df.rename(columns={\"快递公司\": \"企业\"}, inplace=True)\n",
    "\n",
    "    # 准备安监数据\n",
    "    df_anjian_company_std = all_anjian_df[\n",
    "        all_anjian_df[\"企业\"] == config[\"anjian_map_key\"]\n",
    "    ].copy()\n",
    "    if not df_anjian_company_std.empty:\n",
    "        print(\n",
    "            f\"  -> 已从安监数据中筛选出 {len(df_anjian_company_std)} 条 {company_name} (YTO) 记录。\"\n",
    "        )\n",
    "        anjian_ts_cols = [\n",
    "            \"揽收时间\",\n",
    "            \"离开寄件城市时间\",\n",
    "            \"到达收件城市时间\",\n",
    "            \"派送时间\",\n",
    "            \"签收时间\",\n",
    "        ]\n",
    "        for col in anjian_ts_cols:\n",
    "            if col in df_anjian_company_std.columns:\n",
    "                df_anjian_company_std.rename(\n",
    "                    columns={col: f\"{col}_anjian\"}, inplace=True\n",
    "                )\n",
    "        df_anjian_company_std[\"企业\"] = company_name\n",
    "\n",
    "    # 3. 加载猪猪云数据\n",
    "    company_file = zhuzhuyun_dir / config[\"zhuzhu_filename\"]\n",
    "    if not company_file.exists():\n",
    "        print(f\"[警告] 未找到文件: {config['zhuzhu_filename']}，已跳过。\")\n",
    "        return\n",
    "\n",
    "    df_company_zhu = pd.read_excel(company_file, dtype={\"快递单号\": str})\n",
    "    df_company_zhu.rename(\n",
    "        columns={\"快递单号\": \"单号\", \"快递公司\": \"企业\"}, inplace=True\n",
    "    )\n",
    "    df_company_zhu[\"企业\"] = company_name\n",
    "\n",
    "    # 4. 合并数据\n",
    "    df_merged_full = pd.merge(\n",
    "        df_company_zhu, df_anjian_company_std, on=[\"单号\", \"企业\"], how=\"outer\"\n",
    "    )\n",
    "    print(f\"  -> 已整合安监与猪猪云数据，共 {len(df_merged_full)} 条唯一记录。\")\n",
    "\n",
    "    # 5. 运行解析器并选择数据源\n",
    "    parser_func_name = config[\"parser\"]\n",
    "    parser_func = globals()[parser_func_name]\n",
    "    profile = COMPANY_PROFILES_YTO[company_name]\n",
    "    print(f\"  -> 使用 '{parser_func_name}' 解析器运行...\")\n",
    "    extracted_df = df_merged_full.apply(\n",
    "        lambda row: parser_func(row, profile, location_maps), axis=1\n",
    "    )\n",
    "    df_final = pd.concat([df_merged_full, extracted_df], axis=1)\n",
    "\n",
    "    print(\"  -> 根据全局配置选择时间戳来源...\")\n",
    "    source_config = DATA_SOURCE_CONFIG.get(company_name, {})\n",
    "    if not source_config:\n",
    "        raise ValueError(\n",
    "            f\"错误: 在 DATA_SOURCE_CONFIG 中未找到 '{company_name}' 的配置!\"\n",
    "        )\n",
    "\n",
    "    timestamp_map = {\n",
    "        \"揽收时间\": \"collection_time_source\",\n",
    "        \"离开寄件城市时间\": \"departure_time_source\",\n",
    "        \"到达收件城市时间\": \"arrival_time_source\",\n",
    "        \"派送时间\": \"dispatch_time_source\",\n",
    "        \"签收时间\": \"signed_time_source\",\n",
    "    }\n",
    "    for final_col, config_key in timestamp_map.items():\n",
    "        source_type = source_config.get(config_key, \"anjian\")\n",
    "        primary_col = f\"{final_col}_{source_type}\"\n",
    "        fallback_col = (\n",
    "            f\"{final_col}_{'zhuzhuyun' if source_type == 'anjian' else 'anjian'}\"\n",
    "        )\n",
    "        df_final[final_col] = df_final.get(primary_col)\n",
    "        if fallback_col in df_final:\n",
    "            df_final[final_col] = df_final[final_col].fillna(df_final[fallback_col])\n",
    "        print(f\"    - '{final_col}' 已配置使用 '{source_type}' 数据源。\")\n",
    "\n",
    "    df_final[\"到达分拣中心时间\"] = df_final.get(\"到达分拣中心时间_zhuzhuyun\")\n",
    "    df_final[\"离开收件城市分拣中心时间\"] = df_final.get(\n",
    "        \"离开收件城市分拣中心时间_zhuzhuyun\"\n",
    "    )\n",
    "\n",
    "    # 6. 后续处理与保存\n",
    "    if not base_routes_df.empty:\n",
    "        if \"寄出城市_x\" in df_final.columns and \"寄出城市_y\" in df_final.columns:\n",
    "            df_final[\"寄出城市\"] = df_final[\"寄出城市_x\"].fillna(df_final[\"寄出城市_y\"])\n",
    "            df_final[\"寄达城市\"] = df_final[\"寄达城市_x\"].fillna(df_final[\"寄达城市_y\"])\n",
    "        df_final[\"路线_std\"] = (\n",
    "            df_final[\"寄出城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "            + \"-\"\n",
    "            + df_final[\"寄达城市\"].astype(str).str.replace(\"市\", \"\", regex=False)\n",
    "        )\n",
    "        df_final = pd.merge(df_final, base_routes_df, on=\"路线_std\", how=\"left\")\n",
    "\n",
    "    df_final[\"企业\"] = company_name\n",
    "    output_columns = [\n",
    "        \"企业\",\n",
    "        \"单号\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "        \"公里\",\n",
    "        \"寄出省份\",\n",
    "        \"寄达省份\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"完整物流信息\",\n",
    "    ]\n",
    "    final_df_to_save = df_final.reindex(columns=output_columns)\n",
    "    output_file = output_dir / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "    final_df_to_save.to_excel(output_file, index=False)\n",
    "    company_end_time = perf_counter()\n",
    "    print(\n",
    "        f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 运行主流程 ---\n",
    "if \"period\" in locals() and \"DATA_SOURCE_CONFIG\" in locals():\n",
    "    process_yto_data(\n",
    "        zhuzhuyun_merge_path,\n",
    "        anjian_data_path,\n",
    "        pycharm_input_path,\n",
    "        base_data_path,\n",
    "        period,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"[警告] 未定义 'period' 或 'DATA_SOURCE_CONFIG' 变量，跳过 process_yto_data 的执行。请在前面的单元格中定义这些变量。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取基础路线数据: 报告数据/输入/basic_data.xlsx\n",
      "✅ 基础路线数据读取成功。\n",
      "开始处理 顺丰、中通 的安监数据...\n",
      "✅ 顺丰 处理完成，耗时 10.99 秒。文件已保存至: 顺丰_logistics_data_202506.xlsx\n",
      "✅ 中通 处理完成，耗时 10.61 秒。文件已保存至: 中通_logistics_data_202506.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Cell 4.9: 顺丰、中通logistics数据提取（由于无猪猪云输出，仅基于安监数据生成）\n",
    "# -------------------------------------------------------------------\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 假设的路径和变量 (实际使用时请确保这些已在之前的单元格中定义) ---\n",
    "period = \"202506\"\n",
    "input_path = Path(\"报告数据/输入\")\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "pycharm_input_path = Path(\"报告数据/temp/4_logistics数据\")\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# 定义要处理的公司列表\n",
    "companies_to_process = {\"SF\": \"顺丰\", \"ZTO\": \"中通\"}\n",
    "\n",
    "# 定义目标logistics数据文件的完整列名顺序 (已修正)\n",
    "target_columns = [\n",
    "    \"企业\",\n",
    "    \"单号\",\n",
    "    \"寄出城市\",\n",
    "    \"寄达城市\",\n",
    "    \"揽收时间\",\n",
    "    \"离开寄件城市时间\",\n",
    "    \"到达收件城市时间\",\n",
    "    \"派送时间\",\n",
    "    \"签收时间\",\n",
    "    \"公里\",\n",
    "    \"寄出省份\",\n",
    "    \"寄达省份\",\n",
    "    \"到达分拣中心时间\",\n",
    "    \"离开收件城市分拣中心时间\",\n",
    "    \"完整物流信息\",\n",
    "]\n",
    "\n",
    "# 1. 提前读取城市线路基础数据\n",
    "try:\n",
    "    print(f\"正在读取基础路线数据: {base_data_path}\")\n",
    "    routes_df = pd.read_excel(\n",
    "        base_data_path,\n",
    "        sheet_name=\"inter-city_routes\",\n",
    "        usecols=[\"寄出城市\", \"寄达城市\", \"寄出省份\", \"寄达省份\", \"公里\"],\n",
    "    )\n",
    "    print(\"✅ 基础路线数据读取成功。\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"❌ 严重错误: 基础数据文件 '{base_data_path.name}' 未在 '{input_path}' 目录下找到。脚本无法继续。\"\n",
    "    )\n",
    "    routes_df = None\n",
    "except Exception as e:\n",
    "    print(f\"❌ 读取基础数据文件时发生错误: {e}\")\n",
    "    routes_df = None\n",
    "\n",
    "# 动态生成处理通知\n",
    "if routes_df is not None:\n",
    "    companies_str = \"、\".join(companies_to_process.values())\n",
    "    print(f\"开始处理 {companies_str} 的安监数据...\")\n",
    "\n",
    "    # 循环处理每个公司\n",
    "    for raw_name, company_name in companies_to_process.items():\n",
    "        company_start_time = time.perf_counter()\n",
    "        try:\n",
    "            # 2. 根据命名规则构建输入文件名\n",
    "            input_filename = f\"{period[:4]}年{int(period[4:])}月{company_name}抽样.xlsx\"\n",
    "            input_filepath = anjian_data_path / input_filename\n",
    "\n",
    "            output_file = (\n",
    "                pycharm_input_path / f\"{company_name}_logistics_data_{period}.xlsx\"\n",
    "            )\n",
    "\n",
    "            # 3. 读取原始安监数据Excel文件\n",
    "            df = pd.read_excel(input_filepath)\n",
    "\n",
    "            # 4. 标准化企业名称\n",
    "            df[\"企业\"] = company_name\n",
    "\n",
    "            # 5. 合并数据以填充省份和公里数\n",
    "            df_merged = pd.merge(df, routes_df, on=[\"寄出城市\", \"寄达城市\"], how=\"left\")\n",
    "\n",
    "            # 6. 添加剩余的空列\n",
    "            df_merged[\"到达分拣中心时间\"] = pd.NaT\n",
    "            df_merged[\"离开收件城市分拣中心时间\"] = pd.NaT  # 这一列的名称在之前是错误的\n",
    "            df_merged[\"完整物流信息\"] = \"\"\n",
    "\n",
    "            # 7. 重新排列DataFrame的列\n",
    "            df_reordered = df_merged[target_columns]\n",
    "\n",
    "            # 8. 将处理好的DataFrame保存为新的Excel文件\n",
    "            df_reordered.to_excel(output_file, index=False)\n",
    "\n",
    "            company_end_time = time.perf_counter()\n",
    "            print(\n",
    "                f\"✅ {company_name} 处理完成，耗时 {company_end_time - company_start_time:.2f} 秒。文件已保存至: {output_file.name}\"\n",
    "            )\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\n",
    "                f\"❌ 错误: 文件 '{input_filename}' 未在 '{anjian_data_path}' 目录下找到。\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理 {company_name} 时发生未知错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84dc3a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础数据文件应位于: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输入/basic_data.xlsx\n",
      "安监数据文件夹: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输入/安监数据\n",
      "物流明细文件夹: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/3_猪猪云合并数据\n",
      "计算结果将输出至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/5_中转数据\n",
      "----------------------------------------\n",
      "成功加载 297 个目标城市。\n",
      "\n",
      "正在为物流文件 [申通_202506.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月申通抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 申通 (数据期数: 202506) ---\n",
      "在 申通_202506.xlsx 和 2025年6月申通抽样.xlsx 中共找到 83029 条匹配的运单。\n",
      "✔ 申通 的中转数据计算完成，已保存至: 申通_transit_data_202506.xlsx\n",
      "\n",
      "正在为物流文件 [圆通_202506.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月圆通抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 圆通 (数据期数: 202506) ---\n",
      "在 圆通_202506.xlsx 和 2025年6月圆通抽样.xlsx 中共找到 82733 条匹配的运单。\n",
      "✔ 圆通 的中转数据计算完成，已保存至: 圆通_transit_data_202506.xlsx\n",
      "\n",
      "正在为物流文件 [EMS_202506.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月EMS抽样.xlsx]\n",
      "\n",
      "--- 正在处理: EMS (数据期数: 202506) ---\n",
      "在 EMS_202506.xlsx 和 2025年6月EMS抽样.xlsx 中共找到 43390 条匹配的运单。\n",
      "✔ EMS 的中转数据计算完成，已保存至: EMS_transit_data_202506.xlsx\n",
      "\n",
      "正在为物流文件 [极兔_202506.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月极兔抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 极兔 (数据期数: 202506) ---\n",
      "在 极兔_202506.xlsx 和 2025年6月极兔抽样.xlsx 中共找到 81703 条匹配的运单。\n",
      "✔ 极兔 的中转数据计算完成，已保存至: 极兔_transit_data_202506.xlsx\n",
      "\n",
      "正在为物流文件 [京东_202506.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月京东抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 京东 (数据期数: 202506) ---\n",
      "在 京东_202506.xlsx 和 2025年6月京东抽样.xlsx 中共找到 79688 条匹配的运单。\n",
      "✔ 京东 的中转数据计算完成，已保存至: 京东_transit_data_202506.xlsx\n",
      "\n",
      "正在为物流文件 [韵达_202506.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月韵达抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 韵达 (数据期数: 202506) ---\n",
      "在 韵达_202506.xlsx 和 2025年6月韵达抽样.xlsx 中共找到 81797 条匹配的运单。\n",
      "✔ 韵达 的中转数据计算完成，已保存至: 韵达_transit_data_202506.xlsx\n",
      "\n",
      "正在为物流文件 [邮政_202506.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月邮政抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 邮政 (数据期数: 202506) ---\n",
      "在 邮政_202506.xlsx 和 2025年6月邮政抽样.xlsx 中共找到 81571 条匹配的运单。\n",
      "✔ 邮政 的中转数据计算完成，已保存至: 邮政_transit_data_202506.xlsx\n",
      "\n",
      "正在为物流文件 [德邦_202506.xlsx] 寻找对应的安监数据...\n",
      "--> 成功匹配: [2025年6月德邦抽样.xlsx]\n",
      "\n",
      "--- 正在处理: 德邦 (数据期数: 202506) ---\n",
      "在 德邦_202506.xlsx 和 2025年6月德邦抽样.xlsx 中共找到 78551 条匹配的运单。\n",
      "✔ 德邦 的中转数据计算完成，已保存至: 德邦_transit_data_202506.xlsx\n",
      "\n",
      "--- 所有文件处理完毕！ ---\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 5: 中转数据生成\n",
    "# --------------------------------------------------\n",
    "# --- 1. 设置文件路径 ---\n",
    "# 输入路径\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_dir = report_path / \"输入\"\n",
    "anjian_dir = input_dir / \"安监数据\"\n",
    "# 中间过程文件路径\n",
    "temp_path = report_path / \"temp\"\n",
    "# 物流明细数据来自上一步生成的猪猪云合并数据\n",
    "logistics_dir = temp_path / \"3_猪猪云合并数据\"\n",
    "output_dir = temp_path / \"5_中转数据\"\n",
    "basic_data_path = input_dir / \"basic_data.xlsx\"\n",
    "\n",
    "anjian_dir.mkdir(parents=True, exist_ok=True)\n",
    "logistics_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"基础数据文件应位于: {basic_data_path}\")\n",
    "print(f\"安监数据文件夹: {anjian_dir}\")\n",
    "print(f\"物流明细文件夹: {logistics_dir}\")\n",
    "print(f\"计算结果将输出至: {output_dir}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 2. 城市加载与匹配逻辑 ---\n",
    "def load_city_pattern(path_to_basic_data):\n",
    "    try:\n",
    "        if not path_to_basic_data.exists():\n",
    "            return None\n",
    "        df_city = pd.read_excel(\n",
    "            path_to_basic_data, sheet_name=\"city_names_complete_2025\", engine=\"openpyxl\"\n",
    "        )\n",
    "        if \"城市\" not in df_city.columns or \"行政级别\" not in df_city.columns:\n",
    "            return None\n",
    "        df_city[\"行政级别\"] = df_city[\"行政级别\"].str.strip()\n",
    "        target_levels = [\"地级市\", \"直辖市\", \"副省级城市\", \"省直辖县级市\"]\n",
    "        target_df = df_city[df_city[\"行政级别\"].isin(target_levels)].copy()\n",
    "        target_df[\"城市_clean\"] = target_df[\"城市\"].str.replace(\n",
    "            r\"(市|省|自治区|特别行政区)$\", \"\", regex=True\n",
    "        )\n",
    "        city_list = target_df[\"城市_clean\"].dropna().unique().tolist()\n",
    "        city_list.sort(key=len, reverse=True)\n",
    "        print(f\"成功加载 {len(city_list)} 个目标城市。\")\n",
    "        return re.compile(\"|\".join(city_list))\n",
    "    except Exception as e:\n",
    "        print(f\"加载和处理城市数据时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "BASE_CITY_PATTERN = load_city_pattern(basic_data_path)\n",
    "AMBIGUOUS_MAP = {\n",
    "    \"朝阳\": \"北京\",\n",
    "    \"湘潭\": \"天津\",\n",
    "}\n",
    "\n",
    "\n",
    "def find_all_valid_cities_in_text(text, pattern):\n",
    "    if not isinstance(text, str) or not pattern:\n",
    "        return []\n",
    "    cities_found = pattern.findall(text)\n",
    "    valid_cities = []\n",
    "    for city in cities_found:\n",
    "        is_noise = False\n",
    "        if city in AMBIGUOUS_MAP:\n",
    "            context_word = AMBIGUOUS_MAP[city]\n",
    "            if context_word in text:\n",
    "                is_noise = True\n",
    "        if not is_noise:\n",
    "            valid_cities.append(city + \"市\")\n",
    "    return list(dict.fromkeys(valid_cities))\n",
    "\n",
    "\n",
    "# --- “锚点切割”算法---\n",
    "def extract_transit_log_slice(log_entries, origin_city, dest_city):\n",
    "    last_origin_anchor = -1\n",
    "    first_dest_anchor = -1\n",
    "    origin_city_clean = origin_city.replace(\"市\", \"\")\n",
    "    dest_city_clean = dest_city.replace(\"市\", \"\")\n",
    "    for i, entry in enumerate(log_entries):\n",
    "        if origin_city_clean in entry:\n",
    "            last_origin_anchor = i\n",
    "    for i, entry in enumerate(log_entries):\n",
    "        if dest_city_clean in entry:\n",
    "            first_dest_anchor = i\n",
    "            break\n",
    "    if (\n",
    "        last_origin_anchor != -1\n",
    "        and first_dest_anchor != -1\n",
    "        and last_origin_anchor < first_dest_anchor\n",
    "    ):\n",
    "        return log_entries[last_origin_anchor + 1 : first_dest_anchor]\n",
    "    return []\n",
    "\n",
    "\n",
    "# --- 3. 核心处理逻辑 ---\n",
    "def process_company_data(logistics_path, anjian_path, period):\n",
    "    company_name_raw = logistics_path.stem\n",
    "    company_name = company_name_raw.split(\"_\")[0]\n",
    "    rename_map = {\"邮政国内小包\": \"邮政\"}\n",
    "    company_name = rename_map.get(company_name, company_name)\n",
    "    print(f\"\\n--- 正在处理: {company_name} (数据期数: {period}) ---\")\n",
    "\n",
    "    try:\n",
    "        df_anjian = pd.read_excel(anjian_path)\n",
    "        df_logistics = pd.read_excel(logistics_path)\n",
    "\n",
    "        df_anjian = df_anjian[[\"单号\", \"寄出城市\", \"寄达城市\"]].dropna()\n",
    "        df_anjian[\"寄出城市\"] = (\n",
    "            df_anjian[\"寄出城市\"].str.replace(\"市\", \"\", regex=False) + \"市\"\n",
    "        )\n",
    "        df_anjian[\"寄达城市\"] = (\n",
    "            df_anjian[\"寄达城市\"].str.replace(\"市\", \"\", regex=False) + \"市\"\n",
    "        )\n",
    "        df_anjian[\"单号\"] = df_anjian[\"单号\"].astype(str)\n",
    "\n",
    "        df_logistics = df_logistics[[\"快递单号\", \"完整物流信息\"]].dropna()\n",
    "        df_logistics[\"快递单号\"] = df_logistics[\"快递单号\"].astype(str)\n",
    "\n",
    "        merged_df = pd.merge(\n",
    "            df_logistics, df_anjian, left_on=\"快递单号\", right_on=\"单号\", how=\"inner\"\n",
    "        )\n",
    "        print(\n",
    "            f\"在 {logistics_path.name} 和 {anjian_path.name} 中共找到 {len(merged_df)} 条匹配的运单。\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"处理公司 {company_name} 时发生错误: {e}\")\n",
    "        return\n",
    "\n",
    "    all_routes = []\n",
    "    for _, row in merged_df.iterrows():\n",
    "        origin_city_auth = row[\"寄出城市\"]\n",
    "        dest_city_auth = row[\"寄达城市\"]\n",
    "        full_log = row[\"完整物流信息\"]\n",
    "\n",
    "        log_entries = [entry.strip() for entry in full_log.split(\"\\n\") if entry.strip()]\n",
    "        log_entries.reverse()\n",
    "\n",
    "        transit_slice = extract_transit_log_slice(\n",
    "            log_entries, origin_city_auth, dest_city_auth\n",
    "        )\n",
    "\n",
    "        transit_cities = set()\n",
    "        for entry in transit_slice:\n",
    "            cities_in_entry = find_all_valid_cities_in_text(entry, BASE_CITY_PATTERN)\n",
    "            for city in cities_in_entry:\n",
    "                if city not in [origin_city_auth, dest_city_auth]:\n",
    "                    transit_cities.add(city)\n",
    "\n",
    "        all_routes.append(\n",
    "            {\n",
    "                \"出发城市\": origin_city_auth,\n",
    "                \"到达城市\": dest_city_auth,\n",
    "                \"中转城市列表\": sorted(list(transit_cities)),\n",
    "                \"中转次数\": len(transit_cities),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not all_routes:\n",
    "        print(f\"未能在 {company_name} 的数据中提取出任何有效的中转路径。\")\n",
    "        return\n",
    "\n",
    "    routes_df = pd.DataFrame(all_routes)\n",
    "    agg_result = (\n",
    "        routes_df.groupby([\"出发城市\", \"到达城市\"])\n",
    "        .agg(\n",
    "            平均中转次数=(\"中转次数\", \"mean\"),\n",
    "            中转城市=(\n",
    "                \"中转城市列表\",\n",
    "                lambda s: sorted(list(set(c for sub in s for c in sub))),\n",
    "            ),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    agg_result[\"中转城市\"] = agg_result[\"中转城市\"].apply(lambda x: \",\".join(x))\n",
    "    agg_result[\"平均中转次数\"] = agg_result[\"平均中转次数\"].round(2)\n",
    "    agg_result = agg_result.sort_values(by=\"平均中转次数\", ascending=False).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    final_columns = [\"出发城市\", \"到达城市\", \"中转城市\", \"平均中转次数\"]\n",
    "    agg_result = agg_result[final_columns]\n",
    "\n",
    "    # 新增修改: 输出文件名中加入 period\n",
    "    output_filename = f\"{company_name}_transit_data_{period}.xlsx\"\n",
    "    output_path = output_dir / output_filename\n",
    "    agg_result.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "    print(f\"✔ {company_name} 的中转数据计算完成，已保存至: {output_path.name}\")\n",
    "\n",
    "\n",
    "# --- 7. 主程序入口 ---\n",
    "def main(period: str):\n",
    "    if not BASE_CITY_PATTERN:\n",
    "        print(\"由于城市列表未能加载，无法继续处理文件。\")\n",
    "        return\n",
    "\n",
    "    logistics_files = list(logistics_dir.glob(f\"*_{period}.xlsx\")) + list(\n",
    "        logistics_dir.glob(f\"*_{period}.xls\")\n",
    "    )\n",
    "    if not logistics_files:\n",
    "        print(\n",
    "            f\"警告：在物流明细文件夹中未找到任何包含期数 '{period}' 的Excel文件！请检查路径: {logistics_dir}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # 新增修改: 从 period (例如 '202507') 中解析出年份和月份\n",
    "    year = period[:4]\n",
    "    month = int(period[4:])  # 使用 int() 去掉可能存在的前导零，例如 '07' -> 7\n",
    "\n",
    "    for log_path in logistics_files:\n",
    "        # 从物流文件名 (如 \"京东_202507.xlsx\") 中提取公司名 \"京东\"\n",
    "        company_name_raw = log_path.stem\n",
    "        company_name_for_search = company_name_raw.split(\"_\")[0]\n",
    "\n",
    "        print(f\"\\n正在为物流文件 [{log_path.name}] 寻找对应的安监数据...\")\n",
    "\n",
    "        # 新增修改: 根据解析出的年月和公司名，构建精确的安监文件名\n",
    "        target_anjian_filename = f\"{year}年{month}月{company_name_for_search}抽样.xlsx\"\n",
    "        anjian_path = anjian_dir / target_anjian_filename\n",
    "\n",
    "        # 检查文件是否存在，如果不存在，则给出明确提示\n",
    "        if not anjian_path.exists():\n",
    "            # 为了兼容性，也检查一下 .xls 后缀\n",
    "            target_anjian_filename_xls = target_anjian_filename.replace(\".xlsx\", \".xls\")\n",
    "            anjian_path_xls = anjian_dir / target_anjian_filename_xls\n",
    "            if not anjian_path_xls.exists():\n",
    "                print(\n",
    "                    f\"--> 警告：在安监文件夹中未找到精确匹配的文件 '{target_anjian_filename}' 或 '{target_anjian_filename_xls}'，跳过该公司。\"\n",
    "                )\n",
    "                continue\n",
    "            else:\n",
    "                anjian_path = anjian_path_xls\n",
    "\n",
    "        print(f\"--> 成功匹配: [{anjian_path.name}]\")\n",
    "        process_company_data(log_path, anjian_path, period)\n",
    "\n",
    "    print(\"\\n--- 所有文件处理完毕！ ---\")\n",
    "\n",
    "\n",
    "# 在Jupyter环境中，period变量应已在Cell 0中定义\n",
    "if \"period\" in locals() or \"period\" in globals():\n",
    "    main(period)\n",
    "else:\n",
    "    print(\"错误：未找到 'period' 变量。请确保您已运行定义本期的第一个cell。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4099a584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "库导入完成。\n",
      "项目文件夹结构设置/检查完毕。\n",
      "输入数据(Logistics数据)应位于: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/temp/4_logistics数据\n",
      "分析报告将输出至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输出/data_analysis_result\n",
      "\n",
      "正在从 'basic_data.xlsx' 加载 Top 30 城市列表...\n",
      "成功加载 30 个 Top 30 城市。\n",
      "\n",
      "找到 10 个待处理文件 (期数: 202506)，开始分析...\n",
      "\n",
      "--- 正在处理文件: EMS_logistics_data_202506.xlsx ---\n",
      "原始数据量: 43390 条\n",
      "  - 因核心时间戳缺失，删除了 58 行。\n",
      "    -> 因时限存在空值，删除了 901 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 1625 行。\n",
      "分析完成, 结果已保存至: EMS_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 中通_logistics_data_202506.xlsx ---\n",
      "原始数据量: 83160 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "    -> 因时限存在空值，删除了 0 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 20372 行。\n",
      "   -> 提示: 未找到对应的中转文件: 中通_transit_data_202506.xlsx\n",
      "分析完成, 结果已保存至: 中通_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 京东_logistics_data_202506.xlsx ---\n",
      "原始数据量: 79688 条\n",
      "  - 因核心时间戳缺失，删除了 14952 行。\n",
      "    -> 因时限存在空值，删除了 3042 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 1359 行。\n",
      "分析完成, 结果已保存至: 京东_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 圆通_logistics_data_202506.xlsx ---\n",
      "原始数据量: 82733 条\n",
      "  - 因核心时间戳缺失，删除了 7425 行。\n",
      "    -> 因时限存在空值，删除了 5519 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 2275 行。\n",
      "分析完成, 结果已保存至: 圆通_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 德邦_logistics_data_202506.xlsx ---\n",
      "原始数据量: 78551 条\n",
      "  - 因核心时间戳缺失，删除了 8153 行。\n",
      "    -> 因时限存在空值，删除了 8989 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 1322 行。\n",
      "分析完成, 结果已保存至: 德邦_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 极兔_logistics_data_202506.xlsx ---\n",
      "原始数据量: 81703 条\n",
      "  - 因核心时间戳缺失，删除了 9583 行。\n",
      "    -> 因时限存在空值，删除了 146 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 704 行。\n",
      "分析完成, 结果已保存至: 极兔_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 申通_logistics_data_202506.xlsx ---\n",
      "原始数据量: 83029 条\n",
      "  - 因核心时间戳缺失，删除了 3883 行。\n",
      "    -> 因时限存在空值，删除了 11511 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 1980 行。\n",
      "分析完成, 结果已保存至: 申通_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 邮政_logistics_data_202506.xlsx ---\n",
      "原始数据量: 81571 条\n",
      "  - 因核心时间戳缺失，删除了 4501 行。\n",
      "    -> 因时限存在空值，删除了 2058 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 1261 行。\n",
      "分析完成, 结果已保存至: 邮政_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 韵达_logistics_data_202506.xlsx ---\n",
      "原始数据量: 81797 条\n",
      "  - 因核心时间戳缺失，删除了 16666 行。\n",
      "    -> 因时限存在空值，删除了 11860 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 907 行。\n",
      "分析完成, 结果已保存至: 韵达_data_analysis_result_202506.xlsx\n",
      "\n",
      "--- 正在处理文件: 顺丰_logistics_data_202506.xlsx ---\n",
      "原始数据量: 83269 条\n",
      "  - 因核心时间戳缺失，删除了 0 行。\n",
      "    -> 因时限存在空值，删除了 0 行。\n",
      "    -> 因时限值不符合业务范围，又删除了 1229 行。\n",
      "   -> 提示: 未找到对应的中转文件: 顺丰_transit_data_202506.xlsx\n",
      "分析完成, 结果已保存至: 顺丰_data_analysis_result_202506.xlsx\n",
      "\n",
      "\n",
      "所有文件处理完毕！总耗时: 222.31 秒。\n",
      "\n",
      "==================== 数据筛选流程汇总统计 ====================\n",
      "| 公司 (Company)   |   原始数据量 |   因时间戳缺失删除 |   因时限空值删除 |   因范围不符删除 |   **总计删除** |   最终保留 | **保留率**   |\n",
      "|:-----------------|-------------:|-------------------:|-----------------:|-----------------:|---------------:|-----------:|:-------------|\n",
      "| EMS              |        43390 |                 58 |              901 |             1625 |           2584 |      40806 | 94.0%        |\n",
      "| 中通             |        83160 |                  0 |                0 |            20372 |          20372 |      62788 | 75.5%        |\n",
      "| 京东             |        79688 |              14952 |             3042 |             1359 |          19353 |      60335 | 75.7%        |\n",
      "| 圆通             |        82733 |               7425 |             5519 |             2275 |          15219 |      67514 | 81.6%        |\n",
      "| 德邦             |        78551 |               8153 |             8989 |             1322 |          18464 |      60087 | 76.5%        |\n",
      "| 极兔             |        81703 |               9583 |              146 |              704 |          10433 |      71270 | 87.2%        |\n",
      "| 申通             |        83029 |               3883 |            11511 |             1980 |          17374 |      65655 | 79.1%        |\n",
      "| 快包             |        81571 |               4501 |             2058 |             1261 |           7820 |      73751 | 90.4%        |\n",
      "| 韵达             |        81797 |              16666 |            11860 |              907 |          29433 |      52364 | 64.0%        |\n",
      "| 顺丰             |        83269 |                  0 |                0 |             1229 |           1229 |      82040 | 98.5%        |\n",
      "| **总计**         |       778891 |              65221 |            44026 |            33034 |         142281 |     636610 | 81.7%        |\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 6: 核心数据计算层 (最终版)\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy\")\n",
    "print(\"库导入完成。\")\n",
    "\n",
    "\n",
    "# --- 1. 运行模式开关 ---\n",
    "RUN_MODE = \"ALL\"\n",
    "TARGET_COMPANY = \"邮政\"\n",
    "\n",
    "# --- 3. 项目路径设置 ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_data_path = report_path / \"temp\" / \"4_logistics数据\"\n",
    "zhuzhuyun_data_path = report_path / \"temp\" / \"3_猪猪云合并数据\"\n",
    "transit_data_path = report_path / \"temp\" / \"5_中转数据\"\n",
    "output_path = report_path / \"输出\" / \"data_analysis_result\"\n",
    "basic_data_file = report_path / \"输入\" / \"basic_data.xlsx\"\n",
    "\n",
    "# (新增) 公司名到文件名的映射，以及反向映射\n",
    "COMPANY_TO_FILENAME_MAP = {\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"中通\": \"中通\",\n",
    "    \"京东\": \"京东\",\n",
    "    \"圆通\": \"圆通\",\n",
    "    \"德邦\": \"德邦\",\n",
    "    \"极兔\": \"极兔\",\n",
    "    \"申通\": \"申通\",\n",
    "    \"韵达\": \"韵达\",\n",
    "    \"顺丰\": \"顺丰\",\n",
    "    \"快包\": \"邮政\",\n",
    "}\n",
    "FILENAME_TO_COMPANY_MAP = {v: k for k, v in COMPANY_TO_FILENAME_MAP.items()}\n",
    "\n",
    "\n",
    "def load_top_cities(file_path: Path) -> set:\n",
    "    sheet_name = \"30_top_volume_city_2024\"\n",
    "    column_name = \"城市\"\n",
    "    print(f\"\\n正在从 '{file_path.name}' 加载 Top 30 城市列表...\")\n",
    "    try:\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"基础数据文件不存在: {file_path}\")\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"在Sheet '{sheet_name}' 中未找到名为 '{column_name}' 的列。\"\n",
    "            )\n",
    "        cities_set = set(df[column_name].dropna().astype(str).str.strip().tolist())\n",
    "        if not cities_set:\n",
    "            print(f\"警告: 从 '{file_path.name}' 中加载的城市列表为空。\")\n",
    "        else:\n",
    "            print(f\"成功加载 {len(cities_set)} 个 Top 30 城市。\")\n",
    "        return cities_set\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 加载Top 30城市列表失败！错误信息: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def generate_basic_metrics_df(\n",
    "    filtered_data: pd.DataFrame,\n",
    "    original_data_count: int,\n",
    "    top_30_cities: set,\n",
    "    total_dropped_count: int,\n",
    ") -> pd.DataFrame:\n",
    "    metrics_data, distance_metrics = [], []\n",
    "    total_count = filtered_data.shape[0]\n",
    "    top_30_data_all = filtered_data[\n",
    "        (filtered_data[\"寄出城市\"].isin(top_30_cities))\n",
    "        & (filtered_data[\"寄达城市\"].isin(top_30_cities))\n",
    "    ]\n",
    "    total_top_30_count = top_30_data_all.shape[0]\n",
    "    if not filtered_data.empty:\n",
    "        time_cols_map = {\n",
    "            \"全程时限\": \"全程时限\",\n",
    "            \"寄出地处理时限\": \"寄出地处理时限\",\n",
    "            \"运输时限\": \"运输时限\",\n",
    "            \"寄达地处理时限\": \"寄达地处理时限\",\n",
    "            \"投递时限\": \"投递时限\",\n",
    "        }\n",
    "        for name, col in time_cols_map.items():\n",
    "            metrics_data.append(\n",
    "                [\n",
    "                    name,\n",
    "                    filtered_data[col].mean(),\n",
    "                    filtered_data[col].max(),\n",
    "                    filtered_data[col].min(),\n",
    "                ]\n",
    "            )\n",
    "        metrics_data.append(\n",
    "            [\n",
    "                \"72小时准时率\",\n",
    "                (filtered_data[\"全程时限\"] <= 72).mean() if total_count > 0 else 0,\n",
    "                \"\",\n",
    "                \"\",\n",
    "            ]\n",
    "        )\n",
    "        metrics_data.append(\n",
    "            [\n",
    "                \"48小时准时率\",\n",
    "                (top_30_data_all[\"全程时限\"] <= 48).mean()\n",
    "                if total_top_30_count > 0\n",
    "                else 0,\n",
    "                \"\",\n",
    "                \"\",\n",
    "            ]\n",
    "        )\n",
    "        metrics_data.append(\n",
    "            [\n",
    "                f\"总数据量 {original_data_count}\\n总筛选掉数据量 {total_dropped_count}\\n最终有效数据量 {total_count}\\n业务量前30的城市间数据量 {total_top_30_count}\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "            ]\n",
    "        )\n",
    "        buckets = {\n",
    "            \"0-600\": filtered_data[filtered_data[\"公里\"] < 600],\n",
    "            \"600-1500\": filtered_data[\n",
    "                (filtered_data[\"公里\"] >= 600) & (filtered_data[\"公里\"] < 1500)\n",
    "            ],\n",
    "            \"1500-2500\": filtered_data[\n",
    "                (filtered_data[\"公里\"] >= 1500) & (filtered_data[\"公里\"] < 2500)\n",
    "            ],\n",
    "            \"2500以上\": filtered_data[filtered_data[\"公里\"] >= 2500],\n",
    "        }\n",
    "        for _, col_key in time_cols_map.items():\n",
    "            distance_metrics.append([buckets[b][col_key].mean() for b in buckets])\n",
    "        distance_metrics.extend([[\"\"] * 4] * 3)\n",
    "    else:\n",
    "        metrics_data, distance_metrics = (\n",
    "            ([[\"无有效数据\", \"\", \"\", \"\"]] * 8),\n",
    "            ([[\"\"] * 4] * 8),\n",
    "        )\n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=[\"项目\", \"mean\", \"max\", \"min\"])\n",
    "    distance_df = pd.DataFrame(\n",
    "        distance_metrics, columns=[\"0-600\", \"600-1500\", \"1500-2500\", \"2500以上\"]\n",
    "    )\n",
    "    return pd.concat([metrics_df, distance_df], axis=1).round(4).fillna(\"\")\n",
    "\n",
    "\n",
    "# 新增修改: 增加 period 参数\n",
    "def main(period: str):\n",
    "    for p in [\n",
    "        report_path,\n",
    "        input_data_path,\n",
    "        zhuzhuyun_data_path,\n",
    "        transit_data_path,\n",
    "        output_path,\n",
    "    ]:\n",
    "        p.mkdir(exist_ok=True, parents=True)\n",
    "    print(\"项目文件夹结构设置/检查完毕。\")\n",
    "    print(f\"输入数据(Logistics数据)应位于: {input_data_path}\")\n",
    "    print(f\"分析报告将输出至: {output_path}\")\n",
    "\n",
    "    top_30_cities = load_top_cities(basic_data_file)\n",
    "    start_time = perf_counter()\n",
    "\n",
    "    # 新增修改: 精准匹配本期的 logistics 数据文件\n",
    "    files_to_process = list(input_data_path.glob(f\"*_logistics_data_{period}.xlsx\"))\n",
    "\n",
    "    if RUN_MODE == \"SINGLE\":\n",
    "        files_to_process = [\n",
    "            f for f in files_to_process if f.name.startswith(TARGET_COMPANY)\n",
    "        ]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"\\n警告: 未找到任何包含期数 '{period}' 的待处理数据文件，程序即将退出。\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    summary_stats = []\n",
    "    files_to_process = sorted(list(set(files_to_process)))\n",
    "    print(f\"\\n找到 {len(files_to_process)} 个待处理文件 (期数: {period})，开始分析...\")\n",
    "    for file_path in files_to_process:\n",
    "        # 新增修改: 将 period 传递给处理函数\n",
    "        stats = process_single_file(file_path, top_30_cities, period)\n",
    "        if stats:\n",
    "            summary_stats.append(stats)\n",
    "\n",
    "    end_time = perf_counter()\n",
    "    print(f\"\\n\\n所有文件处理完毕！总耗时: {end_time - start_time:.2f} 秒。\")\n",
    "\n",
    "    if summary_stats:\n",
    "        print(\"\\n\" + \"=\" * 20 + \" 数据筛选流程汇总统计 \" + \"=\" * 20)\n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "        summary_df[\"**总计删除**\"] = (\n",
    "            summary_df[\"因时间戳缺失删除\"]\n",
    "            + summary_df[\"因时限空值删除\"]\n",
    "            + summary_df[\"因范围不符删除\"]\n",
    "        )\n",
    "        summary_df[\"最终保留\"] = summary_df[\"原始数据量\"] - summary_df[\"**总计删除**\"]\n",
    "        summary_df[\"**保留率**\"] = (\n",
    "            summary_df[\"最终保留\"] / summary_df[\"原始数据量\"]\n",
    "        ).apply(lambda x: f\"{x:.1%}\")\n",
    "        summary_df = summary_df[\n",
    "            [\n",
    "                \"公司 (Company)\",\n",
    "                \"原始数据量\",\n",
    "                \"因时间戳缺失删除\",\n",
    "                \"因时限空值删除\",\n",
    "                \"因范围不符删除\",\n",
    "                \"**总计删除**\",\n",
    "                \"最终保留\",\n",
    "                \"**保留率**\",\n",
    "            ]\n",
    "        ]\n",
    "        total_row = summary_df.select_dtypes(include=np.number).sum()\n",
    "        total_row[\"公司 (Company)\"] = \"**总计**\"\n",
    "        total_row[\"**保留率**\"] = (\n",
    "            f\"{(total_row['最终保留'] / total_row['原始数据量']):.1%}\"\n",
    "        )\n",
    "        summary_df = pd.concat(\n",
    "            [summary_df, pd.DataFrame(total_row).T], ignore_index=True\n",
    "        )\n",
    "        print(summary_df.to_markdown(index=False))\n",
    "\n",
    "\n",
    "# 新增修改: 增加 period 参数\n",
    "def process_single_file(file_path: Path, top_30_cities: set, period: str):\n",
    "    file_name = file_path.name\n",
    "    print(f\"\\n--- 正在处理文件: {file_name} ---\")\n",
    "\n",
    "    try:\n",
    "        data = pd.read_excel(\n",
    "            file_path, header=0, dtype={\"单号\": str}, engine=\"openpyxl\"\n",
    "        )\n",
    "        original_data_count = data.shape[0]\n",
    "        print(f\"原始数据量: {original_data_count} 条\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件 {file_name} 失败: {e}。跳过此文件。\")\n",
    "        return None\n",
    "\n",
    "    core_required_cols = [\n",
    "        \"单号\",\n",
    "        \"寄出省份\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达省份\",\n",
    "        \"寄达城市\",\n",
    "        \"公里\",\n",
    "        \"揽收时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"签收时间\",\n",
    "    ]\n",
    "    optional_cols = [\"到达分拣中心时间\", \"离开收件城市分拣中心时间\"]\n",
    "    if not all(c in data.columns for c in core_required_cols):\n",
    "        print(f\"错误: 文件 {file_name} 缺少必要的核心列。跳过此文件。\")\n",
    "        return None\n",
    "    for col in optional_cols:\n",
    "        if col not in data.columns:\n",
    "            data[col] = pd.NaT\n",
    "\n",
    "    # 新增修改: 修正猪猪云和中转数据的匹配逻辑\n",
    "    # 从 \"京东_logistics_data_202507.xlsx\" 中提取出 \"京东\"\n",
    "    base_company_name = file_name.split(\"_\")[0]\n",
    "\n",
    "    zhuzhuyun_file = zhuzhuyun_data_path / f\"{base_company_name}_{period}.xlsx\"\n",
    "    if zhuzhuyun_file.exists():\n",
    "        try:\n",
    "            df_zhuzhu = pd.read_excel(\n",
    "                zhuzhuyun_file,\n",
    "                usecols=[\"快递单号\", \"完整物流信息\"],\n",
    "                dtype={\"快递单号\": str},\n",
    "                engine=\"openpyxl\",\n",
    "            ).rename(columns={\"快递单号\": \"单号\"})\n",
    "            data = pd.merge(\n",
    "                data, df_zhuzhu.drop_duplicates(subset=[\"单号\"]), on=\"单号\", how=\"left\"\n",
    "            )\n",
    "            if \"完整物流信息_y\" in data.columns:\n",
    "                data[\"完整物流信息\"] = data[\"完整物流信息_y\"].fillna(\n",
    "                    data[\"完整物流信息_x\"]\n",
    "                )\n",
    "                data.drop(columns=[\"完整物流信息_x\", \"完整物流信息_y\"], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"  - 警告: 合并猪猪云数据失败: {e}\")\n",
    "    if \"完整物流信息\" not in data.columns:\n",
    "        data[\"完整物流信息\"] = \"\"\n",
    "\n",
    "    all_time_cols = [c for c in core_required_cols + optional_cols if \"时间\" in c]\n",
    "    for col in all_time_cols:\n",
    "        data[col] = pd.to_datetime(data[col], errors=\"coerce\")\n",
    "\n",
    "    count_before_time_dropna = len(data)\n",
    "    data.dropna(subset=[c for c in core_required_cols if \"时间\" in c], inplace=True)\n",
    "    time_dropped_count = count_before_time_dropna - len(data)\n",
    "    print(f\"  - 因核心时间戳缺失，删除了 {time_dropped_count} 行。\")\n",
    "\n",
    "    data[\"全程时限\"] = (data[\"签收时间\"] - data[\"揽收时间\"]) / np.timedelta64(1, \"h\")\n",
    "    data[\"寄出地处理时限\"] = (\n",
    "        data[\"离开寄件城市时间\"] - data[\"揽收时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "    data[\"寄达地处理时限\"] = (\n",
    "        data[\"派送时间\"] - data[\"到达收件城市时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "    data[\"投递时限\"] = (data[\"签收时间\"] - data[\"派送时间\"]) / np.timedelta64(1, \"h\")\n",
    "    data[\"运输时限\"] = (\n",
    "        data[\"到达收件城市时间\"] - data[\"离开寄件城市时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "    data[\"揽收-到达寄出地分拣中心时长\"] = (\n",
    "        data[\"到达分拣中心时间\"] - data[\"揽收时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "    data[\"到达寄出地分拣中心-离开寄出地城市时长\"] = (\n",
    "        data[\"离开寄件城市时间\"] - data[\"到达分拣中心时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "    data[\"到达寄达地城市-离开寄达地分拣中心时长\"] = (\n",
    "        data[\"离开收件城市分拣中心时间\"] - data[\"到达收件城市时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "    data[\"离开寄达地分拣中心-派件\"] = (\n",
    "        data[\"派送时间\"] - data[\"离开收件城市分拣中心时间\"]\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "\n",
    "    company_key = FILENAME_TO_COMPANY_MAP.get(base_company_name, base_company_name)\n",
    "    EXCLUDED_COMPANIES = {\"中通\", \"顺丰\"}\n",
    "    count_before_filter = len(data)\n",
    "    core_duration_cols = [\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"运输时限\",\n",
    "    ]\n",
    "    all_duration_cols = core_duration_cols + [\n",
    "        \"揽收-到达寄出地分拣中心时长\",\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "        \"离开寄达地分拣中心-派件\",\n",
    "    ]\n",
    "    columns_to_check_for_nan = (\n",
    "        core_duration_cols\n",
    "        if company_key in EXCLUDED_COMPANIES\n",
    "        else [col for col in all_duration_cols if col in data.columns]\n",
    "    )\n",
    "    data.dropna(subset=columns_to_check_for_nan, inplace=True)\n",
    "    nan_dropped_count = count_before_filter - len(data)\n",
    "    print(f\"    -> 因时限存在空值，删除了 {nan_dropped_count} 行。\")\n",
    "\n",
    "    base_mask = (\n",
    "        data[\"寄出地处理时限\"].between(0.1, 48)\n",
    "        & data[\"运输时限\"].between(0.5, 200)\n",
    "        & data[\"寄达地处理时限\"].between(0.1, 60)\n",
    "        & data[\"投递时限\"].between(0, 36)\n",
    "    )\n",
    "    if company_key not in EXCLUDED_COMPANIES:\n",
    "        for col in [\n",
    "            \"揽收-到达寄出地分拣中心时长\",\n",
    "            \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "            \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "            \"离开寄达地分拣中心-派件\",\n",
    "        ]:\n",
    "            if col in data.columns:\n",
    "                base_mask &= data[col] > 0\n",
    "    filtered_data = data.loc[base_mask].copy()\n",
    "    value_dropped_count = len(data) - len(filtered_data)\n",
    "    print(f\"    -> 因时限值不符合业务范围，又删除了 {value_dropped_count} 行。\")\n",
    "\n",
    "    total_dropped_count = time_dropped_count + nan_dropped_count + value_dropped_count\n",
    "    if filtered_data.empty:\n",
    "        print(\"筛选后无有效数据，无法生成报告。跳过此文件。\")\n",
    "        return None\n",
    "\n",
    "    filtered_data[\"T+1_achieved\"] = filtered_data[\"签收时间\"].dt.normalize() <= (\n",
    "        filtered_data[\"揽收时间\"].dt.normalize() + pd.Timedelta(days=1)\n",
    "    )\n",
    "    filtered_data[\"T+2_achieved\"] = filtered_data[\"签收时间\"].dt.normalize() <= (\n",
    "        filtered_data[\"揽收时间\"].dt.normalize() + pd.Timedelta(days=2)\n",
    "    )\n",
    "    air_keyword = {\"EMS\": \"飞往|发往航站准备安检\", \"快包\": \"飞往|发往航站准备安检\"}.get(\n",
    "        company_key, \"\"\n",
    "    )\n",
    "    filtered_data[\"is_air\"] = (\n",
    "        filtered_data[\"完整物流信息\"].fillna(\"\").str.contains(air_keyword, regex=True)\n",
    "        if air_keyword\n",
    "        else False\n",
    "    )\n",
    "\n",
    "    basic_metrics_df = generate_basic_metrics_df(\n",
    "        filtered_data, original_data_count, top_30_cities, total_dropped_count\n",
    "    )\n",
    "    detailed_routes_df = filtered_data[\n",
    "        [\n",
    "            col\n",
    "            for col in [\n",
    "                \"单号\",\n",
    "                \"寄出省份\",\n",
    "                \"寄出城市\",\n",
    "                \"寄达省份\",\n",
    "                \"寄达城市\",\n",
    "                \"公里\",\n",
    "                \"揽收时间\",\n",
    "                \"离开寄件城市时间\",\n",
    "                \"到达收件城市时间\",\n",
    "                \"派送时间\",\n",
    "                \"签收时间\",\n",
    "                \"到达分拣中心时间\",\n",
    "                \"离开收件城市分拣中心时间\",\n",
    "                \"完整物流信息\",\n",
    "                \"全程时限\",\n",
    "                \"寄出地处理时限\",\n",
    "                \"寄达地处理时限\",\n",
    "                \"投递时限\",\n",
    "                \"运输时限\",\n",
    "                \"揽收-到达寄出地分拣中心时长\",\n",
    "                \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "                \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "                \"离开寄达地分拣中心-派件\",\n",
    "                \"T+1_achieved\",\n",
    "                \"T+2_achieved\",\n",
    "                \"is_air\",\n",
    "            ]\n",
    "            if col in filtered_data.columns\n",
    "        ]\n",
    "    ].round(3)\n",
    "\n",
    "    grouped = filtered_data.groupby([\"寄出城市\", \"寄达城市\"])\n",
    "    agg_dict = {\n",
    "        \"快递数量\": (\"单号\", \"count\"),\n",
    "        \"全程时限\": (\"全程时限\", \"mean\"),\n",
    "        \"寄出地处理时限\": (\"寄出地处理时限\", \"mean\"),\n",
    "        \"运输时限\": (\"运输时限\", \"mean\"),\n",
    "        \"寄达地处理时限\": (\"寄达地处理时限\", \"mean\"),\n",
    "        \"投递时限\": (\"投递时限\", \"mean\"),\n",
    "        \"揽收-到达寄出地分拣中心时长\": (\"揽收-到达寄出地分拣中心时长\", \"mean\"),\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\": (\n",
    "            \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "            \"mean\",\n",
    "        ),\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\": (\n",
    "            \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "            \"mean\",\n",
    "        ),\n",
    "        \"离开寄达地分拣中心-派件\": (\"离开寄达地分拣中心-派件\", \"mean\"),\n",
    "    }\n",
    "    summary_df = grouped.agg(**agg_dict)\n",
    "    total_counts = grouped.size()\n",
    "    summary_df[\"72小时准时率\"] = (\n",
    "        filtered_data[filtered_data[\"全程时限\"] <= 72]\n",
    "        .groupby([\"寄出城市\", \"寄达城市\"])\n",
    "        .size()\n",
    "        / total_counts\n",
    "    ).fillna(0)\n",
    "    summary_df[\"48小时准时率\"] = (\n",
    "        filtered_data[filtered_data[\"全程时限\"] <= 48]\n",
    "        .groupby([\"寄出城市\", \"寄达城市\"])\n",
    "        .size()\n",
    "        / total_counts\n",
    "    ).fillna(0)\n",
    "    filtered_data[\"送达天数\"] = (\n",
    "        filtered_data[\"签收时间\"].dt.normalize()\n",
    "        - filtered_data[\"揽收时间\"].dt.normalize()\n",
    "    ).dt.days\n",
    "    summary_df[\"送达天数_80分位\"] = filtered_data.groupby([\"寄出城市\", \"寄达城市\"])[\n",
    "        \"送达天数\"\n",
    "    ].quantile(0.8, interpolation=\"higher\")\n",
    "    summary_df.reset_index(inplace=True)\n",
    "    summary_df[\"路线\"] = summary_df[\"寄出城市\"] + \"-\" + summary_df[\"寄达城市\"]\n",
    "\n",
    "    transit_filename_key = \"邮政\" if company_key == \"快包\" else base_company_name\n",
    "    # 新增修改: 匹配的中转文件名也需要包含 period\n",
    "    transfer_file = (\n",
    "        transit_data_path / f\"{transit_filename_key}_transit_data_{period}.xlsx\"\n",
    "    )\n",
    "    if transfer_file.exists():\n",
    "        try:\n",
    "            df_transfer = pd.read_excel(transfer_file, engine=\"openpyxl\")\n",
    "            df_transfer[\"路线\"] = (\n",
    "                df_transfer[\"出发城市\"] + \"-\" + df_transfer[\"到达城市\"]\n",
    "            )\n",
    "            df_agg = (\n",
    "                df_transfer.groupby(\"路线\")[[\"平均中转次数\"]]\n",
    "                .mean()\n",
    "                .rename(columns={\"平均中转次数\": \"中转次数\"})\n",
    "            )\n",
    "            summary_df = pd.merge(summary_df, df_agg, on=\"路线\", how=\"left\")\n",
    "        except Exception as e:\n",
    "            print(f\"   -> 警告: 处理中转文件 {transfer_file.name} 失败: {e}\")\n",
    "    else:\n",
    "        print(f\"   -> 提示: 未找到对应的中转文件: {transfer_file.name}\")\n",
    "    if \"中转次数\" not in summary_df.columns:\n",
    "        summary_df[\"中转次数\"] = np.nan\n",
    "\n",
    "    summary_df = summary_df[\n",
    "        [\n",
    "            col\n",
    "            for col in [\n",
    "                \"寄出城市\",\n",
    "                \"寄达城市\",\n",
    "                \"路线\",\n",
    "                \"快递数量\",\n",
    "                \"全程时限\",\n",
    "                \"寄出地处理时限\",\n",
    "                \"运输时限\",\n",
    "                \"寄达地处理时限\",\n",
    "                \"投递时限\",\n",
    "                \"揽收-到达寄出地分拣中心时长\",\n",
    "                \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "                \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "                \"离开寄达地分拣中心-派件\",\n",
    "                \"72小时准时率\",\n",
    "                \"48小时准时率\",\n",
    "                \"送达天数_80分位\",\n",
    "                \"中转次数\",\n",
    "            ]\n",
    "            if col in summary_df.columns\n",
    "        ]\n",
    "    ].round(4)\n",
    "\n",
    "    # 新增修改: 输出文件名中加入 period\n",
    "    output_file_path = (\n",
    "        output_path / f\"{base_company_name}_data_analysis_result_{period}.xlsx\"\n",
    "    )\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        basic_metrics_df.to_excel(writer, sheet_name=\"基础指标\", index=False)\n",
    "        detailed_routes_df.to_excel(writer, sheet_name=\"线路详细数据\", index=False)\n",
    "        summary_df.to_excel(writer, sheet_name=\"线路汇总数据\", index=False)\n",
    "    print(f\"分析完成, 结果已保存至: {output_file_path.name}\")\n",
    "\n",
    "    stats_dict = {\n",
    "        \"公司 (Company)\": company_key,\n",
    "        \"原始数据量\": original_data_count,\n",
    "        \"因时间戳缺失删除\": time_dropped_count,\n",
    "        \"因时限空值删除\": nan_dropped_count,\n",
    "        \"因范围不符删除\": value_dropped_count,\n",
    "    }\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "# 在Jupyter环境中，period变量应已在Cell 0中定义\n",
    "if \"period\" in locals() or \"period\" in globals():\n",
    "    main(period)\n",
    "else:\n",
    "    print(\"错误：未找到 'period' 变量。请确保您已运行定义本期的第一个cell。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
