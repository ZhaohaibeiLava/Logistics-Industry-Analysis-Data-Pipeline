{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaaefffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 1: 导入库并设置项目结构\n",
    "# --------------------------------------------------\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# 忽略来自 openpyxl 的特定 UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "font_path = \"微软雅黑.ttf\"\n",
    "\n",
    "try:\n",
    "    chinese_font = FontProperties(fname=font_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Font file not found. Please provide the correct path.\")\n",
    "    # Fallback to a generic font if the file is not found\n",
    "    chinese_font = FontProperties()\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "\n",
    "# --- 项目路径设置  ---\n",
    "# .\n",
    "# ├── 1_data_preprocess.ipynb\n",
    "# └── 报告数据/\n",
    "#     ├── 输入/\n",
    "#     │   ├── 安监数据/              (存放老师给的原始Excel文件；10家公司，10个文件)\n",
    "#     │   └── basic_data.xlsx          (城市信息、线路信息)\n",
    "#     ├── 输出/                      (存放所有最终生成的报告)\n",
    "#     ├── temp/\n",
    "#     │   ├── 1_待上传猪猪云数据/        (需要逐个手动上传到猪猪云的文件；8家公司，16个文件，排除顺丰和中通)\n",
    "#     │   ├── 2_猪猪云下载数据/          (【手动放入】存放从猪猪云下载的结果文件；8家公司，16个文件，排除顺丰和中通)\n",
    "#     │   ├── 3_猪猪云合并数据/         （猪猪云下载数据按公司合并后数据；8家公司，8个文件，排除顺丰和中通）\n",
    "#     │   ├── 4_logistics数据         （存放logistics数据——提取完整物流信息的时间戳后的数据；8家公司，8个文件，排除顺丰和中通）\n",
    "#     └── └── 5_中转数据/               (存放中转数据——提取中转城市和平均中转次数后的数据；8家公司，8个文件，排除顺丰和中通)\n",
    "# 根目录\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "# 输入路径\n",
    "input_path = report_path / \"输入\"\n",
    "anjian_data_path = input_path / \"安监数据\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "# 输出路径\n",
    "output_path = report_path / \"输出\"\n",
    "# 中间过程文件路径（自动创建，用于存放临时文件）\n",
    "temp_path = report_path / \"temp\"\n",
    "upload_split_path = temp_path / \"1_待上传猪猪云文件\"  # 存放拆分后待上传的文件\n",
    "zhuzhuyun_download_path = temp_path / \"2_猪猪云下载数据\"  # 关键：这是手动放置文件的目录\n",
    "zhuzhuyun_merge_path = temp_path / \"3_猪猪云合并数据\"\n",
    "pycharm_input_path = temp_path / \"4_logistics数据\"\n",
    "transit_data_path = temp_path / \"5_中转数据\"\n",
    "\n",
    "# 创建所有需要的文件夹\n",
    "for p in [\n",
    "    report_path,\n",
    "    input_path,\n",
    "    anjian_data_path,\n",
    "    zhuzhuyun_download_path,\n",
    "    zhuzhuyun_merge_path,\n",
    "    transit_data_path,\n",
    "    output_path,\n",
    "    temp_path,\n",
    "    upload_split_path,\n",
    "    pycharm_input_path,\n",
    "]:\n",
    "    p.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd28b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⭕️ 开始处理数据，读取目录为：'/Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输出/data_analysis_result'\n",
      "   - 正在处理公司: 顺丰，文件: 顺丰_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: EMS，文件: EMS_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: 中通，文件: 中通_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: 极兔，文件: 极兔_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: 韵达，文件: 韵达_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: 圆通，文件: 圆通_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: 京东，文件: 京东_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: 申通，文件: 申通_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: 德邦，文件: 德邦_data_analysis_result.xlsx...\n",
      "   - 正在处理公司: 邮政，文件: 邮政_data_analysis_result.xlsx...\n",
      "\n",
      "==================================================\n",
      "✅ 任务完成！\n",
      "月度汇总数据已保存至: ‘/Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输出/1_汇总数据/月度汇总数据.xlsx’\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 2: 汇总数据\n",
    "# --------------------------------------------------\n",
    "# --- 路径设置 ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_dir = report_path / \"输出\" / \"data_analysis_result\"\n",
    "output_dir = report_path / \"输出\" / \"1_汇总数据\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_metrics_from_basic_sheet(df, kilo_params):\n",
    "    \"\"\"\n",
    "    从“基础指标”DataFrame中直接提取所有需要的指标。\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.Series(dtype=\"float64\")\n",
    "\n",
    "    metrics_table = df.set_index(\"项目\")\n",
    "    results = {}\n",
    "\n",
    "    time_limit_cols = [\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"运输时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "    ]\n",
    "    on_time_cols = [\"72小时准时率\", \"48小时准时率\"]\n",
    "\n",
    "    for col in time_limit_cols + on_time_cols:\n",
    "        results[col] = metrics_table.loc[col, \"mean\"]\n",
    "\n",
    "    for param in kilo_params:\n",
    "        results[param] = metrics_table.loc[\"全程时限\", param]\n",
    "\n",
    "    return pd.Series(results)\n",
    "\n",
    "\n",
    "def main(companies, params):\n",
    "    \"\"\"\n",
    "    主函数，处理所有公司的文件并生成汇总报告。\n",
    "    (此函数逻辑正确，无需修改)\n",
    "    \"\"\"\n",
    "    all_metrics_cols = [\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"运输时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"72小时准时率\",\n",
    "        \"48小时准时率\",\n",
    "    ] + params\n",
    "\n",
    "    summary_df = pd.DataFrame(index=companies, columns=all_metrics_cols)\n",
    "\n",
    "    print(f\"⭕️ 开始处理数据，读取目录为：'{input_dir}'\")\n",
    "\n",
    "    for company in companies:\n",
    "        # 文件名是根据公司名动态生成的\n",
    "        file_name = f\"{company}_data_analysis_result.xlsx\"\n",
    "        # 文件路径是 输入目录 + 文件名\n",
    "        file_path = input_dir / file_name\n",
    "\n",
    "        try:\n",
    "            print(f\"   - 正在处理公司: {company}，文件: {file_name}...\")\n",
    "\n",
    "            # 读取“基础指标”sheet\n",
    "            basic_metrics_df = pd.read_excel(file_path, sheet_name=\"基础指标\")\n",
    "\n",
    "            # 从基础指标中直接提取\n",
    "            metrics = extract_metrics_from_basic_sheet(basic_metrics_df, params)\n",
    "\n",
    "            # 填充到汇总DataFrame\n",
    "            summary_df.loc[company, metrics.index] = metrics\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"   ⚠️ 警告：未找到文件 {file_path}，跳过该公司。\")\n",
    "        except KeyError as e:\n",
    "            print(\n",
    "                f\"   ❌ 错误: 在文件 {file_path} 的'基础指标'Sheet中未找到关键项目: {e}。\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 错误：处理文件 {file_path} 时发生错误: {e}\")\n",
    "\n",
    "    summary_df = summary_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return summary_df.round(4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    companies_list = [\n",
    "        \"顺丰\",\n",
    "        \"EMS\",\n",
    "        \"中通\",\n",
    "        \"极兔\",\n",
    "        \"韵达\",\n",
    "        \"圆通\",\n",
    "        \"京东\",\n",
    "        \"申通\",\n",
    "        \"德邦\",\n",
    "        \"邮政\",\n",
    "    ]\n",
    "    kilometer_params = [\"0-600\", \"600-1500\", \"1500-2500\", \"2500以上\"]\n",
    "\n",
    "    final_summary = main(companies_list, kilometer_params)\n",
    "\n",
    "    output_file = output_dir / \"月度汇总数据.xlsx\"\n",
    "    final_summary.to_excel(output_file)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"✅ 任务完成！\")\n",
    "    print(f\"月度汇总数据已保存至: ‘{output_file}’\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ca03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始执行数据分析工作流...\n",
      "✅ 成功从 'basic_data.xlsx' 加载 50 个城市。\n",
      "\n",
      "==========================================================\n",
      "➡️ Part 1: 开始执行分路线时限分析 (RouteAnalysis)\n",
      "==========================================================\n",
      "  - 正在处理: 圆通\n",
      "  - 正在处理: 申通\n",
      "  - 正在处理: 中通\n",
      "  - 正在处理: 极兔\n",
      "  - 正在处理: 韵达\n",
      "  ✅ [Part 1] 分路线时限分析完成。\n",
      "\n",
      "==========================================================\n",
      "➡️ Part 2: 开始执行宏观比例/时长分析 (MacroRatioAnalysis)\n",
      "==========================================================\n",
      "  - 正在处理: 中通\n",
      "  - 正在处理: 圆通\n",
      "  - 正在处理: 极兔\n",
      "  - 正在处理: 申通\n",
      "  - 正在处理: 韵达\n",
      "  - 正在处理: 顺丰\n",
      "  - 正在处理: 京东\n",
      "  - 正在处理: EMS\n",
      "  - 正在处理: 德邦\n",
      "  ✅ [Part 2] 宏观指标分析完成。\n",
      "\n",
      "... 正在整合结果到Excel文件 ...\n",
      "  - Sheet 'summary_tongdatu' 已写入。\n",
      "  - Sheet 'detail_tongdatu' 已写入。\n",
      "\n",
      "🎉🎉🎉 所有任务已全部完成！最终报告已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输出/5_通达兔数据/通达兔分段分析.xlsx 🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 3: 通达兔专题分析\n",
    "# --------------------------------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 路径设置 ---\n",
    "ROOT_PATH = Path.cwd()\n",
    "DATA_ANALYSIS_DIR = ROOT_PATH / \"报告数据\" / \"输出\" / \"data_analysis_result\"\n",
    "OUTPUT_DIR = ROOT_PATH / \"报告数据\" / \"输出\" / \"5_通达兔数据\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BASIC_DATA_PATH = ROOT_PATH / \"报告数据\" / \"输入\" / \"basic_data.xlsx\"\n",
    "\n",
    "\n",
    "# --- 公司和城市列表 ---\n",
    "def load_city_list_from_excel(file_path, sheet_name):\n",
    "    try:\n",
    "        if not file_path.exists():\n",
    "            print(\n",
    "                f\"❌ 严重错误：基础数据文件 'basic_data.xlsx' 未在以下路径找到:\\n{file_path}\"\n",
    "            )\n",
    "            return None\n",
    "        df_cities = pd.read_excel(file_path, sheet_name=sheet_name, header=0)\n",
    "        if df_cities.columns[0] not in df_cities.columns:\n",
    "            print(f\"❌ 严重错误: 在Sheet '{sheet_name}' 中未找到城市列。\")\n",
    "            return None\n",
    "        city_list = (\n",
    "            df_cities.iloc[:, 0].dropna().astype(str).str.strip().unique().tolist()\n",
    "        )\n",
    "        if not city_list:\n",
    "            print(\n",
    "                f\"❌ 严重错误：在 '{file_path.name}' 的 '{sheet_name}' sheet页中未能加载到任何城市数据。\"\n",
    "            )\n",
    "            return None\n",
    "        print(f\"✅ 成功从 'basic_data.xlsx' 加载 {len(city_list)} 个城市。\")\n",
    "        return city_list\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 严重错误：加载城市列表时发生意外错误: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 公司列表定义\n",
    "COMPANY_FILE_MAP = {\n",
    "    \"中通\": \"中通\",\n",
    "    \"圆通\": \"圆通\",\n",
    "    \"极兔\": \"极兔\",\n",
    "    \"申通\": \"申通\",\n",
    "    \"韵达\": \"韵达\",\n",
    "    \"顺丰\": \"顺丰\",\n",
    "    \"京东\": \"京东\",\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"德邦\": \"德邦\",\n",
    "    \"快包\": \"邮政\",\n",
    "}\n",
    "COMPANIES_FOR_MACRO_ANALYSIS = [c for c in COMPANY_FILE_MAP.keys() if c != \"快包\"]\n",
    "COMPANIES_TONGDATU = [\"圆通\", \"申通\", \"中通\", \"极兔\", \"韵达\"]\n",
    "CITY_LIST = None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Part 1: 分路线时限分析 (逻辑不变)\n",
    "# ==============================================================================\n",
    "def run_route_analysis(city_list):\n",
    "    print(\"\\n==========================================================\")\n",
    "    print(\"➡️ Part 1: 开始执行分路线时限分析 (RouteAnalysis)\")\n",
    "    print(\"==========================================================\")\n",
    "    city_routes = [f\"{c1}-{c2}\" for c1 in city_list for c2 in city_list if c1 != c2]\n",
    "    aggregated_results_df = pd.DataFrame({\"路线\": city_routes})\n",
    "    required_time_cols = [\n",
    "        \"揽收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"签收时间\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达城市\",\n",
    "    ]\n",
    "    for company in COMPANIES_TONGDATU:\n",
    "        file_prefix = COMPANY_FILE_MAP.get(company)\n",
    "        file_path = DATA_ANALYSIS_DIR / f\"{file_prefix}_data_analysis_result.xlsx\"\n",
    "        if not file_path.exists():\n",
    "            continue\n",
    "        print(f\"  - 正在处理: {company}\")\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=\"线路详细数据\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - ⚠️ 警告: 读取文件 {file_path.name} 失败: {e}\")\n",
    "            continue\n",
    "        if not all(col in df.columns for col in required_time_cols):\n",
    "            print(\n",
    "                f\"  - ⚠️ 警告: {file_path.name} 的'线路详细数据'sheet缺少必要的时间列，跳过 Part 1 的 {company} 分析。\"\n",
    "            )\n",
    "            continue\n",
    "        for col in required_time_cols:\n",
    "            if \"时间\" in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        df.dropna(\n",
    "            subset=[col for col in required_time_cols if \"时间\" in col], inplace=True\n",
    "        )\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df[\"路线\"] = (\n",
    "            df[\"寄出城市\"].astype(str).str.strip()\n",
    "            + \"-\"\n",
    "            + df[\"寄达城市\"].astype(str).str.strip()\n",
    "        )\n",
    "        df[f\"揽收-到达寄出地分拣中心（小时）{company}\"] = (\n",
    "            df[\"到达分拣中心时间\"] - df[\"揽收时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        df[f\"到达寄出地分拣中心-离开寄达地分拣中心（小时）{company}\"] = (\n",
    "            df[\"离开收件城市分拣中心时间\"] - df[\"到达分拣中心时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        df[f\"离开寄达地分拣中心-签收（小时）{company}\"] = (\n",
    "            df[\"签收时间\"] - df[\"离开收件城市分拣中心时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        company_metrics = [\n",
    "            f\"揽收-到达寄出地分拣中心（小时）{company}\",\n",
    "            f\"到达寄出地分拣中心-离开寄达地分拣中心（小时）{company}\",\n",
    "            f\"离开寄达地分拣中心-签收（小时）{company}\",\n",
    "        ]\n",
    "        company_agg = df.groupby(\"路线\")[company_metrics].mean().reset_index()\n",
    "        aggregated_results_df = pd.merge(\n",
    "            aggregated_results_df, company_agg, on=\"路线\", how=\"left\"\n",
    "        )\n",
    "    metric_templates = [\n",
    "        \"揽收-到达寄出地分拣中心（小时）\",\n",
    "        \"到达寄出地分拣中心-离开寄达地分拣中心（小时）\",\n",
    "        \"离开寄达地分拣中心-签收（小时）\",\n",
    "    ]\n",
    "    for metric in metric_templates:\n",
    "        company_cols = [\n",
    "            f\"{metric}{comp}\"\n",
    "            for comp in COMPANIES_TONGDATU\n",
    "            if f\"{metric}{comp}\" in aggregated_results_df.columns\n",
    "        ]\n",
    "        if company_cols:\n",
    "            aggregated_results_df[f\"{metric}通达兔均值\"] = aggregated_results_df[\n",
    "                company_cols\n",
    "            ].mean(axis=1)\n",
    "            aggregated_results_df[f\"{metric}通达兔最优\"] = aggregated_results_df[\n",
    "                company_cols\n",
    "            ].min(axis=1)\n",
    "    print(\"  ✅ [Part 1] 分路线时限分析完成。\")\n",
    "    return aggregated_results_df\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Part 2: 宏观比例/时长分析 (按新原则修改)\n",
    "# ==============================================================================\n",
    "def run_macro_ratio_analysis():\n",
    "    print(\"\\n==========================================================\")\n",
    "    print(\"➡️ Part 2: 开始执行宏观比例/时长分析 (MacroRatioAnalysis)\")\n",
    "    print(\"==========================================================\")\n",
    "    all_time_cols = [\n",
    "        \"揽收时间\",\n",
    "        \"到达分拣中心时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"离开收件城市分拣中心时间\",\n",
    "        \"签收时间\",\n",
    "    ]\n",
    "    results = []\n",
    "\n",
    "    for company_key in COMPANIES_FOR_MACRO_ANALYSIS:\n",
    "        file_prefix = COMPANY_FILE_MAP.get(company_key, company_key)\n",
    "        file_path = DATA_ANALYSIS_DIR / f\"{file_prefix}_data_analysis_result.xlsx\"\n",
    "        if not file_path.exists():\n",
    "            continue\n",
    "        print(f\"  - 正在处理: {company_key}\")\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=\"线路详细数据\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - ⚠️ 警告: 读取文件 {file_path.name} 失败: {e}\")\n",
    "            continue\n",
    "        if df.empty or not all(col in df.columns for col in all_time_cols):\n",
    "            print(\n",
    "                f\"  - ⚠️ 警告: {file_path.name} 的'线路详细数据'sheet不完整，跳过 Part 2 的 {company_key} 分析。\"\n",
    "            )\n",
    "            continue\n",
    "        for col in all_time_cols:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        df.dropna(subset=all_time_cols, inplace=True)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        company_metrics = {\"公司\": company_key}\n",
    "        company_metrics[\"揽件和进分拨中心同一天比例\"] = (\n",
    "            df[\"揽收时间\"].dt.date == df[\"到达分拣中心时间\"].dt.date\n",
    "        ).mean()\n",
    "        company_metrics[\"从揽件到离开寄出地在12小时之内的比例\"] = (\n",
    "            (df[\"离开寄件城市时间\"] - df[\"揽收时间\"]) / np.timedelta64(1, \"h\") < 12\n",
    "        ).mean()\n",
    "        handling_time_out = (\n",
    "            df[\"离开寄件城市时间\"] - df[\"到达分拣中心时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        handling_time_in = (\n",
    "            df[\"离开收件城市分拣中心时间\"] - df[\"到达收件城市时间\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        company_metrics[\"寄出地分拣中心处理时长\"] = handling_time_out.mean()\n",
    "        company_metrics[\"寄达地分拣中心处理时长\"] = handling_time_in.mean()\n",
    "        company_metrics[\"寄达地分拨中心处理超过12小时比例\"] = (\n",
    "            handling_time_in > 12\n",
    "        ).mean()\n",
    "        results.append(company_metrics)\n",
    "\n",
    "    if not results:\n",
    "        print(\"  - ❌ 错误: 未能从任何公司文件中计算出宏观指标。\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_final = pd.DataFrame(results).set_index(\"公司\").T\n",
    "\n",
    "    # 计算通达兔均值和最优值\n",
    "    higher_is_better_metrics = [\n",
    "        \"揽件和进分拨中心同一天比例\",\n",
    "        \"从揽件到离开寄出地在12小时之内的比例\",\n",
    "    ]\n",
    "    lower_is_better_metrics = [\n",
    "        \"寄出地分拣中心处理时长\",\n",
    "        \"寄达地分拣中心处理时长\",\n",
    "        \"寄达地分拨中心处理超过12小时比例\",\n",
    "    ]\n",
    "\n",
    "    tongdatu_cols = df_final.columns.intersection(COMPANIES_TONGDATU).tolist()\n",
    "    if tongdatu_cols:\n",
    "        df_final[\"通达兔均值\"] = df_final[tongdatu_cols].mean(axis=1)\n",
    "\n",
    "        # (核心修正) 在赋值前，先筛选出实际存在的指标\n",
    "        existing_higher_metrics = [\n",
    "            m for m in higher_is_better_metrics if m in df_final.index\n",
    "        ]\n",
    "        if existing_higher_metrics:\n",
    "            df_final.loc[existing_higher_metrics, \"通达兔最优值\"] = df_final.loc[\n",
    "                existing_higher_metrics, tongdatu_cols\n",
    "            ].max(axis=1)\n",
    "\n",
    "        existing_lower_metrics = [\n",
    "            m for m in lower_is_better_metrics if m in df_final.index\n",
    "        ]\n",
    "        if existing_lower_metrics:\n",
    "            df_final.loc[existing_lower_metrics, \"通达兔最优值\"] = df_final.loc[\n",
    "                existing_lower_metrics, tongdatu_cols\n",
    "            ].min(axis=1)\n",
    "\n",
    "    print(\"  ✅ [Part 2] 宏观指标分析完成。\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 开始执行数据分析工作流...\")\n",
    "    CITY_LIST = load_city_list_from_excel(BASIC_DATA_PATH, \"50_focus_cities\")\n",
    "    if CITY_LIST is None:\n",
    "        print(\"\\n🛑 由于无法加载城市列表，工作流已中止。\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    df_detail = run_route_analysis(CITY_LIST)\n",
    "    df_summary = run_macro_ratio_analysis()\n",
    "\n",
    "    final_output_path = OUTPUT_DIR / \"通达兔分段分析.xlsx\"\n",
    "    print(f\"\\n... 正在整合结果到Excel文件 ...\")\n",
    "    with pd.ExcelWriter(final_output_path, engine=\"openpyxl\") as writer:\n",
    "        if not df_summary.empty:\n",
    "            df_summary.to_excel(writer, sheet_name=\"summary_tongdatu\", index=True)\n",
    "            print(f\"  - Sheet 'summary_tongdatu' 已写入。\")\n",
    "        else:\n",
    "            print(\"  - ⚠️ 警告: 宏观分析结果为空，未写入 'summary_tongdatu' sheet。\")\n",
    "\n",
    "        if not df_detail.empty:\n",
    "            df_detail.to_excel(writer, sheet_name=\"detail_tongdatu\", index=False)\n",
    "            print(f\"  - Sheet 'detail_tongdatu' 已写入。\")\n",
    "        else:\n",
    "            print(\"  - ⚠️ 警告: 路线分析结果为空，未写入 'detail_tongdatu' sheet。\")\n",
    "    print(f\"\\n🎉🎉🎉 所有任务已全部完成！最终报告已保存至: {final_output_path} 🎉🎉🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0206de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "◎◎◎ 分析流程启动 ◎◎◎\n",
      "--- 任务开始：生成最终线路明细报告 ---\n",
      "\n",
      "[1/4] 动态构建线路全集并合并基础信息...\n",
      "\n",
      "[2/4] 从'线路汇总数据'提取所有预计算指标...\n",
      "\n",
      "[3/4] 计算各项指标的统计值...\n",
      "\n",
      "[4/4] 计算排名...\n",
      "\n",
      "- 正在清理完全为空的指标列...\n",
      "- 清理完成。\n",
      "\n",
      "--- 正在生成数据中台报告: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输出/分析总报告.xlsx ---\n",
      "  - 正在写入'最终线路明细结果' Sheet...\n",
      "  - 正在按 '寄出省份, 寄出城市' 进行加权平均聚合...\n",
      "  - 正在写入'寄出地汇总' Sheet...\n",
      "  - 正在按 '寄出省份, 寄出城市' 进行加权平均聚合...\n",
      "  - 正在写入'寄达地汇总' Sheet...\n",
      "--- ✓ 数据中台生成完毕 (包含汇总表) ---\n",
      "\n",
      "🎉🎉🎉 恭喜！数据中台任务已全部执行完毕！🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 4: 数据中台生成 (最终正确版 - 统一口径)\n",
    "# ==============================================================================\n",
    "import traceback\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0. 全局配置与辅助函数 ---\n",
    "COMPANY_MAPPING = {\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"德邦\": \"德邦\",\n",
    "    \"极兔\": \"极兔\",\n",
    "    \"圆通\": \"圆通\",\n",
    "    \"顺丰\": \"顺丰\",\n",
    "    \"中通\": \"中通\",\n",
    "    \"京东\": \"京东\",\n",
    "    \"韵达\": \"韵达\",\n",
    "    \"申通\": \"申通\",\n",
    "    \"邮政\": \"快包\",\n",
    "    \"快包\": \"快包\",\n",
    "}\n",
    "\n",
    "# 明确定义各个公司列表的用途\n",
    "COMPANIES_FOR_INDUSTRY_COMPARISON = [\n",
    "    \"EMS\",\n",
    "    \"中通\",\n",
    "    \"京东\",\n",
    "    \"圆通\",\n",
    "    \"德邦\",\n",
    "    \"极兔\",\n",
    "    \"申通\",\n",
    "    \"韵达\",\n",
    "    \"顺丰\",\n",
    "]\n",
    "COMPANIES_NINE_MAJOR = COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "COMPANIES_ALL_TEN = COMPANIES_FOR_INDUSTRY_COMPARISON + [\"快包\"]\n",
    "\n",
    "\n",
    "def _find_company_key_from_filename(filename):\n",
    "    name = Path(filename).stem.replace(\"_data_analysis_result\", \"\")\n",
    "    sorted_keys = sorted(COMPANY_MAPPING.keys(), key=len, reverse=True)\n",
    "    for keyword in sorted_keys:\n",
    "        if keyword in name:\n",
    "            return COMPANY_MAPPING[keyword]\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- 1. 核心功能: 主报告生成 ---\n",
    "def generate_main_report():\n",
    "    print(\"--- 任务开始：生成最终线路明细报告 ---\")\n",
    "\n",
    "    base_path = Path.cwd()\n",
    "    report_path = base_path / \"报告数据\"\n",
    "    output_path = report_path / \"输出\"\n",
    "    data_analysis_path = output_path / \"data_analysis_result\"\n",
    "    base_data_path = report_path / \"输入\" / \"basic_data.xlsx\"\n",
    "\n",
    "    print(\"\\n[1/4] 动态构建线路全集并合并基础信息...\")\n",
    "    all_routes = set()\n",
    "    files_in_analysis_result = list(\n",
    "        data_analysis_path.glob(\"*_data_analysis_result.xlsx\")\n",
    "    )\n",
    "    if not files_in_analysis_result:\n",
    "        print(\n",
    "            f\"🔥🔥🔥 错误：在路径 '{data_analysis_path.resolve()}' 中未找到任何 '*_data_analysis_result.xlsx' 文件。\"\n",
    "        )\n",
    "        return None\n",
    "    for file_path in files_in_analysis_result:\n",
    "        try:\n",
    "            all_routes.update(\n",
    "                pd.read_excel(\n",
    "                    file_path,\n",
    "                    sheet_name=\"线路汇总数据\",\n",
    "                    usecols=[\"路线\"],\n",
    "                    engine=\"openpyxl\",\n",
    "                )[\"路线\"].unique()\n",
    "            )\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\n",
    "                f\" -> 警告: 文件 {file_path.name} 已损坏或不是有效的Excel文件，已跳过。\"\n",
    "            )\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\" -> 警告: 读取文件 {file_path.name} 路线列表失败: {e}\")\n",
    "    if not all_routes:\n",
    "        print(\"🔥🔥🔥 错误：未能从任何文件中构建线路列表，流程终止。\")\n",
    "        return None\n",
    "    df_result = pd.DataFrame(list(all_routes), columns=[\"路线\"])\n",
    "    try:\n",
    "        df_base_info = pd.read_excel(\n",
    "            base_data_path, sheet_name=\"inter-city_routes\", engine=\"openpyxl\"\n",
    "        ).rename(columns={\"公里\": \"线路里程\", \"经济圈\": \"城市圈\"})\n",
    "        cols_to_merge = [\n",
    "            \"寄出省份\",\n",
    "            \"寄出城市\",\n",
    "            \"寄达省份\",\n",
    "            \"寄达城市\",\n",
    "            \"路线\",\n",
    "            \"线路里程\",\n",
    "            \"城市圈\",\n",
    "        ]\n",
    "        df_result = pd.merge(\n",
    "            df_result,\n",
    "            df_base_info[[c for c in cols_to_merge if c in df_base_info.columns]],\n",
    "            on=\"路线\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\" -> 🔥🔥🔥 错误：合并基础信息失败: {e}\")\n",
    "\n",
    "    print(\"\\n[2/4] 从'线路汇总数据'提取所有预计算指标...\")\n",
    "    metrics_to_extract = {\n",
    "        \"快递数量\": \"快递数量\",\n",
    "        \"全程时限\": \"全程时限\",\n",
    "        \"寄出地处理时限\": \"寄出地处理时限\",\n",
    "        \"运输时限\": \"运输时限\",\n",
    "        \"寄达地处理时限\": \"寄达地处理时限\",\n",
    "        \"投递时限\": \"投递时限\",\n",
    "        \"揽收-到达寄出地分拣中心时长\": \"揽收-到达寄出地分拣中心时长\",\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\": \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\": \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "        \"离开寄达地分拣中心-派件\": \"离开寄达地分拣中心-派件\",\n",
    "        \"72小时准时率\": \"72小时准时率\",\n",
    "        \"48小时准时率\": \"48小时准时率\",\n",
    "        \"送达天数_80分位\": \"送达天数\",\n",
    "        \"中转次数\": \"平均中转次数\",\n",
    "    }\n",
    "    for file_path in files_in_analysis_result:\n",
    "        company_key = _find_company_key_from_filename(file_path.name)\n",
    "        if not company_key:\n",
    "            continue\n",
    "        try:\n",
    "            df_summary = pd.read_excel(\n",
    "                file_path, sheet_name=\"线路汇总数据\", engine=\"openpyxl\"\n",
    "            )\n",
    "            for source_col, target_metric in metrics_to_extract.items():\n",
    "                if source_col in df_summary.columns:\n",
    "                    if source_col == \"中转次数\":\n",
    "                        new_col_name = f\"{company_key}{target_metric}\"\n",
    "                    else:\n",
    "                        new_col_name = f\"{target_metric}{company_key}\"\n",
    "\n",
    "                    df_metric = (\n",
    "                        df_summary[[\"路线\", source_col]]\n",
    "                        .copy()\n",
    "                        .rename(columns={source_col: new_col_name})\n",
    "                    )\n",
    "                    df_result = pd.merge(df_result, df_metric, on=\"路线\", how=\"left\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\n",
    "                f\" -> 警告: 文件 {file_path.name} 已损坏或不是有效的Excel文件，已跳过。\"\n",
    "            )\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\" -> 错误: 处理文件 {file_path.name} 时出错: {e}\")\n",
    "\n",
    "    print(\"\\n[3/4] 计算各项指标的统计值...\")\n",
    "    all_transfer_cols = [\n",
    "        f\"{comp}平均中转次数\"\n",
    "        for comp in COMPANIES_ALL_TEN\n",
    "        if f\"{comp}平均中转次数\" in df_result.columns\n",
    "    ]\n",
    "    if all_transfer_cols:\n",
    "        cols_for_best_turnover = [c for c in all_transfer_cols if \"快包\" not in c]\n",
    "        if cols_for_best_turnover:\n",
    "            df_result[\"最优中转次数\"] = df_result[cols_for_best_turnover].min(axis=1)\n",
    "\n",
    "    stat_metrics = [\n",
    "        \"快递数量\",\n",
    "        \"48小时准时率\",\n",
    "        \"72小时准时率\",\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"运输时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"送达天数\",\n",
    "    ]\n",
    "    for metric in stat_metrics:\n",
    "        if metric == \"快递数量\":\n",
    "            total_qty_cols = [\n",
    "                f\"快递数量{comp}\"\n",
    "                for comp in COMPANIES_ALL_TEN\n",
    "                if f\"快递数量{comp}\" in df_result.columns\n",
    "            ]\n",
    "            if total_qty_cols:\n",
    "                df_result[f\"快递数量_total\"] = df_result[total_qty_cols].sum(axis=1)\n",
    "            continue\n",
    "        metric_cols = [\n",
    "            f\"{metric}{comp}\"\n",
    "            for comp in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "            if f\"{metric}{comp}\" in df_result.columns\n",
    "        ]\n",
    "        if not metric_cols:\n",
    "            continue\n",
    "        df_result[f\"{metric}_average\"] = df_result[metric_cols].mean(axis=1)\n",
    "        df_result[f\"{metric}_minimum\"] = df_result[metric_cols].min(axis=1)\n",
    "        df_result[f\"{metric}_maximum\"] = df_result[metric_cols].max(axis=1)\n",
    "\n",
    "    print(\"\\n[4/4] 计算排名...\")\n",
    "    rank_metrics = [\n",
    "        \"48小时准时率\",\n",
    "        \"72小时准时率\",\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"运输时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"送达天数\",\n",
    "    ]\n",
    "    for param in rank_metrics:\n",
    "        is_desc = \"准时率\" in param\n",
    "        ems_cols = [\n",
    "            f\"{param}{comp}\"\n",
    "            for comp in COMPANIES_NINE_MAJOR\n",
    "            if f\"{param}{comp}\" in df_result.columns\n",
    "        ]\n",
    "        if ems_cols and f\"{param}EMS\" in df_result.columns:\n",
    "            df_result[f\"{param}_ems排名\"] = df_result[ems_cols].rank(\n",
    "                axis=1, method=\"min\", ascending=not is_desc\n",
    "            )[f\"{param}EMS\"]\n",
    "        kb_cols = [\n",
    "            f\"{param}{comp}\"\n",
    "            for comp in COMPANIES_ALL_TEN\n",
    "            if comp != \"EMS\" and f\"{param}{comp}\" in df_result.columns\n",
    "        ]\n",
    "        if kb_cols and f\"{param}快包\" in df_result.columns:\n",
    "            df_result[f\"{param}_快包排名\"] = df_result[kb_cols].rank(\n",
    "                axis=1, method=\"min\", ascending=not is_desc\n",
    "            )[f\"{param}快包\"]\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def calculate_weighted_summary(df: pd.DataFrame, group_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"对给定的DataFrame按指定列进行分组，并计算加权平均值。\"\"\"\n",
    "    print(f\"  - 正在按 '{', '.join(group_cols)}' 进行加权平均聚合...\")\n",
    "\n",
    "    metrics_to_weight = [\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"运输时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"揽收-到达寄出地分拣中心时长\",\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "        \"离开寄达地分拣中心-派件\",\n",
    "        \"72小时准时率\",\n",
    "        \"48小时准时率\",\n",
    "        \"送达天数\",\n",
    "        \"平均中转次数\",\n",
    "    ]\n",
    "\n",
    "    summary_rows = []\n",
    "    for name, group in df.groupby(group_cols):\n",
    "        row = dict(zip(group_cols, name if isinstance(name, tuple) else [name]))\n",
    "        for comp in COMPANIES_ALL_TEN:\n",
    "            weight_col = f\"快递数量{comp}\"\n",
    "            if weight_col not in group.columns:\n",
    "                continue\n",
    "            total_weight = group[weight_col].sum()\n",
    "            row[weight_col] = total_weight\n",
    "            if total_weight > 0:\n",
    "                for metric in metrics_to_weight:\n",
    "                    metric_col = f\"{metric}{comp}\"\n",
    "                    if metric == \"平均中转次数\":\n",
    "                        metric_col = f\"{comp}{metric}\"\n",
    "                    if metric_col in group.columns:\n",
    "                        metric_values = pd.to_numeric(\n",
    "                            group[metric_col], errors=\"coerce\"\n",
    "                        )\n",
    "                        weight_values = pd.to_numeric(\n",
    "                            group[weight_col], errors=\"coerce\"\n",
    "                        )\n",
    "                        weighted_sum = (metric_values * weight_values).sum()\n",
    "                        row[metric_col] = weighted_sum / total_weight\n",
    "            else:\n",
    "                for metric in metrics_to_weight:\n",
    "                    metric_col = f\"{metric}{comp}\"\n",
    "                    if metric == \"平均中转次数\":\n",
    "                        metric_col = f\"{comp}{metric}\"\n",
    "                    if metric_col in group.columns:\n",
    "                        row[metric_col] = np.nan\n",
    "        summary_rows.append(row)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    # 重新计算行业统计值\n",
    "    for metric in metrics_to_weight:\n",
    "        source_metric_name = metric\n",
    "        metric_cols = [\n",
    "            f\"{source_metric_name}{comp}\"\n",
    "            for comp in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "            if f\"{source_metric_name}{comp}\" in summary_df.columns\n",
    "        ]\n",
    "\n",
    "        if not metric_cols:\n",
    "            continue\n",
    "        summary_df[f\"{source_metric_name}_average\"] = summary_df[metric_cols].mean(\n",
    "            axis=1\n",
    "        )\n",
    "        summary_df[f\"{source_metric_name}_minimum\"] = summary_df[metric_cols].min(\n",
    "            axis=1\n",
    "        )\n",
    "        summary_df[f\"{source_metric_name}_maximum\"] = summary_df[metric_cols].max(\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # 重新计算排名\n",
    "    rank_metrics = [\n",
    "        \"48小时准时率\",\n",
    "        \"72小时准时率\",\n",
    "        \"全程时限\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"运输时限\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "        \"送达天数\",\n",
    "    ]\n",
    "    for param in rank_metrics:\n",
    "        source_param_name = param\n",
    "        is_desc = \"准时率\" in param\n",
    "        ems_cols = [\n",
    "            f\"{source_param_name}{comp}\"\n",
    "            for comp in COMPANIES_NINE_MAJOR\n",
    "            if f\"{source_param_name}{comp}\" in summary_df.columns\n",
    "        ]\n",
    "        if ems_cols and f\"{source_param_name}EMS\" in summary_df.columns:\n",
    "            summary_df[f\"{source_param_name}_ems排名\"] = summary_df[ems_cols].rank(\n",
    "                axis=1, method=\"min\", ascending=not is_desc\n",
    "            )[f\"{source_param_name}EMS\"]\n",
    "        kb_cols = [\n",
    "            f\"{source_param_name}{comp}\"\n",
    "            for comp in COMPANIES_ALL_TEN\n",
    "            if comp != \"EMS\" and f\"{source_param_name}{comp}\" in summary_df.columns\n",
    "        ]\n",
    "        if kb_cols and f\"{source_param_name}快包\" in summary_df.columns:\n",
    "            summary_df[f\"{source_param_name}_快包排名\"] = summary_df[kb_cols].rank(\n",
    "                axis=1, method=\"min\", ascending=not is_desc\n",
    "            )[f\"{source_param_name}快包\"]\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_path = Path.cwd()\n",
    "    report_path = base_path / \"报告数据\"\n",
    "    output_path = report_path / \"输出\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    final_report_path = output_path / \"分析总报告.xlsx\"\n",
    "    print(f\"◎◎◎ 分析流程启动 ◎◎◎\")\n",
    "    df_main_report = generate_main_report()\n",
    "    if df_main_report is None:\n",
    "        print(\"\\n❌❌❌ 由于主报告生成失败，流程终止。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n- 正在清理完全为空的指标列...\")\n",
    "    df_main_report.dropna(axis=1, how=\"all\", inplace=True)\n",
    "    print(\"- 清理完成。\")\n",
    "\n",
    "    print(f\"\\n--- 正在生成数据中台报告: {final_report_path} ---\")\n",
    "    try:\n",
    "        with pd.ExcelWriter(final_report_path, engine=\"xlsxwriter\") as writer:\n",
    "            print(\"  - 正在写入'最终线路明细结果' Sheet...\")\n",
    "\n",
    "            df_main_to_write = df_main_report.copy()\n",
    "            for col in df_main_to_write.columns:\n",
    "                if (\n",
    "                    \"送达天数\" in col\n",
    "                    and \"排名\" not in col\n",
    "                    and df_main_to_write[col].dtype != \"object\"\n",
    "                ):\n",
    "                    df_main_to_write[col] = pd.to_numeric(\n",
    "                        df_main_to_write[col], errors=\"coerce\"\n",
    "                    ).round()\n",
    "                    df_main_to_write[col] = df_main_to_write[col].apply(\n",
    "                        lambda x: f\"T+{int(x)}\" if pd.notna(x) else x\n",
    "                    )\n",
    "\n",
    "            # 动态生成列顺序\n",
    "            base_info_cols = [\n",
    "                \"寄出省份\",\n",
    "                \"寄出城市\",\n",
    "                \"寄达省份\",\n",
    "                \"寄达城市\",\n",
    "                \"路线\",\n",
    "                \"线路里程\",\n",
    "                \"城市圈\",\n",
    "            ]\n",
    "            metrics_ordered = [\n",
    "                \"快递数量\",\n",
    "                \"全程时限\",\n",
    "                \"寄出地处理时限\",\n",
    "                \"运输时限\",\n",
    "                \"寄达地处理时限\",\n",
    "                \"投递时限\",\n",
    "                \"揽收-到达寄出地分拣中心时长\",\n",
    "                \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "                \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "                \"离开寄达地分拣中心-派件\",\n",
    "                \"72小时准时率\",\n",
    "                \"48小时准时率\",\n",
    "            ]\n",
    "            all_companies_ordered_cols = []\n",
    "            for comp in COMPANIES_ALL_TEN:\n",
    "                for metric in metrics_ordered:\n",
    "                    all_companies_ordered_cols.append(f\"{metric}{comp}\")\n",
    "\n",
    "            final_cols = base_info_cols + all_companies_ordered_cols\n",
    "            for comp in COMPANIES_ALL_TEN:\n",
    "                final_cols.append(f\"送达天数{comp}\")\n",
    "            for comp in COMPANIES_ALL_TEN:\n",
    "                final_cols.append(f\"{comp}平均中转次数\")\n",
    "            final_cols.append(\"最优中转次数\")\n",
    "\n",
    "            stat_rank_cols = [col for col in df_main_to_write.columns if \"_\" in col]\n",
    "            final_cols.extend(stat_rank_cols)\n",
    "\n",
    "            final_cols_exist = [\n",
    "                col for col in final_cols if col in df_main_to_write.columns\n",
    "            ]\n",
    "            final_cols_exist += [\n",
    "                col for col in df_main_to_write.columns if col not in final_cols_exist\n",
    "            ]\n",
    "\n",
    "            df_main_to_write[final_cols_exist].to_excel(\n",
    "                writer, sheet_name=\"最终线路明细结果\", index=False\n",
    "            )\n",
    "\n",
    "            # 寄出地汇总\n",
    "            origin_group_cols = [\"寄出省份\", \"寄出城市\"]\n",
    "            if all(c in df_main_report.columns for c in origin_group_cols):\n",
    "                df_origin_summary = calculate_weighted_summary(\n",
    "                    df_main_report, origin_group_cols\n",
    "                )\n",
    "                print(\"  - 正在写入'寄出地汇总' Sheet...\")\n",
    "                df_origin_summary_to_write = df_origin_summary.copy()\n",
    "                df_origin_summary_to_write.dropna(axis=1, how=\"all\", inplace=True)\n",
    "                for col in df_origin_summary_to_write.columns:\n",
    "                    if (\n",
    "                        \"送达天数\" in col\n",
    "                        and \"排名\" not in col\n",
    "                        and df_origin_summary_to_write[col].dtype != \"object\"\n",
    "                    ):\n",
    "                        df_origin_summary_to_write[col] = pd.to_numeric(\n",
    "                            df_origin_summary_to_write[col], errors=\"coerce\"\n",
    "                        ).round()\n",
    "                        df_origin_summary_to_write[col] = df_origin_summary_to_write[\n",
    "                            col\n",
    "                        ].apply(lambda x: f\"T+{int(x)}\" if pd.notna(x) else x)\n",
    "                df_origin_summary_to_write.to_excel(\n",
    "                    writer, sheet_name=\"寄出地汇总\", index=False\n",
    "                )\n",
    "\n",
    "            # 寄达地汇总\n",
    "            dest_group_cols = [\"寄达省份\", \"寄达城市\"]\n",
    "            if all(c in df_main_report.columns for c in dest_group_cols):\n",
    "                # 【修复】: 在main函数内定义metrics_to_weight以解决作用域问题\n",
    "                metrics_to_weight = [\n",
    "                    \"全程时限\",\n",
    "                    \"寄出地处理时限\",\n",
    "                    \"运输时限\",\n",
    "                    \"寄达地处理时限\",\n",
    "                    \"投递时限\",\n",
    "                    \"揽收-到达寄出地分拣中心时长\",\n",
    "                    \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "                    \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "                    \"离开寄达地分拣中心-派件\",\n",
    "                    \"72小时准时率\",\n",
    "                    \"48小时准时率\",\n",
    "                    \"送达天数\",\n",
    "                    \"平均中转次数\",\n",
    "                ]\n",
    "\n",
    "                needed_cols = dest_group_cols.copy()\n",
    "                all_metrics_and_qty = metrics_to_weight + [\"快递数量\"]\n",
    "                for comp in COMPANIES_ALL_TEN:\n",
    "                    for metric in all_metrics_and_qty:\n",
    "                        if metric == \"平均中转次数\":\n",
    "                            col_name = f\"{comp}{metric}\"\n",
    "                        else:\n",
    "                            col_name = f\"{metric}{comp}\"\n",
    "                        if col_name in df_main_report.columns:\n",
    "                            needed_cols.append(col_name)\n",
    "\n",
    "                df_dest_subset = df_main_report[needed_cols]\n",
    "                df_dest_temp = df_dest_subset.rename(\n",
    "                    columns={\"寄达省份\": \"寄出省份\", \"寄达城市\": \"寄出城市\"}\n",
    "                )\n",
    "\n",
    "                df_dest_summary = calculate_weighted_summary(\n",
    "                    df_dest_temp, origin_group_cols\n",
    "                )\n",
    "                df_dest_summary.rename(\n",
    "                    columns={\"寄出省份\": \"寄达省份\", \"寄出城市\": \"寄达城市\"},\n",
    "                    inplace=True,\n",
    "                )\n",
    "                print(\"  - 正在写入'寄达地汇总' Sheet...\")\n",
    "                df_dest_summary_to_write = df_dest_summary.copy()\n",
    "                df_dest_summary_to_write.dropna(axis=1, how=\"all\", inplace=True)\n",
    "                for col in df_dest_summary_to_write.columns:\n",
    "                    if (\n",
    "                        \"送达天数\" in col\n",
    "                        and \"排名\" not in col\n",
    "                        and df_dest_summary_to_write[col].dtype != \"object\"\n",
    "                    ):\n",
    "                        df_dest_summary_to_write[col] = pd.to_numeric(\n",
    "                            df_dest_summary_to_write[col], errors=\"coerce\"\n",
    "                        ).round()\n",
    "                        df_dest_summary_to_write[col] = df_dest_summary_to_write[\n",
    "                            col\n",
    "                        ].apply(lambda x: f\"T+{int(x)}\" if pd.notna(x) else x)\n",
    "                df_dest_summary_to_write.to_excel(\n",
    "                    writer, sheet_name=\"寄达地汇总\", index=False\n",
    "                )\n",
    "\n",
    "        print(\"--- ✓ 数据中台生成完毕 (包含汇总表) ---\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> 🔥🔥🔥 写入数据中台失败: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "    print(\"\\n🎉🎉🎉 恭喜！数据中台任务已全部执行完毕！🎉🎉🎉\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c099ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 任务开始：生成中通月报 (基于已生成的主报告) ---\n",
      "  - ✓ 成功读取主报告。\n",
      "  -> 开始计算所有公司的达成率 (基于日历日)...\n",
      "    - 正在计算 '快包' 的达成率...\n",
      "    - 正在计算 '京东' 的达成率...\n",
      "    - 正在计算 '申通' 的达成率...\n",
      "    - 正在计算 '极兔' 的达成率...\n",
      "    - 正在计算 '圆通' 的达成率...\n",
      "    - 正在计算 '顺丰' 的达成率...\n",
      "    - 正在计算 '韵达' 的达成率...\n",
      "    - 正在计算 '德邦' 的达成率...\n",
      "    - 正在计算 'EMS' 的达成率...\n",
      "    - 正在计算 '中通' 的达成率...\n",
      "  -> ✓ 所有公司达成率计算完毕。\n",
      "  - [1/5] 正在准备'中通报告数据' Sheet...\n",
      "  - [2/5] 正在准备'线路详细分析' Sheet...\n",
      "  - [3/5] 正在准备'寄出城市汇总'和'寄达城市汇总' Sheets...\n",
      "  - [4/5] ✓ 汇总表准备完毕。\n",
      "  - [5/5] 正在写入Excel文件并应用格式...\n",
      "--- ✓ 中通月报生成完毕 --- \n",
      "文件已保存至: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输出/中通月报.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 5: 中通月报生成\n",
    "# ==============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 重新定义必要的全局变量和函数 ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"报告数据\"\n",
    "input_path = report_path / \"输入\"\n",
    "output_path = report_path / \"输出\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "\n",
    "COMPANY_MAPPING = {\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"德邦\": \"德邦\",\n",
    "    \"极兔\": \"极兔\",\n",
    "    \"圆通\": \"圆通\",\n",
    "    \"顺丰\": \"顺丰\",\n",
    "    \"中通\": \"中通\",\n",
    "    \"京东\": \"京东\",\n",
    "    \"韵达\": \"韵达\",\n",
    "    \"申通\": \"申通\",\n",
    "    \"邮政\": \"快包\",\n",
    "    \"快包\": \"快包\",\n",
    "}\n",
    "COMPANIES_NINE_MAJOR = [\n",
    "    \"EMS\",\n",
    "    \"中通\",\n",
    "    \"京东\",\n",
    "    \"圆通\",\n",
    "    \"德邦\",\n",
    "    \"极兔\",\n",
    "    \"申通\",\n",
    "    \"韵达\",\n",
    "    \"顺丰\",\n",
    "]\n",
    "COMPANIES_TONGDATU = [\"圆通\", \"申通\", \"中通\", \"极兔\", \"韵达\"]\n",
    "\n",
    "\n",
    "def _find_company_key_from_filename(filename):\n",
    "    name = Path(filename).stem.replace(\"_data_analysis_result\", \"\")\n",
    "    for keyword, key in COMPANY_MAPPING.items():\n",
    "        if keyword in name:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- 计算所有公司达成率的辅助函数 ---\n",
    "def _calculate_all_achievement_rates(df_standard):\n",
    "    \"\"\"\n",
    "    计算所有公司在所有标准线路上的达成率\n",
    "    基于“揽收时间”和“签收时间”的日历日差异计算\n",
    "    T+0 为当天达，T+1 为次日达。\n",
    "    \"\"\"\n",
    "    print(\"  -> 开始计算所有公司的达成率 (基于日历日)...\")\n",
    "    data_analysis_path = output_path / \"data_analysis_result\"\n",
    "    df_all_rates = df_standard[[\"路线\"]].copy()\n",
    "\n",
    "    START_DATE_COL = \"揽收时间\"\n",
    "    END_DATE_COL = \"签收时间\"\n",
    "\n",
    "    for file_path in data_analysis_path.glob(\"*_data_analysis_result.xlsx\"):\n",
    "        company_key = _find_company_key_from_filename(file_path.name)\n",
    "        if not company_key:\n",
    "            continue\n",
    "        print(f\"    - 正在计算 '{company_key}' 的达成率...\")\n",
    "        try:\n",
    "            df_detail = pd.read_excel(file_path, sheet_name=\"线路详细数据\")\n",
    "            df_detail[\"路线\"] = df_detail[\"寄出城市\"] + \"-\" + df_detail[\"寄达城市\"]\n",
    "\n",
    "            if (\n",
    "                START_DATE_COL not in df_detail.columns\n",
    "                or END_DATE_COL not in df_detail.columns\n",
    "            ):\n",
    "                print(\n",
    "                    f\"      -> 警告：文件 '{file_path.name}' 缺少 '{START_DATE_COL}' 或 '{END_DATE_COL}' 列，已跳过。\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            df_detail[START_DATE_COL] = pd.to_datetime(\n",
    "                df_detail[START_DATE_COL], errors=\"coerce\"\n",
    "            )\n",
    "            df_detail[END_DATE_COL] = pd.to_datetime(\n",
    "                df_detail[END_DATE_COL], errors=\"coerce\"\n",
    "            )\n",
    "            df_detail.dropna(subset=[START_DATE_COL, END_DATE_COL], inplace=True)\n",
    "            if df_detail.empty:\n",
    "                print(\n",
    "                    f\"      -> 警告：文件 '{file_path.name}' 清理后无有效日期数据，已跳过。\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            df_detail[\"实际天数\"] = (\n",
    "                df_detail[END_DATE_COL].dt.normalize()\n",
    "                - df_detail[START_DATE_COL].dt.normalize()\n",
    "            ).dt.days\n",
    "\n",
    "            for std_type in [\"普标\", \"高标\"]:\n",
    "                std_col = f\"{std_type}时效标准\"\n",
    "                target_col = f\"达成率_{company_key}_{std_type}\"\n",
    "                temp_df = pd.merge(\n",
    "                    df_detail,\n",
    "                    df_standard[[\"路线\", f\"{std_col}_days\"]],\n",
    "                    on=\"路线\",\n",
    "                    how=\"inner\",\n",
    "                )\n",
    "                if temp_df.empty:\n",
    "                    continue\n",
    "\n",
    "                temp_df[\"is_met\"] = temp_df[\"实际天数\"] <= temp_df[f\"{std_col}_days\"]\n",
    "                route_rates = temp_df.groupby(\"路线\")[\"is_met\"].mean().reset_index()\n",
    "                route_rates.rename(columns={\"is_met\": target_col}, inplace=True)\n",
    "                df_all_rates = pd.merge(\n",
    "                    df_all_rates, route_rates, on=\"路线\", how=\"left\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"      -> 计算'{company_key}'达成率时出错: {e}\")\n",
    "\n",
    "    print(\"  -> ✓ 所有公司达成率计算完毕。\")\n",
    "    return df_all_rates\n",
    "\n",
    "\n",
    "def generate_zto_monthly_report():\n",
    "    main_report_path = output_path / \"分析总报告.xlsx\"\n",
    "    zto_monthly_report_path = output_path / \"中通月报.xlsx\"\n",
    "\n",
    "    if not main_report_path.exists():\n",
    "        print(\n",
    "            f\"🔥🔥🔥 错误：主报告 '{main_report_path.name}' 不存在，无法生成中通报告。请先运行相关步骤。\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- 任务开始：生成中通月报 (基于已生成的主报告) ---\")\n",
    "    try:\n",
    "        df_main_report = pd.read_excel(main_report_path)\n",
    "        print(\"  - ✓ 成功读取主报告。\")\n",
    "\n",
    "        # --- Part 1: 计算所有公司的达成率 ---\n",
    "        df_standard = pd.read_excel(\n",
    "            base_data_path, sheet_name=\"ZTO_standard_time_limit\"\n",
    "        )\n",
    "        if \"线路\" in df_standard.columns:\n",
    "            df_standard.rename(columns={\"线路\": \"路线\"}, inplace=True)\n",
    "        if \"路线\" not in df_main_report.columns:\n",
    "            df_main_report[\"路线\"] = (\n",
    "                df_main_report[\"寄出城市\"] + \"-\" + df_main_report[\"寄达城市\"]\n",
    "            )\n",
    "\n",
    "        for std_type in [\"普标\", \"高标\"]:\n",
    "            df_standard[f\"{std_type}时效标准_days\"] = (\n",
    "                df_standard[f\"{std_type}时效标准\"]\n",
    "                .str.replace(r\"T\\+\", \"\", regex=True)\n",
    "                .astype(int)\n",
    "            )\n",
    "\n",
    "        df_all_rates = _calculate_all_achievement_rates(df_standard)\n",
    "\n",
    "        # --- Part 2: 准备Sheet 2 (中通报告数据) ---\n",
    "        print(\"  - [1/5] 正在准备'中通报告数据' Sheet...\")\n",
    "        df_sheet2 = df_standard.copy()\n",
    "        for std_type in [\"普标\", \"高标\"]:\n",
    "            std_col_name = f\"{std_type}时效标准\"\n",
    "            df_sheet2[f\"中通达成率_{std_col_name}\"] = df_all_rates.get(\n",
    "                f\"达成率_中通_{std_type}\"\n",
    "            )\n",
    "\n",
    "            tongdatu_cols = [\n",
    "                f\"达成率_{c}_{std_type}\"\n",
    "                for c in COMPANIES_TONGDATU\n",
    "                if f\"达成率_{c}_{std_type}\" in df_all_rates.columns\n",
    "            ]\n",
    "            if tongdatu_cols:\n",
    "                df_sheet2[f\"通达兔最优_{std_col_name}\"] = df_all_rates[\n",
    "                    tongdatu_cols\n",
    "                ].max(axis=1)\n",
    "            else:\n",
    "                df_sheet2[f\"通达兔最优_{std_col_name}\"] = np.nan\n",
    "\n",
    "            industry_cols = [\n",
    "                f\"达成率_{c}_{std_type}\"\n",
    "                for c in COMPANIES_NINE_MAJOR\n",
    "                if f\"达成率_{c}_{std_type}\" in df_all_rates.columns\n",
    "            ]\n",
    "            if industry_cols:\n",
    "                df_sheet2[f\"行业最优_{std_col_name}\"] = df_all_rates[industry_cols].max(\n",
    "                    axis=1\n",
    "                )\n",
    "            else:\n",
    "                df_sheet2[f\"行业最优_{std_col_name}\"] = np.nan\n",
    "\n",
    "        df_sheet2.drop(\n",
    "            columns=[c for c in df_sheet2.columns if \"_days\" in c], inplace=True\n",
    "        )\n",
    "\n",
    "        # --- Part 3: 准备Sheet 1 (线路详细分析) ---\n",
    "        print(\"  - [2/5] 正在准备'线路详细分析' Sheet...\")\n",
    "        additional_info_cols = [\"线路里程\", \"城市圈\"]\n",
    "        existing_additional_cols = [\n",
    "            c for c in additional_info_cols if c in df_main_report.columns\n",
    "        ]\n",
    "        zto_metric_cols = [c for c in df_main_report.columns if \"中通\" in c]\n",
    "        cols_to_extract = list(\n",
    "            dict.fromkeys([\"路线\"] + existing_additional_cols + zto_metric_cols)\n",
    "        )\n",
    "        df_zto_extra_data = df_main_report[cols_to_extract]\n",
    "        df_sheet1 = pd.merge(df_sheet2.copy(), df_zto_extra_data, on=\"路线\", how=\"left\")\n",
    "\n",
    "        # --- Part 4: 准备汇总Sheet (寄出/寄达城市汇总) ---\n",
    "        print(\"  - [3/5] 正在准备'寄出城市汇总'和'寄达城市汇总' Sheets...\")\n",
    "\n",
    "        source_cols_map = {\n",
    "            # 中通时效\n",
    "            \"全程时限中通\": \"全程时限\",\n",
    "            \"寄出地处理时限中通\": \"寄出地处理时限\",\n",
    "            \"运输时限中通\": \"运输时限\",\n",
    "            \"寄达地处理时限中通\": \"寄达地处理时限\",\n",
    "            \"投递时限中通\": \"投递时限\",\n",
    "            # 行业最优 (minimum)\n",
    "            \"全程时限_minimum\": \"全程时限行业最优\",\n",
    "            \"寄出地处理时限_minimum\": \"寄出地处理时限行业最优\",\n",
    "            \"运输时限_minimum\": \"运输时限行业最优\",\n",
    "            \"寄达地处理时限_minimum\": \"寄达地处理时限行业最优\",\n",
    "            \"投递时限_minimum\": \"投递时限行业最优\",\n",
    "            # 行业均值 (average)\n",
    "            \"全程时限_average\": \"全程时限行业均值\",\n",
    "            \"寄出地处理时限_average\": \"寄出地处理时限行业均值\",\n",
    "            \"运输时限_average\": \"运输时限行业均值\",\n",
    "            \"寄达地处理时限_average\": \"寄达地处理时限行业均值\",\n",
    "            \"投递时限_average\": \"投递时限行业均值\",\n",
    "        }\n",
    "\n",
    "        # 提取需要聚合的源列名\n",
    "        agg_source_cols = list(source_cols_map.keys())\n",
    "\n",
    "        # 准备Sheet 3: 寄出城市汇总\n",
    "        origin_group_cols = [\"寄出省份\", \"寄出城市\"]\n",
    "        cols_for_origin_summary = origin_group_cols + agg_source_cols\n",
    "        # 检查所需列是否存在\n",
    "        if all(c in df_main_report.columns for c in cols_for_origin_summary):\n",
    "            df_sheet3_origin = df_main_report.groupby(\n",
    "                origin_group_cols, as_index=False\n",
    "            )[agg_source_cols].mean()\n",
    "            df_sheet3_origin.rename(columns=source_cols_map, inplace=True)\n",
    "        else:\n",
    "            print(\"  -> 警告：主报告中缺少'寄出城市汇总'所需的列，将生成空表。\")\n",
    "            missing_cols = [\n",
    "                c for c in cols_for_origin_summary if c not in df_main_report.columns\n",
    "            ]\n",
    "            print(f\"     缺少的源列: {missing_cols}\")\n",
    "            final_cols = origin_group_cols + list(source_cols_map.values())\n",
    "            df_sheet3_origin = pd.DataFrame(columns=final_cols)\n",
    "\n",
    "        # 准备Sheet 4: 寄达城市汇总\n",
    "        dest_group_cols = [\"寄达省份\", \"寄达城市\"]\n",
    "        cols_for_dest_summary = dest_group_cols + agg_source_cols\n",
    "        # 检查所需列是否存在\n",
    "        if all(c in df_main_report.columns for c in cols_for_dest_summary):\n",
    "            df_sheet4_dest = df_main_report.groupby(dest_group_cols, as_index=False)[\n",
    "                agg_source_cols\n",
    "            ].mean()\n",
    "            df_sheet4_dest.rename(columns=source_cols_map, inplace=True)\n",
    "        else:\n",
    "            print(\"  -> 警告：主报告中缺少'寄达城市汇总'所需的列，将生成空表。\")\n",
    "            missing_cols = [\n",
    "                c for c in cols_for_dest_summary if c not in df_main_report.columns\n",
    "            ]\n",
    "            print(f\"     缺少的源列: {missing_cols}\")\n",
    "            final_cols = dest_group_cols + list(source_cols_map.values())\n",
    "            df_sheet4_dest = pd.DataFrame(columns=final_cols)\n",
    "\n",
    "        print(\"  - [4/5] ✓ 汇总表准备完毕。\")\n",
    "\n",
    "        # --- Part 5: 写入Excel ---\n",
    "        print(\"  - [5/5] 正在写入Excel文件并应用格式...\")\n",
    "        with pd.ExcelWriter(zto_monthly_report_path, engine=\"xlsxwriter\") as writer:\n",
    "            # 写入Sheet 1\n",
    "            df_sheet1.to_excel(writer, sheet_name=\"线路详细分析\", index=False)\n",
    "            # 写入Sheet 2\n",
    "            df_sheet2.to_excel(writer, sheet_name=\"中通报告数据\", index=False)\n",
    "            # 写入Sheet 3 和 Sheet 4\n",
    "            df_sheet3_origin.to_excel(writer, sheet_name=\"寄出城市汇总\", index=False)\n",
    "            df_sheet4_dest.to_excel(writer, sheet_name=\"寄达城市汇总\", index=False)\n",
    "\n",
    "            # ---- 应用格式 ----\n",
    "            workbook = writer.book\n",
    "            worksheet2 = writer.sheets[\"中通报告数据\"]\n",
    "            percent_format = workbook.add_format({\"num_format\": \"0.00%\"})\n",
    "            cols_to_format = [\n",
    "                \"中通达成率_普标时效标准\",\n",
    "                \"通达兔最优_普标时效标准\",\n",
    "                \"行业最优_普标时效标准\",\n",
    "                \"中通达成率_高标时效标准\",\n",
    "                \"通达兔最优_高标时效标准\",\n",
    "                \"行业最优_高标时效标准\",\n",
    "            ]\n",
    "\n",
    "            # 为Sheet2的达成率列应用格式\n",
    "            for col_name in cols_to_format:\n",
    "                if col_name in df_sheet2.columns:\n",
    "                    col_idx = df_sheet2.columns.get_loc(col_name)\n",
    "                    worksheet2.set_column(col_idx, col_idx, 18, percent_format)\n",
    "\n",
    "        print(f\"--- ✓ 中通月报生成完毕 --- \\n文件已保存至: {zto_monthly_report_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "\n",
    "        print(f\"    -> 🔥🔥🔥 生成中通月报失败: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "# --- 执行中通报告生成函数 ---\n",
    "# 在实际运行前，请确保Cell 1中的路径等变量已正确设置\n",
    "generate_zto_monthly_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf8235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 开始生成: 邮政月报.xlsx ====================\n",
      "  - 正在独立加载所有公司的'线路汇总数据'...\n",
      "  - 正在计算'最优中转次数'...\n",
      "✅ 成功从 '30_top_volume_city_2024' 加载 30 个城市。\n",
      "  - [1/4] 正在准备 '线路明细' Sheet for EMS, 快包...\n",
      "  - [2/4] 正在准备 '邮件明细' Sheet for EMS, 快包...\n",
      "      -> [兼容模式] 检测到'完整物流信息'列名冲突，智能合并中...\n",
      "  - [3/4] 正在准备 '分city明细' (恢复原始计算框架 + 加权平均)...\n",
      "  - [4/4] 正在准备 '分province明细' (恢复原始计算框架 + 加权平均)...\n",
      "\n",
      "--- 所有数据计算完成，正在写入最终文件: 邮政月报.xlsx ---\n",
      "  - 正在写入Sheet: 线路明细...\n",
      "  - 正在写入Sheet: 邮件明细...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/7fywcbxd7t52ftqjt6sy8cs40000gn/T/ipykernel_71567/1049195596.py:1024: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 正在写入Sheet: 分城市明细...\n",
      "  - 正在写入Sheet: 分省份明细...\n",
      "\n",
      "🎉🎉🎉 恭喜！已成功生成报告 '邮政月报.xlsx'！🎉🎉🎉\n",
      "\n",
      "==================== 开始生成: 极兔月报.xlsx ====================\n",
      "  - 正在独立加载所有公司的'线路汇总数据'...\n",
      "  - 正在计算'最优中转次数'...\n",
      "✅ 成功从 '30_top_volume_city_2024' 加载 30 个城市。\n",
      "  - [1/4] 正在准备 '线路明细' Sheet for 极兔...\n",
      "  - [2/4] 正在准备 '邮件明细' Sheet for 极兔...\n",
      "      -> [兼容模式] 检测到'完整物流信息'列名冲突，智能合并中...\n",
      "  - [3/4] 正在准备 '分city明细' (恢复原始计算框架 + 加权平均)...\n",
      "  - [4/4] 正在准备 '分province明细' (恢复原始计算框架 + 加权平均)...\n",
      "\n",
      "--- 所有数据计算完成，正在写入最终文件: 极兔月报.xlsx ---\n",
      "  - 正在写入Sheet: 线路明细...\n",
      "  - 正在写入Sheet: 邮件明细...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/7fywcbxd7t52ftqjt6sy8cs40000gn/T/ipykernel_71567/1049195596.py:1024: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 正在写入Sheet: 分城市明细...\n",
      "  - 正在写入Sheet: 分省份明细...\n",
      "\n",
      "🎉🎉🎉 恭喜！已成功生成报告 '极兔月报.xlsx'！🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 6: 邮政、极兔月报生成 (最终正确版 - 统一口径)\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import traceback\n",
    "import zipfile\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. 全局配置与辅助函数 ---\n",
    "COMPANIES_NINE_MAJOR = [\n",
    "    \"EMS\",\n",
    "    \"中通\",\n",
    "    \"京东\",\n",
    "    \"圆通\",\n",
    "    \"德邦\",\n",
    "    \"极兔\",\n",
    "    \"申通\",\n",
    "    \"韵达\",\n",
    "    \"顺丰\",\n",
    "]\n",
    "COMPANIES_EIGHT_OTHERS = [\n",
    "    \"中通\",\n",
    "    \"京东\",\n",
    "    \"圆通\",\n",
    "    \"德邦\",\n",
    "    \"极兔\",\n",
    "    \"申通\",\n",
    "    \"韵达\",\n",
    "    \"顺丰\",\n",
    "]\n",
    "COMPANIES_FOR_INDUSTRY_COMPARISON = COMPANIES_NINE_MAJOR\n",
    "COMPANIES_ALL_TEN = COMPANIES_FOR_INDUSTRY_COMPARISON + [\"快包\"]\n",
    "\n",
    "REPORT_CONFIG = {\n",
    "    \"邮政\": {\n",
    "        \"products\": [\"EMS\", \"快包\"],\n",
    "        \"output_filename\": \"邮政月报.xlsx\",\n",
    "        \"product_to_filename\": {\"EMS\": \"EMS\", \"快包\": \"邮政\"},\n",
    "    },\n",
    "    \"极兔\": {\n",
    "        \"products\": [\"极兔\"],\n",
    "        \"output_filename\": \"极兔月报.xlsx\",\n",
    "        \"product_to_filename\": {\"极兔\": \"极兔\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "FILENAME_TO_COMPANY_MAP = {\n",
    "    v: k\n",
    "    for k, v in {\n",
    "        \"EMS\": \"EMS\",\n",
    "        \"中通\": \"中通\",\n",
    "        \"京东\": \"京东\",\n",
    "        \"圆通\": \"圆通\",\n",
    "        \"德邦\": \"德邦\",\n",
    "        \"极兔\": \"极兔\",\n",
    "        \"申通\": \"申通\",\n",
    "        \"韵达\": \"韵达\",\n",
    "        \"顺丰\": \"顺丰\",\n",
    "        \"快包\": \"邮政\",\n",
    "    }.items()\n",
    "}\n",
    "\n",
    "\n",
    "def _find_company_key_from_filename(filename):\n",
    "    name = (\n",
    "        Path(filename)\n",
    "        .stem.replace(\"_data_analysis_result\", \"\")\n",
    "        .replace(\"_transit_data\", \"\")\n",
    "    )\n",
    "    filename_to_company = {v: k for k, v in FILENAME_TO_COMPANY_MAP.items()}\n",
    "    filename_to_company.update(\n",
    "        {\n",
    "            \"EMS\": \"EMS\",\n",
    "            \"德邦\": \"德邦\",\n",
    "            \"极兔\": \"极兔\",\n",
    "            \"圆通\": \"圆通\",\n",
    "            \"顺丰\": \"顺丰\",\n",
    "            \"中通\": \"中通\",\n",
    "            \"京东\": \"京东\",\n",
    "            \"韵达\": \"韵达\",\n",
    "            \"申通\": \"申通\",\n",
    "            \"邮政\": \"快包\",\n",
    "        }\n",
    "    )\n",
    "    sorted_keys = sorted(filename_to_company.keys(), key=len, reverse=True)\n",
    "    for keyword in sorted_keys:\n",
    "        if keyword in name:\n",
    "            return filename_to_company[keyword]\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_top_cities(file_path: Path, sheet_name: str) -> set:\n",
    "    try:\n",
    "        if not file_path.exists():\n",
    "            print(f\"❌ 错误：基础数据文件 'basic_data.xlsx' 未找到: {file_path}\")\n",
    "            return set()\n",
    "        df_cities = pd.read_excel(file_path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "        if \"城市\" not in df_cities.columns:\n",
    "            return set()\n",
    "        city_list = df_cities[\"城市\"].dropna().astype(str).str.strip().unique().tolist()\n",
    "        print(f\"✅ 成功从 '{sheet_name}' 加载 {len(city_list)} 个城市。\")\n",
    "        return set(city_list)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 错误：加载城市列表 '{sheet_name}' 失败: {e}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def auto_adjust_xlsx_columns(writer, df, sheet_name):\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    for i, col_name in enumerate(df.columns):\n",
    "        column_data = df[col_name]\n",
    "        if isinstance(column_data, pd.DataFrame):\n",
    "            column_data = column_data.iloc[:, 0]\n",
    "        if col_name == \"完整物流信息\":\n",
    "            width = 50\n",
    "        else:\n",
    "            column_len = (\n",
    "                column_data.astype(str).str.len().max() if not column_data.empty else 0\n",
    "            )\n",
    "            width = min(max(column_len, len(str(col_name)), 8) + 2, 40)\n",
    "        worksheet.set_column(i, i, width)\n",
    "\n",
    "\n",
    "# --- 2. 各Sheet的生成函数 ---\n",
    "def create_sheet1_route_details(df_main, products: list):\n",
    "    \"\"\"【恢复】此函数已完全恢复到您的原始版本\"\"\"\n",
    "    print(f\"  - [1/4] 正在准备 '线路明细' Sheet for {', '.join(products)}...\")\n",
    "    column_mapping = {\n",
    "        \"寄出省份\": \"寄出省份\",\n",
    "        \"寄出城市\": \"寄出城市\",\n",
    "        \"寄达省份\": \"寄达省份\",\n",
    "        \"寄达城市\": \"寄达城市\",\n",
    "        \"线路\": \"路线\",\n",
    "        \"线路里程\": \"线路里程\",\n",
    "        \"城市圈\": \"城市圈\",\n",
    "        \"行业最优_中转次数\": \"最优中转次数\",\n",
    "        \"行业最优_48小时妥投率\": \"48小时准时率_maximum\",\n",
    "        \"行业均值_48小时妥投率\": \"48小时准时率_average\",\n",
    "        \"行业最差_48小时妥投率\": \"48小时准时率_minimum\",\n",
    "        \"行业最优_72小时妥投率\": \"72小时准时率_maximum\",\n",
    "        \"行业均值_72小时妥投率\": \"72小时准时率_average\",\n",
    "        \"行业最差_72小时妥投率\": \"72小时准时率_minimum\",\n",
    "        \"行业最差_全程时限\": \"全程时限_maximum\",\n",
    "        \"行业均值_全程时限\": \"全程时限_average\",\n",
    "        \"行业最优_全程时限\": \"全程时限_minimum\",\n",
    "        \"行业最差_T+N\": \"送达天数_maximum\",\n",
    "        \"行业均值_T+N\": \"送达天数_average\",\n",
    "        \"行业最优_T+N\": \"送达天数_minimum\",\n",
    "        \"行业最差_寄出地处理时长\": \"寄出地处理时限_maximum\",\n",
    "        \"行业均值_寄出地处理时长\": \"寄出地处理时限_average\",\n",
    "        \"行业最优_寄出地处理时长\": \"寄出地处理时限_minimum\",\n",
    "        \"行业最差_运输时长\": \"运输时限_maximum\",\n",
    "        \"行业均值_运输时长\": \"运输时限_average\",\n",
    "        \"行业最优_运输时长\": \"运输时限_minimum\",\n",
    "        \"行业最差_寄达地处理时长\": \"寄达地处理时限_maximum\",\n",
    "        \"行业均值_寄达地处理时长\": \"寄达地处理时限_average\",\n",
    "        \"行业最优_寄达地处理时长\": \"寄达地处理时限_minimum\",\n",
    "        \"行业最差_投递时长\": \"投递时限_maximum\",\n",
    "        \"行业均值_投递时长\": \"投递时限_average\",\n",
    "        \"行业最优_投递时长\": \"投递时限_minimum\",\n",
    "    }\n",
    "    source_metric_map = {\n",
    "        \"中转次数\": \"平均中转次数\",\n",
    "        \"48小时妥投率\": \"48小时准时率\",\n",
    "        \"72小时妥投率\": \"72小时准时率\",\n",
    "        \"T+N\": \"送达天数\",\n",
    "        \"寄出地处理时长\": \"寄出地处理时限\",\n",
    "        \"运输时长\": \"运输时限\",\n",
    "        \"寄达地处理时长\": \"寄达地处理时限\",\n",
    "        \"投递时长\": \"投递时限\",\n",
    "        \"全程时限\": \"全程时限\",\n",
    "    }\n",
    "    for prod in products:\n",
    "        for metric, source_metric_name in source_metric_map.items():\n",
    "            rank_source_suffix = f\"_{prod.lower()}排名\"\n",
    "            if prod == \"EMS\":\n",
    "                rank_source_suffix = \"_ems排名\"\n",
    "            elif prod == \"快包\":\n",
    "                rank_source_suffix = \"_快包排名\"\n",
    "            if metric == \"中转次数\":\n",
    "                column_mapping[f\"{prod}_{metric}\"] = f\"{prod}{source_metric_name}\"\n",
    "            else:\n",
    "                column_mapping[f\"{prod}_{metric}\"] = f\"{source_metric_name}{prod}\"\n",
    "                column_mapping[f\"{prod}排名_{metric}\"] = (\n",
    "                    f\"{source_metric_name}{rank_source_suffix}\"\n",
    "                )\n",
    "    df_sheet1 = pd.DataFrame()\n",
    "    for new_col, source_col in column_mapping.items():\n",
    "        if source_col in df_main.columns:\n",
    "            df_sheet1[new_col] = df_main[source_col]\n",
    "        else:\n",
    "            df_sheet1[new_col] = pd.NA\n",
    "    filter_metrics = [\n",
    "        \"运输时长\",\n",
    "        \"寄达地处理时长\",\n",
    "        \"寄出地处理时长\",\n",
    "        \"投递时长\",\n",
    "        \"全程时限\",\n",
    "    ]\n",
    "    for metric in filter_metrics:\n",
    "        for prod in products:\n",
    "            prod_metric_col = f\"{prod}_{metric}\"\n",
    "            compare_col = \"\"\n",
    "            if prod in [\"EMS\", \"极兔\"]:\n",
    "                compare_col = f\"行业最优_{metric}\"\n",
    "            elif prod == \"快包\":\n",
    "                compare_col = f\"行业均值_{metric}\"\n",
    "            else:\n",
    "                continue\n",
    "            filter_col_name = f\"{prod}筛选指标_{metric}\"\n",
    "            if (\n",
    "                prod_metric_col in df_sheet1.columns\n",
    "                and compare_col in df_sheet1.columns\n",
    "            ):\n",
    "                prod_values = pd.to_numeric(df_sheet1[prod_metric_col], errors=\"coerce\")\n",
    "                compare_values = pd.to_numeric(df_sheet1[compare_col], errors=\"coerce\")\n",
    "                df_sheet1[filter_col_name] = prod_values - compare_values\n",
    "            else:\n",
    "                df_sheet1[filter_col_name] = pd.NA\n",
    "    original_cols = list(column_mapping.keys())\n",
    "    new_filter_cols = [col for col in df_sheet1.columns if \"筛选指标\" in col]\n",
    "    final_cols_order = original_cols + new_filter_cols\n",
    "    existing_cols = [col for col in final_cols_order if col in df_sheet1.columns]\n",
    "    return df_sheet1[existing_cols]\n",
    "\n",
    "\n",
    "def create_sheet2_mail_details(\n",
    "    data_analysis_path,\n",
    "    zhuzhuyun_merge_path,\n",
    "    products: list,\n",
    "    product_to_filename: dict,\n",
    "    top_30_cities: set,\n",
    "    df_main,\n",
    "):\n",
    "    print(f\"  - [2/4] 正在准备 '邮件明细' Sheet for {', '.join(products)}...\")\n",
    "    # 步骤1：加载源数据 (此时 df_s2 可能有，也可能没有“完整物流信息”列)\n",
    "    dfs = []\n",
    "    for prod_name, filename_part in product_to_filename.items():\n",
    "        file_path = data_analysis_path / f\"{filename_part}_data_analysis_result.xlsx\"\n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                df = pd.read_excel(\n",
    "                    file_path,\n",
    "                    sheet_name=\"线路详细数据\",\n",
    "                    dtype={\"单号\": str},\n",
    "                    engine=\"openpyxl\",\n",
    "                )\n",
    "                if not df.empty:\n",
    "                    df[\"单号\"] = df[\"单号\"].astype(str).str.strip()\n",
    "                    df[\"产品种类\"] = prod_name\n",
    "                    dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"      -> 警告: 读取文件 {file_path.name} 失败: {e}\")\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_s2 = pd.concat(dfs, ignore_index=True)\n",
    "    df_s2.rename(\n",
    "        columns={\n",
    "            \"单号\": \"邮件号\",\n",
    "            \"公里\": \"线路里程\",\n",
    "            \"签收时间\": \"完成投递时间\",\n",
    "            \"寄出地处理时限\": \"寄出地处理时长\",\n",
    "            \"运输时限\": \"运输时长\",\n",
    "            \"寄达地处理时限\": \"寄达地处理时长\",\n",
    "            \"投递时限\": \"投递时长\",\n",
    "            \"揽收时间\": \"揽件时间\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # 步骤2：始终从“猪猪云”加载并合并数据，这是必须的\n",
    "    trace_dfs = []\n",
    "    for product_type, filename in {\n",
    "        p: f\"{product_to_filename.get(p, p)}.xlsx\" for p in products\n",
    "    }.items():\n",
    "        trace_file_path = zhuzhuyun_merge_path / filename\n",
    "        if trace_file_path.exists():\n",
    "            try:\n",
    "                df_trace_part = pd.read_excel(\n",
    "                    trace_file_path,\n",
    "                    usecols=[\"快递单号\", \"完整物流信息\"],\n",
    "                    dtype={\"快递单号\": str},\n",
    "                    engine=\"openpyxl\",\n",
    "                )\n",
    "                if not df_trace_part.empty:\n",
    "                    df_trace_part[\"快递单号\"] = (\n",
    "                        df_trace_part[\"快递单号\"].astype(str).str.strip()\n",
    "                    )\n",
    "                    df_trace_part[\"产品种类\"] = product_type\n",
    "                    trace_dfs.append(df_trace_part)\n",
    "            except Exception as e:\n",
    "                print(f\"      -> 警告: 读取轨迹文件 {filename} 失败: {e}\")\n",
    "\n",
    "    if trace_dfs:\n",
    "        df_traces = pd.concat(trace_dfs, ignore_index=True).drop_duplicates(\n",
    "            subset=[\"快递单号\", \"产品种类\"]\n",
    "        )\n",
    "        df_s2 = pd.merge(\n",
    "            df_s2,\n",
    "            df_traces,\n",
    "            how=\"left\",\n",
    "            left_on=[\"邮件号\", \"产品种类\"],\n",
    "            right_on=[\"快递单号\", \"产品种类\"],\n",
    "        )\n",
    "        df_s2.drop(columns=[\"快递单号\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # ========================= 核心逻辑：兼容性处理 (START) =========================\n",
    "    #\n",
    "    if \"完整物流信息_y\" in df_s2.columns:\n",
    "        # **处理情况1：源文件和猪猪云都有该列，导致冲突，生成了_x和_y**\n",
    "        # 策略：优先使用猪猪云(_y)的数据，如果猪猪云没匹配上(值为NaN)，则用源文件(_x)的数据填充\n",
    "        print(\"      -> [兼容模式] 检测到'完整物流信息'列名冲突，智能合并中...\")\n",
    "        df_s2[\"完整物流信息\"] = df_s2[\"完整物流信息_y\"].fillna(df_s2[\"完整物流信息_x\"])\n",
    "        # 清理掉临时的_x和_y列\n",
    "        df_s2.drop(columns=[\"完整物流信息_x\", \"完整物流信息_y\"], inplace=True)\n",
    "\n",
    "    elif \"完整物流信息\" in df_s2.columns:\n",
    "        print(\"      -> [兼容模式] '完整物流信息'列已存在且无冲突，流程继续。\")\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(\"      -> [兼容模式] 未找到'完整物流信息'列，已创建空列作为保障。\")\n",
    "        df_s2[\"完整物流信息\"] = \"\"\n",
    "\n",
    "    df_s2[\"完整物流信息\"].fillna(\"\", inplace=True)\n",
    "    #\n",
    "    # ========================== 核心逻辑：兼容性处理 (END) ==========================\n",
    "\n",
    "    df_s2[\"线路\"] = df_s2[\"寄出城市\"] + \"-\" + df_s2[\"寄达城市\"]\n",
    "    time_cols = [\n",
    "        \"揽件时间\",\n",
    "        \"完成投递时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "    ]\n",
    "    for col in time_cols:\n",
    "        if col in df_s2.columns:\n",
    "            df_s2[col] = pd.to_datetime(df_s2[col], errors=\"coerce\")\n",
    "\n",
    "    if \"揽件时间\" in df_s2.columns and \"完成投递时间\" in df_s2.columns:\n",
    "        valid_times = df_s2[\"揽件时间\"].notna() & df_s2[\"完成投递时间\"].notna()\n",
    "        days_diff = (\n",
    "            df_s2.loc[valid_times, \"完成投递时间\"].dt.normalize()\n",
    "            - df_s2.loc[valid_times, \"揽件时间\"].dt.normalize()\n",
    "        ).dt.days\n",
    "        df_s2.loc[valid_times, \"T+N\"] = \"T+\" + days_diff.astype(int).astype(str)\n",
    "\n",
    "    df_s2[\"是否达成72小时妥投率\"] = np.where(df_s2[\"全程时限\"] <= 72, \"是\", \"否\")\n",
    "    top_30_mask = (df_s2[\"寄出城市\"].isin(top_30_cities)) & (\n",
    "        df_s2[\"寄达城市\"].isin(top_30_cities)\n",
    "    )\n",
    "    df_s2[\"是否达成48小时妥投率\"] = \"不适用\"\n",
    "    df_s2.loc[top_30_mask, \"是否达成48小时妥投率\"] = np.where(\n",
    "        df_s2.loc[top_30_mask, \"全程时限\"] <= 48, \"是\", \"否\"\n",
    "    )\n",
    "\n",
    "    # ... 后续代码部分保持不变 ...\n",
    "    df_main_renamed = df_main.rename(columns={\"路线\": \"线路\"})\n",
    "    cols_to_merge_from_main = [\"线路\", \"送达天数_average\", \"送达天数_minimum\"]\n",
    "    metrics_for_merge = [\"48小时准时率\", \"72小时准时率\", \"全程时限\"]\n",
    "    for m in metrics_for_merge:\n",
    "        for p in products:\n",
    "            cols_to_merge_from_main.append(f\"{m}{p}\")\n",
    "        for agg in [\"maximum\", \"average\", \"minimum\"]:\n",
    "            cols_to_merge_from_main.append(f\"{m}_{agg}\")\n",
    "\n",
    "    if \"全程时限_minimum\" in df_main_renamed.columns:\n",
    "        cols_to_merge_from_main.append(\"全程时限_minimum\")\n",
    "    if \"全程时限_maximum\" in df_main_renamed.columns:\n",
    "        cols_to_merge_from_main.append(\"全程时限_maximum\")\n",
    "\n",
    "    existing_cols_to_merge = [\n",
    "        c for c in cols_to_merge_from_main if c in df_main_renamed.columns\n",
    "    ]\n",
    "    df_s2 = pd.merge(\n",
    "        df_s2, df_main_renamed[existing_cols_to_merge], on=\"线路\", how=\"left\"\n",
    "    )\n",
    "    df_s2[\"行业均值\"] = df_s2.get(\"送达天数_average\", \"\").fillna(\"\").astype(str)\n",
    "    df_s2[\"行业最优\"] = df_s2.get(\"送达天数_minimum\", \"\").fillna(\"\").astype(str)\n",
    "\n",
    "    rename_dict = {\n",
    "        \"48小时准时率_maximum\": \"行业最优_48小时妥投率\",\n",
    "        \"48小时准时率_average\": \"行业均值_48小时妥投率\",\n",
    "        \"48小时准时率_minimum\": \"行业最差_48小时妥投率\",\n",
    "        \"72小时准时率_maximum\": \"行业最优_72小时妥投率\",\n",
    "        \"72小时准时率_average\": \"行业均值_72小时妥投率\",\n",
    "        \"72小时准时率_minimum\": \"行业最差_72小时妥投率\",\n",
    "        \"全程时限_minimum\": \"行业最优_线路全程时限\",\n",
    "        \"全程时限_average\": \"行业均值_线路全程时限\",\n",
    "        \"全程时限_maximum\": \"行业最差_线路全程时限\",\n",
    "    }\n",
    "    for p in products:\n",
    "        rename_dict[f\"48小时准时率{p}\"] = f\"{p}_48小时妥投率\"\n",
    "        rename_dict[f\"72小时准时率{p}\"] = f\"{p}_72小时妥投率\"\n",
    "        rename_dict[f\"全程时限{p}\"] = f\"{p}_线路全程时限\"\n",
    "    df_s2.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    final_columns_order_s2 = [\n",
    "        \"邮件号\",\n",
    "        \"产品种类\",\n",
    "        \"寄出省份\",\n",
    "        \"寄出城市\",\n",
    "        \"寄达省份\",\n",
    "        \"寄达城市\",\n",
    "        \"线路\",\n",
    "        \"线路里程\",\n",
    "        \"全程时限\",\n",
    "        \"T+N\",\n",
    "        \"行业均值\",\n",
    "        \"行业最优\",\n",
    "        \"是否达成48小时妥投率\",\n",
    "        \"是否达成72小时妥投率\",\n",
    "        \"寄出地处理时长\",\n",
    "        \"运输时长\",\n",
    "        \"寄达地处理时长\",\n",
    "        \"投递时长\",\n",
    "        \"揽件时间\",\n",
    "        \"离开寄件城市时间\",\n",
    "        \"到达收件城市时间\",\n",
    "        \"派送时间\",\n",
    "        \"完成投递时间\",\n",
    "    ]\n",
    "\n",
    "    newly_added_columns = [\n",
    "        \"EMS_48小时妥投率\",\n",
    "        \"快包_48小时妥投率\",\n",
    "        \"行业最优_48小时妥投率\",\n",
    "        \"行业均值_48小时妥投率\",\n",
    "        \"行业最差_48小时妥投率\",\n",
    "        \"EMS_72小时妥投率\",\n",
    "        \"快包_72小时妥投率\",\n",
    "        \"行业最优_72小时妥投率\",\n",
    "        \"行业均值_72小时妥投率\",\n",
    "        \"行业最差_72小时妥投率\",\n",
    "        \"EMS_线路全程时限\",\n",
    "        \"快包_线路全程时限\",\n",
    "        \"行业最优_线路全程时限\",\n",
    "        \"行业均值_线路全程时限\",\n",
    "        \"行业最差_线路全程时限\",\n",
    "    ]\n",
    "    if \"极兔\" in products:\n",
    "        newly_added_columns = [\n",
    "            c for c in newly_added_columns if \"极兔\" in c or \"行业\" in c\n",
    "        ]\n",
    "\n",
    "    final_columns_order_s2.extend(newly_added_columns)\n",
    "    final_columns_order_s2.append(\"完整物流信息\")\n",
    "\n",
    "    existing_cols = [col for col in final_columns_order_s2 if col in df_s2.columns]\n",
    "    return df_s2[existing_cols]\n",
    "\n",
    "\n",
    "def weighted_agg(group, metrics, company):\n",
    "    \"\"\"辅助函数：对单个分组进行加权平均计算\"\"\"\n",
    "    weight_col = f\"快递数量{company}\"\n",
    "    if weight_col not in group.columns or group[weight_col].sum() == 0:\n",
    "        return pd.Series([np.nan] * len(metrics), index=metrics)\n",
    "\n",
    "    total_weight = group[weight_col].sum()\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        metric_col = f\"{metric}{company}\"\n",
    "        if metric_col in group.columns:\n",
    "            weighted_sum = (group[metric_col] * group[weight_col]).sum()\n",
    "            results[metric] = weighted_sum / total_weight\n",
    "        else:\n",
    "            results[metric] = np.nan\n",
    "    return pd.Series(results)\n",
    "\n",
    "\n",
    "def create_regional_report_data(\n",
    "    df_main: pd.DataFrame, grouping_level: str, final_col_order: list, products: list\n",
    "):\n",
    "    \"\"\"【最终修复】: 恢复原始计算框架，并注入加权平均\"\"\"\n",
    "    print(\n",
    "        f\"  - [{3 if grouping_level == 'city' else 4}/4] 正在准备 '分{grouping_level}明细' (恢复原始计算框架 + 加权平均)...\"\n",
    "    )\n",
    "\n",
    "    # 1. 恢复“分离式聚合”框架\n",
    "    metrics_by_destination = [\n",
    "        \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "        \"离开寄达地分拣中心-派件\",\n",
    "        \"寄达地处理时限\",\n",
    "        \"投递时限\",\n",
    "    ]\n",
    "    metrics_by_origin = [\n",
    "        \"揽收-到达寄出地分拣中心时长\",\n",
    "        \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "        \"寄出地处理时限\",\n",
    "        \"运输时限\",\n",
    "        \"全程时限\",\n",
    "    ]\n",
    "\n",
    "    all_pivots_origin = []\n",
    "    group_cols_origin = {\"city\": [\"寄出省份\", \"寄出城市\"], \"province\": [\"寄出省份\"]}[\n",
    "        grouping_level\n",
    "    ]\n",
    "    for comp in COMPANIES_ALL_TEN:\n",
    "        comp_metrics = [m for m in metrics_by_origin if f\"{m}{comp}\" in df_main.columns]\n",
    "        if not comp_metrics or f\"快递数量{comp}\" not in df_main.columns:\n",
    "            continue\n",
    "\n",
    "        # ========================= 修改点 1 of 3 =========================\n",
    "        df_agg = (\n",
    "            df_main.groupby(group_cols_origin)\n",
    "            .apply(\n",
    "                weighted_agg,\n",
    "                metrics=comp_metrics,\n",
    "                company=comp,  # <-- 删除 include_groups=False\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        # ===============================================================\n",
    "        df_agg[\"company\"] = comp\n",
    "        all_pivots_origin.append(df_agg)\n",
    "\n",
    "    if not all_pivots_origin:\n",
    "        df_pivot_origin = pd.DataFrame(\n",
    "            index=pd.MultiIndex.from_tuples([], names=group_cols_origin)\n",
    "        )\n",
    "    else:\n",
    "        df_pivot_origin = pd.concat(all_pivots_origin).pivot_table(\n",
    "            index=group_cols_origin, columns=\"company\", values=metrics_by_origin\n",
    "        )\n",
    "        if not df_pivot_origin.empty:\n",
    "            df_pivot_origin.columns = [f\"{v}_{c}\" for v, c in df_pivot_origin.columns]\n",
    "\n",
    "    all_pivots_dest = []\n",
    "    group_cols_dest = {\"city\": [\"寄达省份\", \"寄达城市\"], \"province\": [\"寄达省份\"]}[\n",
    "        grouping_level\n",
    "    ]\n",
    "\n",
    "    needed_cols = group_cols_dest.copy()\n",
    "    for comp in COMPANIES_ALL_TEN:\n",
    "        needed_cols.append(f\"快递数量{comp}\")\n",
    "        for metric in metrics_by_destination:\n",
    "            needed_cols.append(f\"{metric}{comp}\")\n",
    "\n",
    "    existing_needed_cols = [c for c in needed_cols if c in df_main.columns]\n",
    "\n",
    "    df_dest_subset = df_main[existing_needed_cols]\n",
    "    df_dest_temp = df_dest_subset.rename(\n",
    "        columns={\"寄达省份\": \"寄出省份\", \"寄达城市\": \"寄出城市\"}\n",
    "    )\n",
    "\n",
    "    for comp in COMPANIES_ALL_TEN:\n",
    "        comp_metrics = [\n",
    "            m for m in metrics_by_destination if f\"{m}{comp}\" in df_dest_temp.columns\n",
    "        ]\n",
    "        if not comp_metrics or f\"快递数量{comp}\" not in df_dest_temp.columns:\n",
    "            continue\n",
    "\n",
    "        # ========================= 修改点 2 of 3 =========================\n",
    "        df_agg = (\n",
    "            df_dest_temp.groupby(group_cols_origin)\n",
    "            .apply(\n",
    "                weighted_agg,\n",
    "                metrics=comp_metrics,\n",
    "                company=comp,  # <-- 删除 include_groups=False\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        # ===============================================================\n",
    "        df_agg[\"company\"] = comp\n",
    "        all_pivots_dest.append(df_agg)\n",
    "\n",
    "    if not all_pivots_dest:\n",
    "        df_pivot_dest = pd.DataFrame(\n",
    "            index=pd.MultiIndex.from_tuples([], names=group_cols_origin)\n",
    "        )\n",
    "    else:\n",
    "        df_pivot_dest = pd.concat(all_pivots_dest).pivot_table(\n",
    "            index=group_cols_origin, columns=\"company\", values=metrics_by_destination\n",
    "        )\n",
    "        if not df_pivot_dest.empty:\n",
    "            df_pivot_dest.columns = [f\"{v}_{c}\" for v, c in df_pivot_dest.columns]\n",
    "\n",
    "    df_merged_main = pd.merge(\n",
    "        df_pivot_origin, df_pivot_dest, left_index=True, right_index=True, how=\"outer\"\n",
    "    )\n",
    "\n",
    "    # 2. 恢复“双挂全程时限”计算框架 + 注入加权平均\n",
    "    df_sent = df_main.rename(columns={\"寄出省份\": \"省份\", \"寄出城市\": \"城市\"})\n",
    "    df_recv = df_main.rename(columns={\"寄达省份\": \"省份\", \"寄达城市\": \"城市\"})\n",
    "    df_dual = pd.concat([df_sent, df_recv]).dropna(subset=[\"省份\"])\n",
    "    dual_group_cols = [\"省份\", \"城市\"] if grouping_level == \"city\" else [\"省份\"]\n",
    "\n",
    "    all_pivots_dual = []\n",
    "    for comp in COMPANIES_ALL_TEN:\n",
    "        if (\n",
    "            f\"全程时限{comp}\" not in df_dual.columns\n",
    "            or f\"快递数量{comp}\" not in df_dual.columns\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # ========================= 修改点 3 of 3 =========================\n",
    "        df_agg = (\n",
    "            df_dual.groupby(dual_group_cols)\n",
    "            .apply(\n",
    "                weighted_agg,\n",
    "                metrics=[\"全程时限\"],\n",
    "                company=comp,  # <-- 删除 include_groups=False\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        # ===============================================================\n",
    "        df_agg[\"company\"] = comp\n",
    "        all_pivots_dual.append(df_agg)\n",
    "\n",
    "    if not all_pivots_dual:\n",
    "        df_pivot_dual = pd.DataFrame(\n",
    "            index=pd.MultiIndex.from_tuples([], names=dual_group_cols)\n",
    "        )\n",
    "    else:\n",
    "        df_pivot_dual = pd.concat(all_pivots_dual).pivot_table(\n",
    "            index=dual_group_cols, columns=\"company\", values=\"全程时限\"\n",
    "        )\n",
    "        if not df_pivot_dual.empty:\n",
    "            df_pivot_dual.columns = [\n",
    "                f\"全程（双挂）时限_{c}\" for c in df_pivot_dual.columns\n",
    "            ]\n",
    "            df_pivot_dual.index.names = group_cols_origin\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_merged_main, df_pivot_dual, left_index=True, right_index=True, how=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # 【修复】: 恢复原始代码中对列名的处理方式\n",
    "    final_report_data = df_merged.copy()\n",
    "    rename_map = {}\n",
    "    for col in final_report_data.columns:\n",
    "        if isinstance(col, str) and \"_\" in col:\n",
    "            parts = col.split(\"_\")\n",
    "            metric = parts[0]\n",
    "            company = parts[1]\n",
    "            if metric == \"全程时限\":\n",
    "                metric = \"全程（寄出地）时限\"\n",
    "            if company in COMPANIES_ALL_TEN:\n",
    "                rename_map[col] = f\"{company}_{metric}\"\n",
    "    final_report_data.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # 3. 恢复后续的行业统计、排名和附加计算\n",
    "    all_metrics_for_ranking = [\n",
    "        m.replace(\"全程时限\", \"全程（寄出地）时限\")\n",
    "        for m in (metrics_by_origin + metrics_by_destination)\n",
    "    ] + [\"全程（双挂）时限\"]\n",
    "    all_metrics_for_ranking = list(set(all_metrics_for_ranking))\n",
    "\n",
    "    for metric in all_metrics_for_ranking:\n",
    "        industry_cols = [\n",
    "            f\"{prod}_{metric}\"\n",
    "            for prod in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "            if f\"{prod}_{metric}\" in final_report_data.columns\n",
    "        ]\n",
    "        if not industry_cols:\n",
    "            continue\n",
    "        final_report_data[f\"行业均值_{metric}\"] = final_report_data[industry_cols].mean(\n",
    "            axis=1\n",
    "        )\n",
    "        final_report_data[f\"行业最优_{metric}\"] = final_report_data[industry_cols].min(\n",
    "            axis=1\n",
    "        )\n",
    "        for prod in products:\n",
    "            rank_pool = (\n",
    "                COMPANIES_EIGHT_OTHERS + [\"快包\"]\n",
    "                if prod == \"快包\"\n",
    "                else list(COMPANIES_NINE_MAJOR)\n",
    "            )\n",
    "            rank_cols = [\n",
    "                f\"{p}_{metric}\"\n",
    "                for p in rank_pool\n",
    "                if f\"{p}_{metric}\" in final_report_data.columns\n",
    "            ]\n",
    "            prod_col_name = f\"{prod}_{metric}\"\n",
    "            if rank_cols and prod_col_name in final_report_data.columns:\n",
    "                final_report_data[f\"{prod}排名_{metric}\"] = final_report_data[\n",
    "                    rank_cols\n",
    "                ].rank(axis=1, method=\"min\", ascending=True)[prod_col_name]\n",
    "\n",
    "    sum_metric = \"寄出地处理时限+寄达地处理时限+投递时限\"\n",
    "    part_metrics = [\"寄出地处理时限\", \"寄达地处理时限\", \"投递时限\"]\n",
    "    all_report_companies = products + [\n",
    "        c for c in COMPANIES_FOR_INDUSTRY_COMPARISON if c not in products\n",
    "    ]\n",
    "    for company in all_report_companies:\n",
    "        part_cols = [f\"{company}_{p}\" for p in part_metrics]\n",
    "        if all(c in final_report_data.columns for c in part_cols):\n",
    "            final_report_data[f\"{company}_{sum_metric}\"] = final_report_data[\n",
    "                part_cols\n",
    "            ].sum(axis=1, min_count=3)\n",
    "    avg_part_cols = [f\"行业均值_{p}\" for p in part_metrics]\n",
    "    if all(c in final_report_data.columns for c in avg_part_cols):\n",
    "        final_report_data[f\"行业均值_{sum_metric}\"] = final_report_data[\n",
    "            avg_part_cols\n",
    "        ].sum(axis=1, min_count=3)\n",
    "    total_sum_cols = [\n",
    "        f\"{c}_{sum_metric}\"\n",
    "        for c in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "        if f\"{c}_{sum_metric}\" in final_report_data.columns\n",
    "    ]\n",
    "    if total_sum_cols:\n",
    "        final_report_data[f\"行业最优_{sum_metric}\"] = final_report_data[\n",
    "            total_sum_cols\n",
    "        ].min(axis=1, skipna=True)\n",
    "    for prod in products:\n",
    "        rank_pool = (\n",
    "            COMPANIES_EIGHT_OTHERS + [\"快包\"]\n",
    "            if prod == \"快包\"\n",
    "            else list(COMPANIES_NINE_MAJOR)\n",
    "        )\n",
    "        rank_cols = [\n",
    "            f\"{c}_{sum_metric}\"\n",
    "            for c in rank_pool\n",
    "            if f\"{c}_{sum_metric}\" in final_report_data.columns\n",
    "        ]\n",
    "        prod_sum_col = f\"{prod}_{sum_metric}\"\n",
    "        if rank_cols and prod_sum_col in final_report_data.columns:\n",
    "            final_report_data[f\"{prod}排名_{sum_metric}\"] = final_report_data[\n",
    "                rank_cols\n",
    "            ].rank(axis=1, method=\"min\", ascending=True)[prod_sum_col]\n",
    "\n",
    "    # 4. 最后整理\n",
    "    final_report_data.rename(\n",
    "        columns={\"寄出省份\": \"省份\", \"寄出城市\": \"地市\"}, inplace=True\n",
    "    )\n",
    "    existing_cols = [c for c in final_col_order if c in final_report_data.columns]\n",
    "    final_df = final_report_data[existing_cols].copy()\n",
    "    sort_keys = [\"省份\", \"地市\"] if grouping_level == \"city\" else [\"省份\"]\n",
    "    if any(k in final_df.columns for k in sort_keys):\n",
    "        final_df.sort_values(\n",
    "            by=[k for k in sort_keys if k in final_df.columns], inplace=True\n",
    "        )\n",
    "    if not final_df.empty:\n",
    "        final_df.insert(0, \"序号\", range(1, len(final_df) + 1))\n",
    "    return final_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _generate_dynamic_column_order(products: list, level: str) -> list:\n",
    "    \"\"\"【恢复】此函数已完全恢复到您的原始版本\"\"\"\n",
    "    base_cols = [\"序号\", \"省份\"]\n",
    "    if level == \"city\":\n",
    "        base_cols.append(\"地市\")\n",
    "    order = base_cols.copy()\n",
    "    sections = {\n",
    "        \"寄出地处理\": [\n",
    "            \"揽收-到达寄出地分拣中心时长\",\n",
    "            \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "            \"寄出地处理时限\",\n",
    "        ],\n",
    "        \"寄达地处理\": [\n",
    "            \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "            \"离开寄达地分拣中心-派件\",\n",
    "            \"寄达地处理时限\",\n",
    "        ],\n",
    "        \"投递\": [\"投递时限\"],\n",
    "        \"端到端处理\": [\"寄出地处理时限+寄达地处理时限+投递时限\"],\n",
    "        \"寄出地全程\": [\"全程（寄出地）时限\"],\n",
    "        \"双挂全程\": [\"全程（双挂）时限\"],\n",
    "    }\n",
    "    for _, metrics in sections.items():\n",
    "        for prod in products:\n",
    "            for metric in metrics:\n",
    "                order.extend([f\"{prod}_{metric}\", f\"{prod}排名_{metric}\"])\n",
    "        for metric in metrics:\n",
    "            order.append(f\"行业均值_{metric}\")\n",
    "        for metric in metrics:\n",
    "            order.append(f\"行业最优_{metric}\")\n",
    "    return order\n",
    "\n",
    "\n",
    "# --- 3. 主执行函数 ---\n",
    "def generate_company_monthly_report(report_type: str):\n",
    "    if report_type not in REPORT_CONFIG:\n",
    "        print(f\"🔥🔥🔥 错误: 未知的报告类型 '{report_type}'。\")\n",
    "        return\n",
    "    config = REPORT_CONFIG[report_type]\n",
    "    print(f\"\\n{'=' * 20} 开始生成: {config['output_filename']} {'=' * 20}\")\n",
    "\n",
    "    base_path = Path.cwd()\n",
    "    output_path = base_path / \"报告数据\" / \"输出\"\n",
    "    data_analysis_path = output_path / \"data_analysis_result\"\n",
    "    zhuzhuyun_merge_path = base_path / \"报告数据\" / \"temp\" / \"3_猪猪云合并数据\"\n",
    "    basic_data_path = base_path / \"报告数据\" / \"输入\" / \"basic_data.xlsx\"\n",
    "    final_output_path = output_path / config[\"output_filename\"]\n",
    "    for p in [output_path, data_analysis_path, zhuzhuyun_merge_path]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"  - 正在独立加载所有公司的'线路汇总数据'...\")\n",
    "    try:\n",
    "        all_routes = set()\n",
    "        files_in_analysis_result = list(\n",
    "            data_analysis_path.glob(\"*_data_analysis_result.xlsx\")\n",
    "        )\n",
    "        if not files_in_analysis_result:\n",
    "            raise FileNotFoundError(\n",
    "                f\"在路径 '{data_analysis_path.resolve()}' 中未找到任何 '*_data_analysis_result.xlsx' 文件。\"\n",
    "            )\n",
    "\n",
    "        for file_path in files_in_analysis_result:\n",
    "            if file_path.name.startswith(\"~$\"):\n",
    "                continue\n",
    "            try:\n",
    "                all_routes.update(\n",
    "                    pd.read_excel(\n",
    "                        file_path,\n",
    "                        sheet_name=\"线路汇总数据\",\n",
    "                        usecols=[\"路线\"],\n",
    "                        engine=\"openpyxl\",\n",
    "                    )[\"路线\"].unique()\n",
    "                )\n",
    "            except zipfile.BadZipFile:\n",
    "                print(\n",
    "                    f\"      -> 警告: 文件 {file_path.name} 已损坏或不是有效的Excel文件，已跳过。\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        df_main = pd.DataFrame(list(all_routes), columns=[\"路线\"])\n",
    "        df_base_info = pd.read_excel(\n",
    "            basic_data_path, sheet_name=\"inter-city_routes\", engine=\"openpyxl\"\n",
    "        ).rename(columns={\"公里\": \"线路里程\", \"经济圈\": \"城市圈\"})\n",
    "        cols_to_merge = [\n",
    "            \"寄出省份\",\n",
    "            \"寄出城市\",\n",
    "            \"寄达省份\",\n",
    "            \"寄达城市\",\n",
    "            \"路线\",\n",
    "            \"线路里程\",\n",
    "            \"城市圈\",\n",
    "        ]\n",
    "        df_main = pd.merge(\n",
    "            df_main,\n",
    "            df_base_info[[c for c in cols_to_merge if c in df_base_info.columns]],\n",
    "            on=\"路线\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        metrics_to_extract = [\n",
    "            \"快递数量\",\n",
    "            \"全程时限\",\n",
    "            \"寄出地处理时限\",\n",
    "            \"运输时限\",\n",
    "            \"寄达地处理时限\",\n",
    "            \"投递时限\",\n",
    "            \"揽收-到达寄出地分拣中心时长\",\n",
    "            \"到达寄出地分拣中心-离开寄出地城市时长\",\n",
    "            \"到达寄达地城市-离开寄达地分拣中心时长\",\n",
    "            \"离开寄达地分拣中心-派件\",\n",
    "            \"72小时准时率\",\n",
    "            \"48小时准时率\",\n",
    "            \"送达天数_80分位\",\n",
    "            \"中转次数\",\n",
    "        ]\n",
    "\n",
    "        for file_path in files_in_analysis_result:\n",
    "            if file_path.name.startswith(\"~$\"):\n",
    "                continue\n",
    "            company_key = _find_company_key_from_filename(file_path.name)\n",
    "            if not company_key:\n",
    "                continue\n",
    "            try:\n",
    "                df_summary = pd.read_excel(\n",
    "                    file_path, sheet_name=\"线路汇总数据\", engine=\"openpyxl\"\n",
    "                )\n",
    "                for metric in metrics_to_extract:\n",
    "                    if metric in df_summary.columns:\n",
    "                        if metric == \"中转次数\":\n",
    "                            new_col_name = f\"{company_key}平均中转次数\"\n",
    "                        else:\n",
    "                            new_col_name = (\n",
    "                                f\"{metric.replace('_80分位', '')}{company_key}\"\n",
    "                            )\n",
    "\n",
    "                        df_metric = (\n",
    "                            df_summary[[\"路线\", metric]]\n",
    "                            .copy()\n",
    "                            .rename(columns={metric: new_col_name})\n",
    "                        )\n",
    "                        df_main = pd.merge(df_main, df_metric, on=\"路线\", how=\"left\")\n",
    "            except zipfile.BadZipFile:\n",
    "                print(\n",
    "                    f\"      -> 警告: 文件 {file_path.name} 已损坏或不是有效的Excel文件，已跳过。\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        df_main.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "        print(\"  - 正在计算'最优中转次数'...\")\n",
    "        all_transfer_cols = [\n",
    "            f\"{comp}平均中转次数\"\n",
    "            for comp in COMPANIES_ALL_TEN\n",
    "            if f\"{comp}平均中转次数\" in df_main.columns\n",
    "        ]\n",
    "        if all_transfer_cols:\n",
    "            cols_for_best_turnover = [c for c in all_transfer_cols if \"快包\" not in c]\n",
    "            if cols_for_best_turnover:\n",
    "                df_main[\"最优中转次数\"] = df_main[cols_for_best_turnover].min(axis=1)\n",
    "\n",
    "        # 重新计算行业统计值\n",
    "        stat_metrics = [\n",
    "            metric\n",
    "            for metric in metrics_to_extract\n",
    "            if metric not in [\"快递数量\", \"中转次数\"]\n",
    "        ]\n",
    "        for metric in stat_metrics:\n",
    "            base_metric_name = metric.replace(\"_80分位\", \"\")\n",
    "            metric_cols = [\n",
    "                f\"{base_metric_name}{comp}\"\n",
    "                for comp in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "                if f\"{base_metric_name}{comp}\" in df_main.columns\n",
    "            ]\n",
    "            if metric_cols:\n",
    "                df_main[f\"{base_metric_name}_average\"] = df_main[metric_cols].mean(\n",
    "                    axis=1\n",
    "                )\n",
    "                df_main[f\"{base_metric_name}_minimum\"] = df_main[metric_cols].min(\n",
    "                    axis=1\n",
    "                )\n",
    "                df_main[f\"{base_metric_name}_maximum\"] = df_main[metric_cols].max(\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "        # 重新计算排名\n",
    "        rank_metrics = [\n",
    "            \"48小时准时率\",\n",
    "            \"72小时准时率\",\n",
    "            \"全程时限\",\n",
    "            \"寄出地处理时限\",\n",
    "            \"运输时限\",\n",
    "            \"寄达地处理时限\",\n",
    "            \"投递时限\",\n",
    "            \"送达天数\",\n",
    "        ]\n",
    "        for param in rank_metrics:\n",
    "            is_desc = \"准时率\" in param\n",
    "            ems_cols = [\n",
    "                f\"{param}{comp}\"\n",
    "                for comp in COMPANIES_NINE_MAJOR\n",
    "                if f\"{param}{comp}\" in df_main.columns\n",
    "            ]\n",
    "            if ems_cols and f\"{param}EMS\" in df_main.columns:\n",
    "                df_main[f\"{param}_ems排名\"] = df_main[ems_cols].rank(\n",
    "                    axis=1, method=\"min\", ascending=not is_desc\n",
    "                )[f\"{param}EMS\"]\n",
    "            kb_cols = [\n",
    "                f\"{param}{comp}\"\n",
    "                for comp in COMPANIES_ALL_TEN\n",
    "                if comp != \"EMS\" and f\"{param}{comp}\" in df_main.columns\n",
    "            ]\n",
    "            if kb_cols and f\"{param}快包\" in df_main.columns:\n",
    "                df_main[f\"{param}_快包排名\"] = df_main[kb_cols].rank(\n",
    "                    axis=1, method=\"min\", ascending=not is_desc\n",
    "                )[f\"{param}快包\"]\n",
    "\n",
    "        # T+N 格式化\n",
    "        for col in df_main.columns:\n",
    "            if (\n",
    "                \"送达天数\" in col\n",
    "                and \"排名\" not in col\n",
    "                and df_main[col].dtype != \"object\"\n",
    "            ):\n",
    "                df_main[col] = pd.to_numeric(df_main[col], errors=\"coerce\").round()\n",
    "                df_main[col] = df_main[col].apply(\n",
    "                    lambda x: f\"T+{int(x)}\" if pd.notna(x) else x\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🔥🔥🔥 独立加载数据失败: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # top_30_cities 的加载仍然是需要的，因为它可能被其他函数（或未来的需求）使用\n",
    "    # 但在 create_sheet2_mail_details 中不再直接用于判断单个邮件\n",
    "    top_30_cities = load_top_cities(basic_data_path, \"30_top_volume_city_2024\")\n",
    "    if not top_30_cities:\n",
    "        print(\"🔥🔥🔥 错误：未能加载Top 30城市列表，流程中止。\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_s1 = create_sheet1_route_details(df_main, config[\"products\"])\n",
    "        # 在调用时，不再需要传递 top_30_cities\n",
    "        df_s2 = create_sheet2_mail_details(\n",
    "            data_analysis_path,\n",
    "            zhuzhuyun_merge_path,\n",
    "            config[\"products\"],\n",
    "            config[\"product_to_filename\"],\n",
    "            top_30_cities,\n",
    "            df_main,\n",
    "        )\n",
    "\n",
    "        city_cols_order = _generate_dynamic_column_order(config[\"products\"], \"city\")\n",
    "        province_cols_order = _generate_dynamic_column_order(\n",
    "            config[\"products\"], \"province\"\n",
    "        )\n",
    "\n",
    "        df_s3 = create_regional_report_data(\n",
    "            df_main, \"city\", city_cols_order, config[\"products\"]\n",
    "        )\n",
    "        df_s4 = create_regional_report_data(\n",
    "            df_main, \"province\", province_cols_order, config[\"products\"]\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- 所有数据计算完成，正在写入最终文件: {final_output_path.name} ---\")\n",
    "        with pd.ExcelWriter(final_output_path, engine=\"xlsxwriter\") as writer:\n",
    "            sheets_to_write = {\n",
    "                \"线路明细\": df_s1,\n",
    "                \"邮件明细\": df_s2,\n",
    "                \"分城市明细\": df_s3,\n",
    "                \"分省份明细\": df_s4,\n",
    "            }\n",
    "            for sheet_name, df in sheets_to_write.items():\n",
    "                print(f\"  - 正在写入Sheet: {sheet_name}...\")\n",
    "                if df is not None and not df.empty:\n",
    "                    df = df.loc[:, ~df.columns.duplicated()]\n",
    "                    for col in df.select_dtypes(\n",
    "                        include=[\"datetime64[ns]\", \"datetimetz\"]\n",
    "                    ).columns:\n",
    "                        df[col] = (\n",
    "                            df[col].dt.strftime(\"%Y-%m-%d %H:%M:%S\").replace(\"NaT\", \"\")\n",
    "                        )\n",
    "                    df.to_excel(\n",
    "                        writer, sheet_name=sheet_name, index=False, float_format=\"%.2f\"\n",
    "                    )\n",
    "                    auto_adjust_xlsx_columns(writer, df, sheet_name)\n",
    "\n",
    "        print(f\"\\n🎉🎉🎉 恭喜！已成功生成报告 '{config['output_filename']}'！🎉🎉🎉\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🔥🔥🔥 生成报告 '{config['output_filename']}' 时发生严重错误: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# --- 4. 执行主函数 ---\n",
    "if __name__ == \"__main__\":\n",
    "    generate_company_monthly_report(\"邮政\")\n",
    "    generate_company_monthly_report(\"极兔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649c511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据输入路径: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输出/data_analysis_result\n",
      "图片输出路径: /Users/lava/Documents/国家邮政局发展研究中心实习/python_data_analysis/报告数据/输出/4_报告图片\n",
      "Matplotlib 全局字体已成功设置为: Microsoft YaHei\n",
      "🚀 开始执行绘图与分析任务 (新版流程)...\n",
      "\n",
      "--- 正在处理公司: EMS ---\n",
      "    -> 图片已保存: EMS_揽收时间_分布图.png\n",
      "    -> 图片已保存: EMS_到达分拣中心时间_分布图.png\n",
      "    -> 图片已保存: EMS_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: EMS_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: EMS_派送时间_分布图.png\n",
      "    -> 图片已保存: EMS_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 快包 ---\n",
      "    -> 图片已保存: 快包_揽收时间_分布图.png\n",
      "    -> 图片已保存: 快包_到达分拣中心时间_分布图.png\n",
      "    -> 图片已保存: 快包_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 快包_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 快包_派送时间_分布图.png\n",
      "    -> 图片已保存: 快包_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 中通 ---\n",
      "    -> 图片已保存: 中通_揽收时间_分布图.png\n",
      "    -> 中通 的 '到达分拣中心时间' 数据为空或无法处理，不生成图片。\n",
      "    -> 图片已保存: 中通_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 中通_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 中通_派送时间_分布图.png\n",
      "    -> 图片已保存: 中通_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 京东 ---\n",
      "    -> 图片已保存: 京东_揽收时间_分布图.png\n",
      "    -> 图片已保存: 京东_到达分拣中心时间_分布图.png\n",
      "    -> 图片已保存: 京东_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 京东_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 京东_派送时间_分布图.png\n",
      "    -> 图片已保存: 京东_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 圆通 ---\n",
      "    -> 图片已保存: 圆通_揽收时间_分布图.png\n",
      "    -> 图片已保存: 圆通_到达分拣中心时间_分布图.png\n",
      "    -> 图片已保存: 圆通_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 圆通_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 圆通_派送时间_分布图.png\n",
      "    -> 图片已保存: 圆通_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 德邦 ---\n",
      "    -> 图片已保存: 德邦_揽收时间_分布图.png\n",
      "    -> 图片已保存: 德邦_到达分拣中心时间_分布图.png\n",
      "    -> 图片已保存: 德邦_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 德邦_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 德邦_派送时间_分布图.png\n",
      "    -> 图片已保存: 德邦_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 极兔 ---\n",
      "    -> 图片已保存: 极兔_揽收时间_分布图.png\n",
      "    -> 图片已保存: 极兔_到达分拣中心时间_分布图.png\n",
      "    -> 图片已保存: 极兔_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 极兔_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 极兔_派送时间_分布图.png\n",
      "    -> 图片已保存: 极兔_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 申通 ---\n",
      "    -> 图片已保存: 申通_揽收时间_分布图.png\n",
      "    -> 图片已保存: 申通_到达分拣中心时间_分布图.png\n",
      "    -> 图片已保存: 申通_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 申通_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 申通_派送时间_分布图.png\n",
      "    -> 图片已保存: 申通_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 韵达 ---\n",
      "    -> 图片已保存: 韵达_揽收时间_分布图.png\n",
      "    -> 图片已保存: 韵达_到达分拣中心时间_分布图.png\n",
      "    -> 图片已保存: 韵达_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 韵达_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 韵达_派送时间_分布图.png\n",
      "    -> 图片已保存: 韵达_签收时间_分布图.png\n",
      "\n",
      "--- 正在处理公司: 顺丰 ---\n",
      "    -> 图片已保存: 顺丰_揽收时间_分布图.png\n",
      "    -> 顺丰 的 '到达分拣中心时间' 数据为空或无法处理，不生成图片。\n",
      "    -> 图片已保存: 顺丰_离开寄件城市时间_分布图.png\n",
      "    -> 图片已保存: 顺丰_到达收件城市时间_分布图.png\n",
      "    -> 图片已保存: 顺丰_派送时间_分布图.png\n",
      "    -> 图片已保存: 顺丰_签收时间_分布图.png\n",
      "\n",
      "\n",
      "--- 📈 分析结果汇总 ---\n",
      "\n",
      "--- 到达分拣中心时间 (16:00 - 22:00) 占比排名 ---\n",
      "排名 | 公司   | 占比\n",
      "-----|--------|-------\n",
      "1    | 德邦     | 79.69%\n",
      "2    | EMS    | 61.22%\n",
      "3    | 极兔     | 55.08%\n",
      "4    | 韵达     | 49.64%\n",
      "5    | 快包     | 47.61%\n",
      "6    | 圆通     | 47.48%\n",
      "7    | 申通     | 47.45%\n",
      "8    | 京东     | 31.82%\n",
      "\n",
      "--- 离开寄件城市时间 (16:00 - 22:00) 占比排名 ---\n",
      "排名 | 公司   | 占比\n",
      "-----|--------|-------\n",
      "1    | 中通     | 55.57%\n",
      "2    | 京东     | 51.83%\n",
      "3    | 极兔     | 48.30%\n",
      "4    | EMS    | 46.93%\n",
      "5    | 申通     | 43.20%\n",
      "6    | 韵达     | 42.72%\n",
      "7    | 快包     | 34.38%\n",
      "8    | 圆通     | 29.71%\n",
      "9    | 顺丰     | 18.42%\n",
      "10   | 德邦     | 17.65%\n",
      "\n",
      "--- 离开寄件城市时间 (22:00 - 24:00) 占比排名 ---\n",
      "排名 | 公司   | 占比\n",
      "-----|--------|-------\n",
      "1    | 京东     | 27.67%\n",
      "2    | 韵达     | 26.64%\n",
      "3    | 申通     | 25.95%\n",
      "4    | 极兔     | 23.84%\n",
      "5    | 中通     | 22.67%\n",
      "6    | 圆通     | 22.20%\n",
      "7    | 顺丰     | 19.27%\n",
      "8    | EMS    | 17.96%\n",
      "9    | 快包     | 13.38%\n",
      "10   | 德邦     |  3.95%\n",
      "\n",
      "🎉 全部任务执行完毕！\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 7: 画图\n",
    "# ==============================================================================\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties, fontManager\n",
    "\n",
    "# --- 1. 配置区域 ---\n",
    "\n",
    "# 路径配置\n",
    "ROOT_PATH = Path.cwd()\n",
    "DATA_ANALYSIS_PATH = ROOT_PATH / \"报告数据\" / \"输出\" / \"data_analysis_result\"\n",
    "OUTPUT_IMAGE_PATH = ROOT_PATH / \"报告数据\" / \"输出\" / \"4_报告图片\"\n",
    "\n",
    "OUTPUT_IMAGE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"数据输入路径: {DATA_ANALYSIS_PATH}\")\n",
    "print(f\"图片输出路径: {OUTPUT_IMAGE_PATH}\")\n",
    "\n",
    "# 公司列表与文件名映射\n",
    "COMPANY_MAPPING = {\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"中通\": \"中通\",\n",
    "    \"京东\": \"京东\",\n",
    "    \"圆通\": \"圆通\",\n",
    "    \"德邦\": \"德邦\",\n",
    "    \"极兔\": \"极兔\",\n",
    "    \"申通\": \"申通\",\n",
    "    \"韵达\": \"韵达\",\n",
    "    \"顺丰\": \"顺丰\",\n",
    "    \"邮政\": \"邮政\",  # \"邮政\" 对应 \"邮政\" 文件\n",
    "}\n",
    "# 分析对象包含 \"快包\"，但在文件名层面它由 \"邮政\" 文件代表\n",
    "COMPANIES_TO_ANALYZE = [\n",
    "    \"EMS\",\n",
    "    \"快包\",\n",
    "    \"中通\",\n",
    "    \"京东\",\n",
    "    \"圆通\",\n",
    "    \"德邦\",\n",
    "    \"极兔\",\n",
    "    \"申通\",\n",
    "    \"韵达\",\n",
    "    \"顺丰\",\n",
    "]\n",
    "\n",
    "METRICS_TO_PLOT = [\n",
    "    \"揽收时间\",\n",
    "    \"到达分拣中心时间\",\n",
    "    \"离开寄件城市时间\",\n",
    "    \"到达收件城市时间\",\n",
    "    \"派送时间\",\n",
    "    \"签收时间\",\n",
    "]\n",
    "\n",
    "# --- 2. 字体解决方案 ---\n",
    "font_path_str = \"微软雅黑.ttf\"\n",
    "font_path_obj = Path(font_path_str)\n",
    "if font_path_obj.exists():\n",
    "    fontManager.addfont(str(font_path_obj))\n",
    "    chinese_font = FontProperties(fname=font_path_str)\n",
    "    plt.rcParams[\"font.sans-serif\"] = [chinese_font.get_name()]\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "    print(f\"Matplotlib 全局字体已成功设置为: {chinese_font.get_name()}\")\n",
    "else:\n",
    "    print(f\"错误: 在当前目录找不到字体文件 '{font_path_str}'。\")\n",
    "\n",
    "TIME_LABELS = [f\"{h:02d}:00-{(h + 1) % 24:02d}:00\" for h in range(24)]\n",
    "COLORS = [\n",
    "    \"olive\",\n",
    "    \"grey\",\n",
    "    \"yellow\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"palegoldenrod\",\n",
    "    \"darkolivegreen\",\n",
    "    \"pink\",\n",
    "    \"Thistle\",\n",
    "    \"steelblue\",\n",
    "    \"darkslategrey\",\n",
    "    \"slategray\",\n",
    "    \"tan\",\n",
    "    \"darkolivegreen\",\n",
    "    \"grey\",\n",
    "    \"pink\",\n",
    "    \"goldenrod\",\n",
    "    \"mediumslateblue\",\n",
    "    \"saddlebrown\",\n",
    "    \"olive\",\n",
    "    \"navy\",\n",
    "    \"sandybrown\",\n",
    "    \"moccasin\",\n",
    "    \"black\",\n",
    "]\n",
    "\n",
    "# --- 3. 辅助函数 ---\n",
    "\n",
    "\n",
    "def calculate_hourly_distribution(time_series: pd.Series) -> list:\n",
    "    \"\"\"高效计算给定时间序列中每小时的数据点数量\"\"\"\n",
    "    if time_series.empty or time_series.isna().all():\n",
    "        return [0] * 24\n",
    "\n",
    "    dt_series = pd.to_datetime(time_series, errors=\"coerce\").dropna()\n",
    "    if dt_series.empty:\n",
    "        return [0] * 24\n",
    "\n",
    "    counts = dt_series.dt.hour.value_counts().sort_index()\n",
    "    hourly_counts = [0] * 24\n",
    "    for hour, count in counts.items():\n",
    "        if 0 <= hour < 24:\n",
    "            hourly_counts[hour] = int(count)\n",
    "    return hourly_counts\n",
    "\n",
    "\n",
    "def plot_and_save_distribution(\n",
    "    company_name: str, metric_name: str, hourly_counts: list, output_path: Path\n",
    "):\n",
    "    if sum(hourly_counts) == 0:\n",
    "        print(\n",
    "            f\"    -> {company_name} 的 '{metric_name}' 数据为空或无法处理，不生成图片。\"\n",
    "        )\n",
    "        return\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    bars = plt.bar(TIME_LABELS, hourly_counts, width=0.5, color=COLORS)\n",
    "    total_count = sum(hourly_counts)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            percentage = height / total_count\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{percentage:.1%}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=16,\n",
    "            )\n",
    "    plt.xlabel(\"24小时分布\", fontsize=16)\n",
    "    plt.ylabel(\"快件数量\", fontsize=16)\n",
    "    plt.xticks(rotation=75, fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    save_path = output_path / f\"{company_name}_{metric_name}_分布图.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"    -> 图片已保存: {save_path.name}\")\n",
    "\n",
    "\n",
    "# --- 4. 主执行流程 ---\n",
    "\n",
    "\n",
    "def run_plotting_and_analysis():\n",
    "    print(\"🚀 开始执行分析任务...\")\n",
    "    analysis_results = {}\n",
    "\n",
    "    for company in COMPANIES_TO_ANALYZE:\n",
    "        print(f\"\\n--- 正在处理公司: {company} ---\")\n",
    "\n",
    "        file_prefix = (\n",
    "            \"邮政\" if company == \"快包\" else COMPANY_MAPPING.get(company, company)\n",
    "        )\n",
    "        file_path = DATA_ANALYSIS_PATH / f\"{file_prefix}_data_analysis_result.xlsx\"\n",
    "\n",
    "        if not file_path.exists():\n",
    "            print(f\"  -> 未找到文件: {file_path.name}，跳过该公司。\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # 读取 '线路详细数据' sheet\n",
    "            df = pd.read_excel(file_path, sheet_name=\"线路详细数据\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> 读取文件 {file_path.name} 失败: {e}，跳过该公司。\")\n",
    "            continue\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"  -> 文件 {file_path.name} 的 '线路详细数据' sheet 为空，跳过。\")\n",
    "            continue\n",
    "\n",
    "        analysis_results[company] = {}\n",
    "\n",
    "        for metric in METRICS_TO_PLOT:\n",
    "            if metric not in df.columns:\n",
    "                print(f\"    -> '{metric}' 列不存在，跳过。\")\n",
    "                continue\n",
    "\n",
    "            hourly_counts = calculate_hourly_distribution(df[metric])\n",
    "            plot_and_save_distribution(\n",
    "                company, metric, hourly_counts, OUTPUT_IMAGE_PATH\n",
    "            )\n",
    "\n",
    "            total_count = sum(hourly_counts)\n",
    "            if total_count == 0:\n",
    "                continue\n",
    "\n",
    "            if metric == \"到达分拣中心时间\":\n",
    "                count_16_22 = sum(hourly_counts[16:22])\n",
    "                analysis_results[company][\"分拣中心_16_22_占比\"] = (\n",
    "                    count_16_22 / total_count\n",
    "                )\n",
    "            if metric == \"离开寄件城市时间\":\n",
    "                count_16_22 = sum(hourly_counts[16:22])\n",
    "                analysis_results[company][\"离开城市_16_22_占比\"] = (\n",
    "                    count_16_22 / total_count\n",
    "                )\n",
    "                count_22_24 = sum(hourly_counts[22:24])\n",
    "                analysis_results[company][\"离开城市_22_24_占比\"] = (\n",
    "                    count_22_24 / total_count\n",
    "                )\n",
    "\n",
    "    print(\"\\n\\n--- 📈 分析结果汇总 ---\")\n",
    "\n",
    "    def print_ranking_results(metric_key: str, description: str):\n",
    "        print(f\"\\n--- {description} 占比排名 ---\")\n",
    "        company_ratios = []\n",
    "        for company, metrics in analysis_results.items():\n",
    "            if metric_key in metrics:\n",
    "                company_ratios.append((company, metrics[metric_key]))\n",
    "        if not company_ratios:\n",
    "            print(\"无相关数据可供排名。\")\n",
    "            return\n",
    "        sorted_ratios = sorted(company_ratios, key=lambda item: item[1], reverse=True)\n",
    "        print(\"排名 | 公司   | 占比\")\n",
    "        print(\"-----|--------|-------\")\n",
    "        for i, (company, ratio) in enumerate(sorted_ratios):\n",
    "            print(f\"{i + 1:<4} | {company:<6} | {ratio:>6.2%}\")\n",
    "\n",
    "    print_ranking_results(\"分拣中心_16_22_占比\", \"到达分拣中心时间 (16:00 - 22:00)\")\n",
    "    print_ranking_results(\"离开城市_16_22_占比\", \"离开寄件城市时间 (16:00 - 22:00)\")\n",
    "    print_ranking_results(\"离开城市_22_24_占比\", \"离开寄件城市时间 (22:00 - 24:00)\")\n",
    "    print(\"\\n🎉 全部任务执行完毕！\")\n",
    "\n",
    "\n",
    "# --- 5. 执行主函数 ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_plotting_and_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
