{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaaefffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 1: å¯¼å…¥åº“å¹¶è®¾ç½®é¡¹ç›®ç»“æ„\n",
    "# --------------------------------------------------\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# å¿½ç•¥æ¥è‡ª openpyxl çš„ç‰¹å®š UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "font_path = \"å¾®è½¯é›…é»‘.ttf\"\n",
    "\n",
    "try:\n",
    "    chinese_font = FontProperties(fname=font_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Font file not found. Please provide the correct path.\")\n",
    "    # Fallback to a generic font if the file is not found\n",
    "    chinese_font = FontProperties()\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "\n",
    "# --- é¡¹ç›®è·¯å¾„è®¾ç½®  ---\n",
    "# .\n",
    "# â”œâ”€â”€ 1_data_preprocess.ipynb\n",
    "# â””â”€â”€ æŠ¥å‘Šæ•°æ®/\n",
    "#     â”œâ”€â”€ è¾“å…¥/\n",
    "#     â”‚   â”œâ”€â”€ å®‰ç›‘æ•°æ®/              (å­˜æ”¾è€å¸ˆç»™çš„åŸå§‹Excelæ–‡ä»¶ï¼›10å®¶å…¬å¸ï¼Œ10ä¸ªæ–‡ä»¶)\n",
    "#     â”‚   â””â”€â”€ basic_data.xlsx          (åŸå¸‚ä¿¡æ¯ã€çº¿è·¯ä¿¡æ¯)\n",
    "#     â”œâ”€â”€ è¾“å‡º/                      (å­˜æ”¾æ‰€æœ‰æœ€ç»ˆç”Ÿæˆçš„æŠ¥å‘Š)\n",
    "#     â”œâ”€â”€ temp/\n",
    "#     â”‚   â”œâ”€â”€ 1_å¾…ä¸Šä¼ çŒªçŒªäº‘æ•°æ®/        (éœ€è¦é€ä¸ªæ‰‹åŠ¨ä¸Šä¼ åˆ°çŒªçŒªäº‘çš„æ–‡ä»¶ï¼›8å®¶å…¬å¸ï¼Œ16ä¸ªæ–‡ä»¶ï¼Œæ’é™¤é¡ºä¸°å’Œä¸­é€š)\n",
    "#     â”‚   â”œâ”€â”€ 2_çŒªçŒªäº‘ä¸‹è½½æ•°æ®/          (ã€æ‰‹åŠ¨æ”¾å…¥ã€‘å­˜æ”¾ä»çŒªçŒªäº‘ä¸‹è½½çš„ç»“æœæ–‡ä»¶ï¼›8å®¶å…¬å¸ï¼Œ16ä¸ªæ–‡ä»¶ï¼Œæ’é™¤é¡ºä¸°å’Œä¸­é€š)\n",
    "#     â”‚   â”œâ”€â”€ 3_çŒªçŒªäº‘åˆå¹¶æ•°æ®/         ï¼ˆçŒªçŒªäº‘ä¸‹è½½æ•°æ®æŒ‰å…¬å¸åˆå¹¶åæ•°æ®ï¼›8å®¶å…¬å¸ï¼Œ8ä¸ªæ–‡ä»¶ï¼Œæ’é™¤é¡ºä¸°å’Œä¸­é€šï¼‰\n",
    "#     â”‚   â”œâ”€â”€ 4_logisticsæ•°æ®         ï¼ˆå­˜æ”¾logisticsæ•°æ®â€”â€”æå–å®Œæ•´ç‰©æµä¿¡æ¯çš„æ—¶é—´æˆ³åçš„æ•°æ®ï¼›8å®¶å…¬å¸ï¼Œ8ä¸ªæ–‡ä»¶ï¼Œæ’é™¤é¡ºä¸°å’Œä¸­é€šï¼‰\n",
    "#     â””â”€â”€ â””â”€â”€ 5_ä¸­è½¬æ•°æ®/               (å­˜æ”¾ä¸­è½¬æ•°æ®â€”â€”æå–ä¸­è½¬åŸå¸‚å’Œå¹³å‡ä¸­è½¬æ¬¡æ•°åçš„æ•°æ®ï¼›8å®¶å…¬å¸ï¼Œ8ä¸ªæ–‡ä»¶ï¼Œæ’é™¤é¡ºä¸°å’Œä¸­é€š)\n",
    "# æ ¹ç›®å½•\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"æŠ¥å‘Šæ•°æ®\"\n",
    "# è¾“å…¥è·¯å¾„\n",
    "input_path = report_path / \"è¾“å…¥\"\n",
    "anjian_data_path = input_path / \"å®‰ç›‘æ•°æ®\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "# è¾“å‡ºè·¯å¾„\n",
    "output_path = report_path / \"è¾“å‡º\"\n",
    "# ä¸­é—´è¿‡ç¨‹æ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼Œç”¨äºå­˜æ”¾ä¸´æ—¶æ–‡ä»¶ï¼‰\n",
    "temp_path = report_path / \"temp\"\n",
    "upload_split_path = temp_path / \"1_å¾…ä¸Šä¼ çŒªçŒªäº‘æ–‡ä»¶\"  # å­˜æ”¾æ‹†åˆ†åå¾…ä¸Šä¼ çš„æ–‡ä»¶\n",
    "zhuzhuyun_download_path = temp_path / \"2_çŒªçŒªäº‘ä¸‹è½½æ•°æ®\"  # å…³é”®ï¼šè¿™æ˜¯æ‰‹åŠ¨æ”¾ç½®æ–‡ä»¶çš„ç›®å½•\n",
    "zhuzhuyun_merge_path = temp_path / \"3_çŒªçŒªäº‘åˆå¹¶æ•°æ®\"\n",
    "pycharm_input_path = temp_path / \"4_logisticsæ•°æ®\"\n",
    "transit_data_path = temp_path / \"5_ä¸­è½¬æ•°æ®\"\n",
    "\n",
    "# åˆ›å»ºæ‰€æœ‰éœ€è¦çš„æ–‡ä»¶å¤¹\n",
    "for p in [\n",
    "    report_path,\n",
    "    input_path,\n",
    "    anjian_data_path,\n",
    "    zhuzhuyun_download_path,\n",
    "    zhuzhuyun_merge_path,\n",
    "    transit_data_path,\n",
    "    output_path,\n",
    "    temp_path,\n",
    "    upload_split_path,\n",
    "    pycharm_input_path,\n",
    "]:\n",
    "    p.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd28b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­•ï¸ å¼€å§‹å¤„ç†æ•°æ®ï¼Œè¯»å–ç›®å½•ä¸ºï¼š'/Users/lava/Documents/å›½å®¶é‚®æ”¿å±€å‘å±•ç ”ç©¶ä¸­å¿ƒå®ä¹ /python_data_analysis/æŠ¥å‘Šæ•°æ®/è¾“å‡º/data_analysis_result'\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: é¡ºä¸°ï¼Œæ–‡ä»¶: é¡ºä¸°_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: EMSï¼Œæ–‡ä»¶: EMS_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: ä¸­é€šï¼Œæ–‡ä»¶: ä¸­é€š_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: æå…”ï¼Œæ–‡ä»¶: æå…”_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: éŸµè¾¾ï¼Œæ–‡ä»¶: éŸµè¾¾_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: åœ†é€šï¼Œæ–‡ä»¶: åœ†é€š_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: äº¬ä¸œï¼Œæ–‡ä»¶: äº¬ä¸œ_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: ç”³é€šï¼Œæ–‡ä»¶: ç”³é€š_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: å¾·é‚¦ï¼Œæ–‡ä»¶: å¾·é‚¦_data_analysis_result.xlsx...\n",
      "   - æ­£åœ¨å¤„ç†å…¬å¸: é‚®æ”¿ï¼Œæ–‡ä»¶: é‚®æ”¿_data_analysis_result.xlsx...\n",
      "\n",
      "==================================================\n",
      "âœ… ä»»åŠ¡å®Œæˆï¼\n",
      "æœˆåº¦æ±‡æ€»æ•°æ®å·²ä¿å­˜è‡³: â€˜/Users/lava/Documents/å›½å®¶é‚®æ”¿å±€å‘å±•ç ”ç©¶ä¸­å¿ƒå®ä¹ /python_data_analysis/æŠ¥å‘Šæ•°æ®/è¾“å‡º/1_æ±‡æ€»æ•°æ®/æœˆåº¦æ±‡æ€»æ•°æ®.xlsxâ€™\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 2: æ±‡æ€»æ•°æ®\n",
    "# --------------------------------------------------\n",
    "# --- è·¯å¾„è®¾ç½® ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"æŠ¥å‘Šæ•°æ®\"\n",
    "input_dir = report_path / \"è¾“å‡º\" / \"data_analysis_result\"\n",
    "output_dir = report_path / \"è¾“å‡º\" / \"1_æ±‡æ€»æ•°æ®\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_metrics_from_basic_sheet(df, kilo_params):\n",
    "    \"\"\"\n",
    "    ä»â€œåŸºç¡€æŒ‡æ ‡â€DataFrameä¸­ç›´æ¥æå–æ‰€æœ‰éœ€è¦çš„æŒ‡æ ‡ã€‚\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.Series(dtype=\"float64\")\n",
    "\n",
    "    metrics_table = df.set_index(\"é¡¹ç›®\")\n",
    "    results = {}\n",
    "\n",
    "    time_limit_cols = [\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é™\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é™\",\n",
    "    ]\n",
    "    on_time_cols = [\"72å°æ—¶å‡†æ—¶ç‡\", \"48å°æ—¶å‡†æ—¶ç‡\"]\n",
    "\n",
    "    for col in time_limit_cols + on_time_cols:\n",
    "        results[col] = metrics_table.loc[col, \"mean\"]\n",
    "\n",
    "    for param in kilo_params:\n",
    "        results[param] = metrics_table.loc[\"å…¨ç¨‹æ—¶é™\", param]\n",
    "\n",
    "    return pd.Series(results)\n",
    "\n",
    "\n",
    "def main(companies, params):\n",
    "    \"\"\"\n",
    "    ä¸»å‡½æ•°ï¼Œå¤„ç†æ‰€æœ‰å…¬å¸çš„æ–‡ä»¶å¹¶ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šã€‚\n",
    "    (æ­¤å‡½æ•°é€»è¾‘æ­£ç¡®ï¼Œæ— éœ€ä¿®æ”¹)\n",
    "    \"\"\"\n",
    "    all_metrics_cols = [\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é™\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é™\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "    ] + params\n",
    "\n",
    "    summary_df = pd.DataFrame(index=companies, columns=all_metrics_cols)\n",
    "\n",
    "    print(f\"â­•ï¸ å¼€å§‹å¤„ç†æ•°æ®ï¼Œè¯»å–ç›®å½•ä¸ºï¼š'{input_dir}'\")\n",
    "\n",
    "    for company in companies:\n",
    "        # æ–‡ä»¶åæ˜¯æ ¹æ®å…¬å¸ååŠ¨æ€ç”Ÿæˆçš„\n",
    "        file_name = f\"{company}_data_analysis_result.xlsx\"\n",
    "        # æ–‡ä»¶è·¯å¾„æ˜¯ è¾“å…¥ç›®å½• + æ–‡ä»¶å\n",
    "        file_path = input_dir / file_name\n",
    "\n",
    "        try:\n",
    "            print(f\"   - æ­£åœ¨å¤„ç†å…¬å¸: {company}ï¼Œæ–‡ä»¶: {file_name}...\")\n",
    "\n",
    "            # è¯»å–â€œåŸºç¡€æŒ‡æ ‡â€sheet\n",
    "            basic_metrics_df = pd.read_excel(file_path, sheet_name=\"åŸºç¡€æŒ‡æ ‡\")\n",
    "\n",
    "            # ä»åŸºç¡€æŒ‡æ ‡ä¸­ç›´æ¥æå–\n",
    "            metrics = extract_metrics_from_basic_sheet(basic_metrics_df, params)\n",
    "\n",
    "            # å¡«å……åˆ°æ±‡æ€»DataFrame\n",
    "            summary_df.loc[company, metrics.index] = metrics\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"   âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {file_path}ï¼Œè·³è¿‡è¯¥å…¬å¸ã€‚\")\n",
    "        except KeyError as e:\n",
    "            print(\n",
    "                f\"   âŒ é”™è¯¯: åœ¨æ–‡ä»¶ {file_path} çš„'åŸºç¡€æŒ‡æ ‡'Sheetä¸­æœªæ‰¾åˆ°å…³é”®é¡¹ç›®: {e}ã€‚\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ é”™è¯¯ï¼šå¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "\n",
    "    summary_df = summary_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return summary_df.round(4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    companies_list = [\n",
    "        \"é¡ºä¸°\",\n",
    "        \"EMS\",\n",
    "        \"ä¸­é€š\",\n",
    "        \"æå…”\",\n",
    "        \"éŸµè¾¾\",\n",
    "        \"åœ†é€š\",\n",
    "        \"äº¬ä¸œ\",\n",
    "        \"ç”³é€š\",\n",
    "        \"å¾·é‚¦\",\n",
    "        \"é‚®æ”¿\",\n",
    "    ]\n",
    "    kilometer_params = [\"0-600\", \"600-1500\", \"1500-2500\", \"2500ä»¥ä¸Š\"]\n",
    "\n",
    "    final_summary = main(companies_list, kilometer_params)\n",
    "\n",
    "    output_file = output_dir / \"æœˆåº¦æ±‡æ€»æ•°æ®.xlsx\"\n",
    "    final_summary.to_excel(output_file)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"âœ… ä»»åŠ¡å®Œæˆï¼\")\n",
    "    print(f\"æœˆåº¦æ±‡æ€»æ•°æ®å·²ä¿å­˜è‡³: â€˜{output_file}â€™\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ca03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®åˆ†æå·¥ä½œæµ...\n",
      "âœ… æˆåŠŸä» 'basic_data.xlsx' åŠ è½½ 50 ä¸ªåŸå¸‚ã€‚\n",
      "\n",
      "==========================================================\n",
      "â¡ï¸ Part 1: å¼€å§‹æ‰§è¡Œåˆ†è·¯çº¿æ—¶é™åˆ†æ (RouteAnalysis)\n",
      "==========================================================\n",
      "  - æ­£åœ¨å¤„ç†: åœ†é€š\n",
      "  - æ­£åœ¨å¤„ç†: ç”³é€š\n",
      "  - æ­£åœ¨å¤„ç†: ä¸­é€š\n",
      "  - æ­£åœ¨å¤„ç†: æå…”\n",
      "  - æ­£åœ¨å¤„ç†: éŸµè¾¾\n",
      "  âœ… [Part 1] åˆ†è·¯çº¿æ—¶é™åˆ†æå®Œæˆã€‚\n",
      "\n",
      "==========================================================\n",
      "â¡ï¸ Part 2: å¼€å§‹æ‰§è¡Œå®è§‚æ¯”ä¾‹/æ—¶é•¿åˆ†æ (MacroRatioAnalysis)\n",
      "==========================================================\n",
      "  - æ­£åœ¨å¤„ç†: ä¸­é€š\n",
      "  - æ­£åœ¨å¤„ç†: åœ†é€š\n",
      "  - æ­£åœ¨å¤„ç†: æå…”\n",
      "  - æ­£åœ¨å¤„ç†: ç”³é€š\n",
      "  - æ­£åœ¨å¤„ç†: éŸµè¾¾\n",
      "  - æ­£åœ¨å¤„ç†: é¡ºä¸°\n",
      "  - æ­£åœ¨å¤„ç†: äº¬ä¸œ\n",
      "  - æ­£åœ¨å¤„ç†: EMS\n",
      "  - æ­£åœ¨å¤„ç†: å¾·é‚¦\n",
      "  âœ… [Part 2] å®è§‚æŒ‡æ ‡åˆ†æå®Œæˆã€‚\n",
      "\n",
      "... æ­£åœ¨æ•´åˆç»“æœåˆ°Excelæ–‡ä»¶ ...\n",
      "  - Sheet 'summary_tongdatu' å·²å†™å…¥ã€‚\n",
      "  - Sheet 'detail_tongdatu' å·²å†™å…¥ã€‚\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆï¼æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜è‡³: /Users/lava/Documents/å›½å®¶é‚®æ”¿å±€å‘å±•ç ”ç©¶ä¸­å¿ƒå®ä¹ /python_data_analysis/æŠ¥å‘Šæ•°æ®/è¾“å‡º/5_é€šè¾¾å…”æ•°æ®/é€šè¾¾å…”åˆ†æ®µåˆ†æ.xlsx ğŸ‰ğŸ‰ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Cell 3: é€šè¾¾å…”ä¸“é¢˜åˆ†æ\n",
    "# --------------------------------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- è·¯å¾„è®¾ç½® ---\n",
    "ROOT_PATH = Path.cwd()\n",
    "DATA_ANALYSIS_DIR = ROOT_PATH / \"æŠ¥å‘Šæ•°æ®\" / \"è¾“å‡º\" / \"data_analysis_result\"\n",
    "OUTPUT_DIR = ROOT_PATH / \"æŠ¥å‘Šæ•°æ®\" / \"è¾“å‡º\" / \"5_é€šè¾¾å…”æ•°æ®\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BASIC_DATA_PATH = ROOT_PATH / \"æŠ¥å‘Šæ•°æ®\" / \"è¾“å…¥\" / \"basic_data.xlsx\"\n",
    "\n",
    "\n",
    "# --- å…¬å¸å’ŒåŸå¸‚åˆ—è¡¨ ---\n",
    "def load_city_list_from_excel(file_path, sheet_name):\n",
    "    try:\n",
    "        if not file_path.exists():\n",
    "            print(\n",
    "                f\"âŒ ä¸¥é‡é”™è¯¯ï¼šåŸºç¡€æ•°æ®æ–‡ä»¶ 'basic_data.xlsx' æœªåœ¨ä»¥ä¸‹è·¯å¾„æ‰¾åˆ°:\\n{file_path}\"\n",
    "            )\n",
    "            return None\n",
    "        df_cities = pd.read_excel(file_path, sheet_name=sheet_name, header=0)\n",
    "        if df_cities.columns[0] not in df_cities.columns:\n",
    "            print(f\"âŒ ä¸¥é‡é”™è¯¯: åœ¨Sheet '{sheet_name}' ä¸­æœªæ‰¾åˆ°åŸå¸‚åˆ—ã€‚\")\n",
    "            return None\n",
    "        city_list = (\n",
    "            df_cities.iloc[:, 0].dropna().astype(str).str.strip().unique().tolist()\n",
    "        )\n",
    "        if not city_list:\n",
    "            print(\n",
    "                f\"âŒ ä¸¥é‡é”™è¯¯ï¼šåœ¨ '{file_path.name}' çš„ '{sheet_name}' sheeté¡µä¸­æœªèƒ½åŠ è½½åˆ°ä»»ä½•åŸå¸‚æ•°æ®ã€‚\"\n",
    "            )\n",
    "            return None\n",
    "        print(f\"âœ… æˆåŠŸä» 'basic_data.xlsx' åŠ è½½ {len(city_list)} ä¸ªåŸå¸‚ã€‚\")\n",
    "        return city_list\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¸¥é‡é”™è¯¯ï¼šåŠ è½½åŸå¸‚åˆ—è¡¨æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# å…¬å¸åˆ—è¡¨å®šä¹‰\n",
    "COMPANY_FILE_MAP = {\n",
    "    \"ä¸­é€š\": \"ä¸­é€š\",\n",
    "    \"åœ†é€š\": \"åœ†é€š\",\n",
    "    \"æå…”\": \"æå…”\",\n",
    "    \"ç”³é€š\": \"ç”³é€š\",\n",
    "    \"éŸµè¾¾\": \"éŸµè¾¾\",\n",
    "    \"é¡ºä¸°\": \"é¡ºä¸°\",\n",
    "    \"äº¬ä¸œ\": \"äº¬ä¸œ\",\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"å¾·é‚¦\": \"å¾·é‚¦\",\n",
    "    \"å¿«åŒ…\": \"é‚®æ”¿\",\n",
    "}\n",
    "COMPANIES_FOR_MACRO_ANALYSIS = [c for c in COMPANY_FILE_MAP.keys() if c != \"å¿«åŒ…\"]\n",
    "COMPANIES_TONGDATU = [\"åœ†é€š\", \"ç”³é€š\", \"ä¸­é€š\", \"æå…”\", \"éŸµè¾¾\"]\n",
    "CITY_LIST = None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Part 1: åˆ†è·¯çº¿æ—¶é™åˆ†æ (é€»è¾‘ä¸å˜)\n",
    "# ==============================================================================\n",
    "def run_route_analysis(city_list):\n",
    "    print(\"\\n==========================================================\")\n",
    "    print(\"â¡ï¸ Part 1: å¼€å§‹æ‰§è¡Œåˆ†è·¯çº¿æ—¶é™åˆ†æ (RouteAnalysis)\")\n",
    "    print(\"==========================================================\")\n",
    "    city_routes = [f\"{c1}-{c2}\" for c1 in city_list for c2 in city_list if c1 != c2]\n",
    "    aggregated_results_df = pd.DataFrame({\"è·¯çº¿\": city_routes})\n",
    "    required_time_cols = [\n",
    "        \"æ½æ”¶æ—¶é—´\",\n",
    "        \"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\",\n",
    "        \"ç¦»å¼€æ”¶ä»¶åŸå¸‚åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\",\n",
    "        \"ç­¾æ”¶æ—¶é—´\",\n",
    "        \"å¯„å‡ºåŸå¸‚\",\n",
    "        \"å¯„è¾¾åŸå¸‚\",\n",
    "    ]\n",
    "    for company in COMPANIES_TONGDATU:\n",
    "        file_prefix = COMPANY_FILE_MAP.get(company)\n",
    "        file_path = DATA_ANALYSIS_DIR / f\"{file_prefix}_data_analysis_result.xlsx\"\n",
    "        if not file_path.exists():\n",
    "            continue\n",
    "        print(f\"  - æ­£åœ¨å¤„ç†: {company}\")\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=\"çº¿è·¯è¯¦ç»†æ•°æ®\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - âš ï¸ è­¦å‘Š: è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}\")\n",
    "            continue\n",
    "        if not all(col in df.columns for col in required_time_cols):\n",
    "            print(\n",
    "                f\"  - âš ï¸ è­¦å‘Š: {file_path.name} çš„'çº¿è·¯è¯¦ç»†æ•°æ®'sheetç¼ºå°‘å¿…è¦çš„æ—¶é—´åˆ—ï¼Œè·³è¿‡ Part 1 çš„ {company} åˆ†æã€‚\"\n",
    "            )\n",
    "            continue\n",
    "        for col in required_time_cols:\n",
    "            if \"æ—¶é—´\" in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        df.dropna(\n",
    "            subset=[col for col in required_time_cols if \"æ—¶é—´\" in col], inplace=True\n",
    "        )\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df[\"è·¯çº¿\"] = (\n",
    "            df[\"å¯„å‡ºåŸå¸‚\"].astype(str).str.strip()\n",
    "            + \"-\"\n",
    "            + df[\"å¯„è¾¾åŸå¸‚\"].astype(str).str.strip()\n",
    "        )\n",
    "        df[f\"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒï¼ˆå°æ—¶ï¼‰{company}\"] = (\n",
    "            df[\"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\"] - df[\"æ½æ”¶æ—¶é—´\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        df[f\"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒï¼ˆå°æ—¶ï¼‰{company}\"] = (\n",
    "            df[\"ç¦»å¼€æ”¶ä»¶åŸå¸‚åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\"] - df[\"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        df[f\"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-ç­¾æ”¶ï¼ˆå°æ—¶ï¼‰{company}\"] = (\n",
    "            df[\"ç­¾æ”¶æ—¶é—´\"] - df[\"ç¦»å¼€æ”¶ä»¶åŸå¸‚åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        company_metrics = [\n",
    "            f\"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒï¼ˆå°æ—¶ï¼‰{company}\",\n",
    "            f\"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒï¼ˆå°æ—¶ï¼‰{company}\",\n",
    "            f\"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-ç­¾æ”¶ï¼ˆå°æ—¶ï¼‰{company}\",\n",
    "        ]\n",
    "        company_agg = df.groupby(\"è·¯çº¿\")[company_metrics].mean().reset_index()\n",
    "        aggregated_results_df = pd.merge(\n",
    "            aggregated_results_df, company_agg, on=\"è·¯çº¿\", how=\"left\"\n",
    "        )\n",
    "    metric_templates = [\n",
    "        \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒï¼ˆå°æ—¶ï¼‰\",\n",
    "        \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒï¼ˆå°æ—¶ï¼‰\",\n",
    "        \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-ç­¾æ”¶ï¼ˆå°æ—¶ï¼‰\",\n",
    "    ]\n",
    "    for metric in metric_templates:\n",
    "        company_cols = [\n",
    "            f\"{metric}{comp}\"\n",
    "            for comp in COMPANIES_TONGDATU\n",
    "            if f\"{metric}{comp}\" in aggregated_results_df.columns\n",
    "        ]\n",
    "        if company_cols:\n",
    "            aggregated_results_df[f\"{metric}é€šè¾¾å…”å‡å€¼\"] = aggregated_results_df[\n",
    "                company_cols\n",
    "            ].mean(axis=1)\n",
    "            aggregated_results_df[f\"{metric}é€šè¾¾å…”æœ€ä¼˜\"] = aggregated_results_df[\n",
    "                company_cols\n",
    "            ].min(axis=1)\n",
    "    print(\"  âœ… [Part 1] åˆ†è·¯çº¿æ—¶é™åˆ†æå®Œæˆã€‚\")\n",
    "    return aggregated_results_df\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Part 2: å®è§‚æ¯”ä¾‹/æ—¶é•¿åˆ†æ (æŒ‰æ–°åŸåˆ™ä¿®æ”¹)\n",
    "# ==============================================================================\n",
    "def run_macro_ratio_analysis():\n",
    "    print(\"\\n==========================================================\")\n",
    "    print(\"â¡ï¸ Part 2: å¼€å§‹æ‰§è¡Œå®è§‚æ¯”ä¾‹/æ—¶é•¿åˆ†æ (MacroRatioAnalysis)\")\n",
    "    print(\"==========================================================\")\n",
    "    all_time_cols = [\n",
    "        \"æ½æ”¶æ—¶é—´\",\n",
    "        \"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\",\n",
    "        \"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´\",\n",
    "        \"åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´\",\n",
    "        \"ç¦»å¼€æ”¶ä»¶åŸå¸‚åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\",\n",
    "        \"ç­¾æ”¶æ—¶é—´\",\n",
    "    ]\n",
    "    results = []\n",
    "\n",
    "    for company_key in COMPANIES_FOR_MACRO_ANALYSIS:\n",
    "        file_prefix = COMPANY_FILE_MAP.get(company_key, company_key)\n",
    "        file_path = DATA_ANALYSIS_DIR / f\"{file_prefix}_data_analysis_result.xlsx\"\n",
    "        if not file_path.exists():\n",
    "            continue\n",
    "        print(f\"  - æ­£åœ¨å¤„ç†: {company_key}\")\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=\"çº¿è·¯è¯¦ç»†æ•°æ®\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - âš ï¸ è­¦å‘Š: è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}\")\n",
    "            continue\n",
    "        if df.empty or not all(col in df.columns for col in all_time_cols):\n",
    "            print(\n",
    "                f\"  - âš ï¸ è­¦å‘Š: {file_path.name} çš„'çº¿è·¯è¯¦ç»†æ•°æ®'sheetä¸å®Œæ•´ï¼Œè·³è¿‡ Part 2 çš„ {company_key} åˆ†æã€‚\"\n",
    "            )\n",
    "            continue\n",
    "        for col in all_time_cols:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        df.dropna(subset=all_time_cols, inplace=True)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        company_metrics = {\"å…¬å¸\": company_key}\n",
    "        company_metrics[\"æ½ä»¶å’Œè¿›åˆ†æ‹¨ä¸­å¿ƒåŒä¸€å¤©æ¯”ä¾‹\"] = (\n",
    "            df[\"æ½æ”¶æ—¶é—´\"].dt.date == df[\"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\"].dt.date\n",
    "        ).mean()\n",
    "        company_metrics[\"ä»æ½ä»¶åˆ°ç¦»å¼€å¯„å‡ºåœ°åœ¨12å°æ—¶ä¹‹å†…çš„æ¯”ä¾‹\"] = (\n",
    "            (df[\"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´\"] - df[\"æ½æ”¶æ—¶é—´\"]) / np.timedelta64(1, \"h\") < 12\n",
    "        ).mean()\n",
    "        handling_time_out = (\n",
    "            df[\"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´\"] - df[\"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        handling_time_in = (\n",
    "            df[\"ç¦»å¼€æ”¶ä»¶åŸå¸‚åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\"] - df[\"åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´\"]\n",
    "        ) / np.timedelta64(1, \"h\")\n",
    "        company_metrics[\"å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒå¤„ç†æ—¶é•¿\"] = handling_time_out.mean()\n",
    "        company_metrics[\"å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒå¤„ç†æ—¶é•¿\"] = handling_time_in.mean()\n",
    "        company_metrics[\"å¯„è¾¾åœ°åˆ†æ‹¨ä¸­å¿ƒå¤„ç†è¶…è¿‡12å°æ—¶æ¯”ä¾‹\"] = (\n",
    "            handling_time_in > 12\n",
    "        ).mean()\n",
    "        results.append(company_metrics)\n",
    "\n",
    "    if not results:\n",
    "        print(\"  - âŒ é”™è¯¯: æœªèƒ½ä»ä»»ä½•å…¬å¸æ–‡ä»¶ä¸­è®¡ç®—å‡ºå®è§‚æŒ‡æ ‡ã€‚\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_final = pd.DataFrame(results).set_index(\"å…¬å¸\").T\n",
    "\n",
    "    # è®¡ç®—é€šè¾¾å…”å‡å€¼å’Œæœ€ä¼˜å€¼\n",
    "    higher_is_better_metrics = [\n",
    "        \"æ½ä»¶å’Œè¿›åˆ†æ‹¨ä¸­å¿ƒåŒä¸€å¤©æ¯”ä¾‹\",\n",
    "        \"ä»æ½ä»¶åˆ°ç¦»å¼€å¯„å‡ºåœ°åœ¨12å°æ—¶ä¹‹å†…çš„æ¯”ä¾‹\",\n",
    "    ]\n",
    "    lower_is_better_metrics = [\n",
    "        \"å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒå¤„ç†æ—¶é•¿\",\n",
    "        \"å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒå¤„ç†æ—¶é•¿\",\n",
    "        \"å¯„è¾¾åœ°åˆ†æ‹¨ä¸­å¿ƒå¤„ç†è¶…è¿‡12å°æ—¶æ¯”ä¾‹\",\n",
    "    ]\n",
    "\n",
    "    tongdatu_cols = df_final.columns.intersection(COMPANIES_TONGDATU).tolist()\n",
    "    if tongdatu_cols:\n",
    "        df_final[\"é€šè¾¾å…”å‡å€¼\"] = df_final[tongdatu_cols].mean(axis=1)\n",
    "\n",
    "        # (æ ¸å¿ƒä¿®æ­£) åœ¨èµ‹å€¼å‰ï¼Œå…ˆç­›é€‰å‡ºå®é™…å­˜åœ¨çš„æŒ‡æ ‡\n",
    "        existing_higher_metrics = [\n",
    "            m for m in higher_is_better_metrics if m in df_final.index\n",
    "        ]\n",
    "        if existing_higher_metrics:\n",
    "            df_final.loc[existing_higher_metrics, \"é€šè¾¾å…”æœ€ä¼˜å€¼\"] = df_final.loc[\n",
    "                existing_higher_metrics, tongdatu_cols\n",
    "            ].max(axis=1)\n",
    "\n",
    "        existing_lower_metrics = [\n",
    "            m for m in lower_is_better_metrics if m in df_final.index\n",
    "        ]\n",
    "        if existing_lower_metrics:\n",
    "            df_final.loc[existing_lower_metrics, \"é€šè¾¾å…”æœ€ä¼˜å€¼\"] = df_final.loc[\n",
    "                existing_lower_metrics, tongdatu_cols\n",
    "            ].min(axis=1)\n",
    "\n",
    "    print(\"  âœ… [Part 2] å®è§‚æŒ‡æ ‡åˆ†æå®Œæˆã€‚\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®åˆ†æå·¥ä½œæµ...\")\n",
    "    CITY_LIST = load_city_list_from_excel(BASIC_DATA_PATH, \"50_focus_cities\")\n",
    "    if CITY_LIST is None:\n",
    "        print(\"\\nğŸ›‘ ç”±äºæ— æ³•åŠ è½½åŸå¸‚åˆ—è¡¨ï¼Œå·¥ä½œæµå·²ä¸­æ­¢ã€‚\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    df_detail = run_route_analysis(CITY_LIST)\n",
    "    df_summary = run_macro_ratio_analysis()\n",
    "\n",
    "    final_output_path = OUTPUT_DIR / \"é€šè¾¾å…”åˆ†æ®µåˆ†æ.xlsx\"\n",
    "    print(f\"\\n... æ­£åœ¨æ•´åˆç»“æœåˆ°Excelæ–‡ä»¶ ...\")\n",
    "    with pd.ExcelWriter(final_output_path, engine=\"openpyxl\") as writer:\n",
    "        if not df_summary.empty:\n",
    "            df_summary.to_excel(writer, sheet_name=\"summary_tongdatu\", index=True)\n",
    "            print(f\"  - Sheet 'summary_tongdatu' å·²å†™å…¥ã€‚\")\n",
    "        else:\n",
    "            print(\"  - âš ï¸ è­¦å‘Š: å®è§‚åˆ†æç»“æœä¸ºç©ºï¼Œæœªå†™å…¥ 'summary_tongdatu' sheetã€‚\")\n",
    "\n",
    "        if not df_detail.empty:\n",
    "            df_detail.to_excel(writer, sheet_name=\"detail_tongdatu\", index=False)\n",
    "            print(f\"  - Sheet 'detail_tongdatu' å·²å†™å…¥ã€‚\")\n",
    "        else:\n",
    "            print(\"  - âš ï¸ è­¦å‘Š: è·¯çº¿åˆ†æç»“æœä¸ºç©ºï¼Œæœªå†™å…¥ 'detail_tongdatu' sheetã€‚\")\n",
    "    print(f\"\\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆï¼æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜è‡³: {final_output_path} ğŸ‰ğŸ‰ğŸ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0206de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â—â—â— åˆ†ææµç¨‹å¯åŠ¨ â—â—â—\n",
      "--- ä»»åŠ¡å¼€å§‹ï¼šç”Ÿæˆæœ€ç»ˆçº¿è·¯æ˜ç»†æŠ¥å‘Š ---\n",
      "\n",
      "[1/4] åŠ¨æ€æ„å»ºçº¿è·¯å…¨é›†å¹¶åˆå¹¶åŸºç¡€ä¿¡æ¯...\n",
      "\n",
      "[2/4] ä»'çº¿è·¯æ±‡æ€»æ•°æ®'æå–æ‰€æœ‰é¢„è®¡ç®—æŒ‡æ ‡...\n",
      "\n",
      "[3/4] è®¡ç®—å„é¡¹æŒ‡æ ‡çš„ç»Ÿè®¡å€¼...\n",
      "\n",
      "[4/4] è®¡ç®—æ’å...\n",
      "\n",
      "- æ­£åœ¨æ¸…ç†å®Œå…¨ä¸ºç©ºçš„æŒ‡æ ‡åˆ—...\n",
      "- æ¸…ç†å®Œæˆã€‚\n",
      "\n",
      "--- æ­£åœ¨ç”Ÿæˆæ•°æ®ä¸­å°æŠ¥å‘Š: /Users/lava/Documents/å›½å®¶é‚®æ”¿å±€å‘å±•ç ”ç©¶ä¸­å¿ƒå®ä¹ /python_data_analysis/æŠ¥å‘Šæ•°æ®/è¾“å‡º/åˆ†ææ€»æŠ¥å‘Š.xlsx ---\n",
      "  - æ­£åœ¨å†™å…¥'æœ€ç»ˆçº¿è·¯æ˜ç»†ç»“æœ' Sheet...\n",
      "  - æ­£åœ¨æŒ‰ 'å¯„å‡ºçœä»½, å¯„å‡ºåŸå¸‚' è¿›è¡ŒåŠ æƒå¹³å‡èšåˆ...\n",
      "  - æ­£åœ¨å†™å…¥'å¯„å‡ºåœ°æ±‡æ€»' Sheet...\n",
      "  - æ­£åœ¨æŒ‰ 'å¯„å‡ºçœä»½, å¯„å‡ºåŸå¸‚' è¿›è¡ŒåŠ æƒå¹³å‡èšåˆ...\n",
      "  - æ­£åœ¨å†™å…¥'å¯„è¾¾åœ°æ±‡æ€»' Sheet...\n",
      "--- âœ“ æ•°æ®ä¸­å°ç”Ÿæˆå®Œæ¯• (åŒ…å«æ±‡æ€»è¡¨) ---\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ•°æ®ä¸­å°ä»»åŠ¡å·²å…¨éƒ¨æ‰§è¡Œå®Œæ¯•ï¼ğŸ‰ğŸ‰ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 4: æ•°æ®ä¸­å°ç”Ÿæˆ (æœ€ç»ˆæ­£ç¡®ç‰ˆ - ç»Ÿä¸€å£å¾„)\n",
    "# ==============================================================================\n",
    "import traceback\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0. å…¨å±€é…ç½®ä¸è¾…åŠ©å‡½æ•° ---\n",
    "COMPANY_MAPPING = {\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"å¾·é‚¦\": \"å¾·é‚¦\",\n",
    "    \"æå…”\": \"æå…”\",\n",
    "    \"åœ†é€š\": \"åœ†é€š\",\n",
    "    \"é¡ºä¸°\": \"é¡ºä¸°\",\n",
    "    \"ä¸­é€š\": \"ä¸­é€š\",\n",
    "    \"äº¬ä¸œ\": \"äº¬ä¸œ\",\n",
    "    \"éŸµè¾¾\": \"éŸµè¾¾\",\n",
    "    \"ç”³é€š\": \"ç”³é€š\",\n",
    "    \"é‚®æ”¿\": \"å¿«åŒ…\",\n",
    "    \"å¿«åŒ…\": \"å¿«åŒ…\",\n",
    "}\n",
    "\n",
    "# æ˜ç¡®å®šä¹‰å„ä¸ªå…¬å¸åˆ—è¡¨çš„ç”¨é€”\n",
    "COMPANIES_FOR_INDUSTRY_COMPARISON = [\n",
    "    \"EMS\",\n",
    "    \"ä¸­é€š\",\n",
    "    \"äº¬ä¸œ\",\n",
    "    \"åœ†é€š\",\n",
    "    \"å¾·é‚¦\",\n",
    "    \"æå…”\",\n",
    "    \"ç”³é€š\",\n",
    "    \"éŸµè¾¾\",\n",
    "    \"é¡ºä¸°\",\n",
    "]\n",
    "COMPANIES_NINE_MAJOR = COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "COMPANIES_ALL_TEN = COMPANIES_FOR_INDUSTRY_COMPARISON + [\"å¿«åŒ…\"]\n",
    "\n",
    "\n",
    "def _find_company_key_from_filename(filename):\n",
    "    name = Path(filename).stem.replace(\"_data_analysis_result\", \"\")\n",
    "    sorted_keys = sorted(COMPANY_MAPPING.keys(), key=len, reverse=True)\n",
    "    for keyword in sorted_keys:\n",
    "        if keyword in name:\n",
    "            return COMPANY_MAPPING[keyword]\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- 1. æ ¸å¿ƒåŠŸèƒ½: ä¸»æŠ¥å‘Šç”Ÿæˆ ---\n",
    "def generate_main_report():\n",
    "    print(\"--- ä»»åŠ¡å¼€å§‹ï¼šç”Ÿæˆæœ€ç»ˆçº¿è·¯æ˜ç»†æŠ¥å‘Š ---\")\n",
    "\n",
    "    base_path = Path.cwd()\n",
    "    report_path = base_path / \"æŠ¥å‘Šæ•°æ®\"\n",
    "    output_path = report_path / \"è¾“å‡º\"\n",
    "    data_analysis_path = output_path / \"data_analysis_result\"\n",
    "    base_data_path = report_path / \"è¾“å…¥\" / \"basic_data.xlsx\"\n",
    "\n",
    "    print(\"\\n[1/4] åŠ¨æ€æ„å»ºçº¿è·¯å…¨é›†å¹¶åˆå¹¶åŸºç¡€ä¿¡æ¯...\")\n",
    "    all_routes = set()\n",
    "    files_in_analysis_result = list(\n",
    "        data_analysis_path.glob(\"*_data_analysis_result.xlsx\")\n",
    "    )\n",
    "    if not files_in_analysis_result:\n",
    "        print(\n",
    "            f\"ğŸ”¥ğŸ”¥ğŸ”¥ é”™è¯¯ï¼šåœ¨è·¯å¾„ '{data_analysis_path.resolve()}' ä¸­æœªæ‰¾åˆ°ä»»ä½• '*_data_analysis_result.xlsx' æ–‡ä»¶ã€‚\"\n",
    "        )\n",
    "        return None\n",
    "    for file_path in files_in_analysis_result:\n",
    "        try:\n",
    "            all_routes.update(\n",
    "                pd.read_excel(\n",
    "                    file_path,\n",
    "                    sheet_name=\"çº¿è·¯æ±‡æ€»æ•°æ®\",\n",
    "                    usecols=[\"è·¯çº¿\"],\n",
    "                    engine=\"openpyxl\",\n",
    "                )[\"è·¯çº¿\"].unique()\n",
    "            )\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\n",
    "                f\" -> è­¦å‘Š: æ–‡ä»¶ {file_path.name} å·²æŸåæˆ–ä¸æ˜¯æœ‰æ•ˆçš„Excelæ–‡ä»¶ï¼Œå·²è·³è¿‡ã€‚\"\n",
    "            )\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\" -> è­¦å‘Š: è¯»å–æ–‡ä»¶ {file_path.name} è·¯çº¿åˆ—è¡¨å¤±è´¥: {e}\")\n",
    "    if not all_routes:\n",
    "        print(\"ğŸ”¥ğŸ”¥ğŸ”¥ é”™è¯¯ï¼šæœªèƒ½ä»ä»»ä½•æ–‡ä»¶ä¸­æ„å»ºçº¿è·¯åˆ—è¡¨ï¼Œæµç¨‹ç»ˆæ­¢ã€‚\")\n",
    "        return None\n",
    "    df_result = pd.DataFrame(list(all_routes), columns=[\"è·¯çº¿\"])\n",
    "    try:\n",
    "        df_base_info = pd.read_excel(\n",
    "            base_data_path, sheet_name=\"inter-city_routes\", engine=\"openpyxl\"\n",
    "        ).rename(columns={\"å…¬é‡Œ\": \"çº¿è·¯é‡Œç¨‹\", \"ç»æµåœˆ\": \"åŸå¸‚åœˆ\"})\n",
    "        cols_to_merge = [\n",
    "            \"å¯„å‡ºçœä»½\",\n",
    "            \"å¯„å‡ºåŸå¸‚\",\n",
    "            \"å¯„è¾¾çœä»½\",\n",
    "            \"å¯„è¾¾åŸå¸‚\",\n",
    "            \"è·¯çº¿\",\n",
    "            \"çº¿è·¯é‡Œç¨‹\",\n",
    "            \"åŸå¸‚åœˆ\",\n",
    "        ]\n",
    "        df_result = pd.merge(\n",
    "            df_result,\n",
    "            df_base_info[[c for c in cols_to_merge if c in df_base_info.columns]],\n",
    "            on=\"è·¯çº¿\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\" -> ğŸ”¥ğŸ”¥ğŸ”¥ é”™è¯¯ï¼šåˆå¹¶åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}\")\n",
    "\n",
    "    print(\"\\n[2/4] ä»'çº¿è·¯æ±‡æ€»æ•°æ®'æå–æ‰€æœ‰é¢„è®¡ç®—æŒ‡æ ‡...\")\n",
    "    metrics_to_extract = {\n",
    "        \"å¿«é€’æ•°é‡\": \"å¿«é€’æ•°é‡\",\n",
    "        \"å…¨ç¨‹æ—¶é™\": \"å…¨ç¨‹æ—¶é™\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é™\": \"è¿è¾“æ—¶é™\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é™\": \"æŠ•é€’æ—¶é™\",\n",
    "        \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\": \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "        \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„å‡ºåœ°åŸå¸‚æ—¶é•¿\": \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„å‡ºåœ°åŸå¸‚æ—¶é•¿\",\n",
    "        \"åˆ°è¾¾å¯„è¾¾åœ°åŸå¸‚-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\": \"åˆ°è¾¾å¯„è¾¾åœ°åŸå¸‚-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "        \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-æ´¾ä»¶\": \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-æ´¾ä»¶\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡\": \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡\": \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"é€è¾¾å¤©æ•°_80åˆ†ä½\": \"é€è¾¾å¤©æ•°\",\n",
    "        \"ä¸­è½¬æ¬¡æ•°\": \"å¹³å‡ä¸­è½¬æ¬¡æ•°\",\n",
    "    }\n",
    "    for file_path in files_in_analysis_result:\n",
    "        company_key = _find_company_key_from_filename(file_path.name)\n",
    "        if not company_key:\n",
    "            continue\n",
    "        try:\n",
    "            df_summary = pd.read_excel(\n",
    "                file_path, sheet_name=\"çº¿è·¯æ±‡æ€»æ•°æ®\", engine=\"openpyxl\"\n",
    "            )\n",
    "            for source_col, target_metric in metrics_to_extract.items():\n",
    "                if source_col in df_summary.columns:\n",
    "                    if source_col == \"ä¸­è½¬æ¬¡æ•°\":\n",
    "                        new_col_name = f\"{company_key}{target_metric}\"\n",
    "                    else:\n",
    "                        new_col_name = f\"{target_metric}{company_key}\"\n",
    "\n",
    "                    df_metric = (\n",
    "                        df_summary[[\"è·¯çº¿\", source_col]]\n",
    "                        .copy()\n",
    "                        .rename(columns={source_col: new_col_name})\n",
    "                    )\n",
    "                    df_result = pd.merge(df_result, df_metric, on=\"è·¯çº¿\", how=\"left\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\n",
    "                f\" -> è­¦å‘Š: æ–‡ä»¶ {file_path.name} å·²æŸåæˆ–ä¸æ˜¯æœ‰æ•ˆçš„Excelæ–‡ä»¶ï¼Œå·²è·³è¿‡ã€‚\"\n",
    "            )\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\" -> é”™è¯¯: å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "    print(\"\\n[3/4] è®¡ç®—å„é¡¹æŒ‡æ ‡çš„ç»Ÿè®¡å€¼...\")\n",
    "    all_transfer_cols = [\n",
    "        f\"{comp}å¹³å‡ä¸­è½¬æ¬¡æ•°\"\n",
    "        for comp in COMPANIES_ALL_TEN\n",
    "        if f\"{comp}å¹³å‡ä¸­è½¬æ¬¡æ•°\" in df_result.columns\n",
    "    ]\n",
    "    if all_transfer_cols:\n",
    "        cols_for_best_turnover = [c for c in all_transfer_cols if \"å¿«åŒ…\" not in c]\n",
    "        if cols_for_best_turnover:\n",
    "            df_result[\"æœ€ä¼˜ä¸­è½¬æ¬¡æ•°\"] = df_result[cols_for_best_turnover].min(axis=1)\n",
    "\n",
    "    stat_metrics = [\n",
    "        \"å¿«é€’æ•°é‡\",\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é™\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é™\",\n",
    "        \"é€è¾¾å¤©æ•°\",\n",
    "    ]\n",
    "    for metric in stat_metrics:\n",
    "        if metric == \"å¿«é€’æ•°é‡\":\n",
    "            total_qty_cols = [\n",
    "                f\"å¿«é€’æ•°é‡{comp}\"\n",
    "                for comp in COMPANIES_ALL_TEN\n",
    "                if f\"å¿«é€’æ•°é‡{comp}\" in df_result.columns\n",
    "            ]\n",
    "            if total_qty_cols:\n",
    "                df_result[f\"å¿«é€’æ•°é‡_total\"] = df_result[total_qty_cols].sum(axis=1)\n",
    "            continue\n",
    "        metric_cols = [\n",
    "            f\"{metric}{comp}\"\n",
    "            for comp in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "            if f\"{metric}{comp}\" in df_result.columns\n",
    "        ]\n",
    "        if not metric_cols:\n",
    "            continue\n",
    "        df_result[f\"{metric}_average\"] = df_result[metric_cols].mean(axis=1)\n",
    "        df_result[f\"{metric}_minimum\"] = df_result[metric_cols].min(axis=1)\n",
    "        df_result[f\"{metric}_maximum\"] = df_result[metric_cols].max(axis=1)\n",
    "\n",
    "    print(\"\\n[4/4] è®¡ç®—æ’å...\")\n",
    "    rank_metrics = [\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é™\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é™\",\n",
    "        \"é€è¾¾å¤©æ•°\",\n",
    "    ]\n",
    "    for param in rank_metrics:\n",
    "        is_desc = \"å‡†æ—¶ç‡\" in param\n",
    "        ems_cols = [\n",
    "            f\"{param}{comp}\"\n",
    "            for comp in COMPANIES_NINE_MAJOR\n",
    "            if f\"{param}{comp}\" in df_result.columns\n",
    "        ]\n",
    "        if ems_cols and f\"{param}EMS\" in df_result.columns:\n",
    "            df_result[f\"{param}_emsæ’å\"] = df_result[ems_cols].rank(\n",
    "                axis=1, method=\"min\", ascending=not is_desc\n",
    "            )[f\"{param}EMS\"]\n",
    "        kb_cols = [\n",
    "            f\"{param}{comp}\"\n",
    "            for comp in COMPANIES_ALL_TEN\n",
    "            if comp != \"EMS\" and f\"{param}{comp}\" in df_result.columns\n",
    "        ]\n",
    "        if kb_cols and f\"{param}å¿«åŒ…\" in df_result.columns:\n",
    "            df_result[f\"{param}_å¿«åŒ…æ’å\"] = df_result[kb_cols].rank(\n",
    "                axis=1, method=\"min\", ascending=not is_desc\n",
    "            )[f\"{param}å¿«åŒ…\"]\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def calculate_weighted_summary(df: pd.DataFrame, group_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"å¯¹ç»™å®šçš„DataFrameæŒ‰æŒ‡å®šåˆ—è¿›è¡Œåˆ†ç»„ï¼Œå¹¶è®¡ç®—åŠ æƒå¹³å‡å€¼ã€‚\"\"\"\n",
    "    print(f\"  - æ­£åœ¨æŒ‰ '{', '.join(group_cols)}' è¿›è¡ŒåŠ æƒå¹³å‡èšåˆ...\")\n",
    "\n",
    "    metrics_to_weight = [\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é™\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é™\",\n",
    "        \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "        \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„å‡ºåœ°åŸå¸‚æ—¶é•¿\",\n",
    "        \"åˆ°è¾¾å¯„è¾¾åœ°åŸå¸‚-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "        \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-æ´¾ä»¶\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"é€è¾¾å¤©æ•°\",\n",
    "        \"å¹³å‡ä¸­è½¬æ¬¡æ•°\",\n",
    "    ]\n",
    "\n",
    "    summary_rows = []\n",
    "    for name, group in df.groupby(group_cols):\n",
    "        row = dict(zip(group_cols, name if isinstance(name, tuple) else [name]))\n",
    "        for comp in COMPANIES_ALL_TEN:\n",
    "            weight_col = f\"å¿«é€’æ•°é‡{comp}\"\n",
    "            if weight_col not in group.columns:\n",
    "                continue\n",
    "            total_weight = group[weight_col].sum()\n",
    "            row[weight_col] = total_weight\n",
    "            if total_weight > 0:\n",
    "                for metric in metrics_to_weight:\n",
    "                    metric_col = f\"{metric}{comp}\"\n",
    "                    if metric == \"å¹³å‡ä¸­è½¬æ¬¡æ•°\":\n",
    "                        metric_col = f\"{comp}{metric}\"\n",
    "                    if metric_col in group.columns:\n",
    "                        metric_values = pd.to_numeric(\n",
    "                            group[metric_col], errors=\"coerce\"\n",
    "                        )\n",
    "                        weight_values = pd.to_numeric(\n",
    "                            group[weight_col], errors=\"coerce\"\n",
    "                        )\n",
    "                        weighted_sum = (metric_values * weight_values).sum()\n",
    "                        row[metric_col] = weighted_sum / total_weight\n",
    "            else:\n",
    "                for metric in metrics_to_weight:\n",
    "                    metric_col = f\"{metric}{comp}\"\n",
    "                    if metric == \"å¹³å‡ä¸­è½¬æ¬¡æ•°\":\n",
    "                        metric_col = f\"{comp}{metric}\"\n",
    "                    if metric_col in group.columns:\n",
    "                        row[metric_col] = np.nan\n",
    "        summary_rows.append(row)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    # é‡æ–°è®¡ç®—è¡Œä¸šç»Ÿè®¡å€¼\n",
    "    for metric in metrics_to_weight:\n",
    "        source_metric_name = metric\n",
    "        metric_cols = [\n",
    "            f\"{source_metric_name}{comp}\"\n",
    "            for comp in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "            if f\"{source_metric_name}{comp}\" in summary_df.columns\n",
    "        ]\n",
    "\n",
    "        if not metric_cols:\n",
    "            continue\n",
    "        summary_df[f\"{source_metric_name}_average\"] = summary_df[metric_cols].mean(\n",
    "            axis=1\n",
    "        )\n",
    "        summary_df[f\"{source_metric_name}_minimum\"] = summary_df[metric_cols].min(\n",
    "            axis=1\n",
    "        )\n",
    "        summary_df[f\"{source_metric_name}_maximum\"] = summary_df[metric_cols].max(\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # é‡æ–°è®¡ç®—æ’å\n",
    "    rank_metrics = [\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é™\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é™\",\n",
    "        \"é€è¾¾å¤©æ•°\",\n",
    "    ]\n",
    "    for param in rank_metrics:\n",
    "        source_param_name = param\n",
    "        is_desc = \"å‡†æ—¶ç‡\" in param\n",
    "        ems_cols = [\n",
    "            f\"{source_param_name}{comp}\"\n",
    "            for comp in COMPANIES_NINE_MAJOR\n",
    "            if f\"{source_param_name}{comp}\" in summary_df.columns\n",
    "        ]\n",
    "        if ems_cols and f\"{source_param_name}EMS\" in summary_df.columns:\n",
    "            summary_df[f\"{source_param_name}_emsæ’å\"] = summary_df[ems_cols].rank(\n",
    "                axis=1, method=\"min\", ascending=not is_desc\n",
    "            )[f\"{source_param_name}EMS\"]\n",
    "        kb_cols = [\n",
    "            f\"{source_param_name}{comp}\"\n",
    "            for comp in COMPANIES_ALL_TEN\n",
    "            if comp != \"EMS\" and f\"{source_param_name}{comp}\" in summary_df.columns\n",
    "        ]\n",
    "        if kb_cols and f\"{source_param_name}å¿«åŒ…\" in summary_df.columns:\n",
    "            summary_df[f\"{source_param_name}_å¿«åŒ…æ’å\"] = summary_df[kb_cols].rank(\n",
    "                axis=1, method=\"min\", ascending=not is_desc\n",
    "            )[f\"{source_param_name}å¿«åŒ…\"]\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_path = Path.cwd()\n",
    "    report_path = base_path / \"æŠ¥å‘Šæ•°æ®\"\n",
    "    output_path = report_path / \"è¾“å‡º\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    final_report_path = output_path / \"åˆ†ææ€»æŠ¥å‘Š.xlsx\"\n",
    "    print(f\"â—â—â— åˆ†ææµç¨‹å¯åŠ¨ â—â—â—\")\n",
    "    df_main_report = generate_main_report()\n",
    "    if df_main_report is None:\n",
    "        print(\"\\nâŒâŒâŒ ç”±äºä¸»æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢ã€‚\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n- æ­£åœ¨æ¸…ç†å®Œå…¨ä¸ºç©ºçš„æŒ‡æ ‡åˆ—...\")\n",
    "    df_main_report.dropna(axis=1, how=\"all\", inplace=True)\n",
    "    print(\"- æ¸…ç†å®Œæˆã€‚\")\n",
    "\n",
    "    print(f\"\\n--- æ­£åœ¨ç”Ÿæˆæ•°æ®ä¸­å°æŠ¥å‘Š: {final_report_path} ---\")\n",
    "    try:\n",
    "        with pd.ExcelWriter(final_report_path, engine=\"xlsxwriter\") as writer:\n",
    "            print(\"  - æ­£åœ¨å†™å…¥'æœ€ç»ˆçº¿è·¯æ˜ç»†ç»“æœ' Sheet...\")\n",
    "\n",
    "            df_main_to_write = df_main_report.copy()\n",
    "            for col in df_main_to_write.columns:\n",
    "                if (\n",
    "                    \"é€è¾¾å¤©æ•°\" in col\n",
    "                    and \"æ’å\" not in col\n",
    "                    and df_main_to_write[col].dtype != \"object\"\n",
    "                ):\n",
    "                    df_main_to_write[col] = pd.to_numeric(\n",
    "                        df_main_to_write[col], errors=\"coerce\"\n",
    "                    ).round()\n",
    "                    df_main_to_write[col] = df_main_to_write[col].apply(\n",
    "                        lambda x: f\"T+{int(x)}\" if pd.notna(x) else x\n",
    "                    )\n",
    "\n",
    "            # åŠ¨æ€ç”Ÿæˆåˆ—é¡ºåº\n",
    "            base_info_cols = [\n",
    "                \"å¯„å‡ºçœä»½\",\n",
    "                \"å¯„å‡ºåŸå¸‚\",\n",
    "                \"å¯„è¾¾çœä»½\",\n",
    "                \"å¯„è¾¾åŸå¸‚\",\n",
    "                \"è·¯çº¿\",\n",
    "                \"çº¿è·¯é‡Œç¨‹\",\n",
    "                \"åŸå¸‚åœˆ\",\n",
    "            ]\n",
    "            metrics_ordered = [\n",
    "                \"å¿«é€’æ•°é‡\",\n",
    "                \"å…¨ç¨‹æ—¶é™\",\n",
    "                \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "                \"è¿è¾“æ—¶é™\",\n",
    "                \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "                \"æŠ•é€’æ—¶é™\",\n",
    "                \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "                \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„å‡ºåœ°åŸå¸‚æ—¶é•¿\",\n",
    "                \"åˆ°è¾¾å¯„è¾¾åœ°åŸå¸‚-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "                \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-æ´¾ä»¶\",\n",
    "                \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "                \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "            ]\n",
    "            all_companies_ordered_cols = []\n",
    "            for comp in COMPANIES_ALL_TEN:\n",
    "                for metric in metrics_ordered:\n",
    "                    all_companies_ordered_cols.append(f\"{metric}{comp}\")\n",
    "\n",
    "            final_cols = base_info_cols + all_companies_ordered_cols\n",
    "            for comp in COMPANIES_ALL_TEN:\n",
    "                final_cols.append(f\"é€è¾¾å¤©æ•°{comp}\")\n",
    "            for comp in COMPANIES_ALL_TEN:\n",
    "                final_cols.append(f\"{comp}å¹³å‡ä¸­è½¬æ¬¡æ•°\")\n",
    "            final_cols.append(\"æœ€ä¼˜ä¸­è½¬æ¬¡æ•°\")\n",
    "\n",
    "            stat_rank_cols = [col for col in df_main_to_write.columns if \"_\" in col]\n",
    "            final_cols.extend(stat_rank_cols)\n",
    "\n",
    "            final_cols_exist = [\n",
    "                col for col in final_cols if col in df_main_to_write.columns\n",
    "            ]\n",
    "            final_cols_exist += [\n",
    "                col for col in df_main_to_write.columns if col not in final_cols_exist\n",
    "            ]\n",
    "\n",
    "            df_main_to_write[final_cols_exist].to_excel(\n",
    "                writer, sheet_name=\"æœ€ç»ˆçº¿è·¯æ˜ç»†ç»“æœ\", index=False\n",
    "            )\n",
    "\n",
    "            # å¯„å‡ºåœ°æ±‡æ€»\n",
    "            origin_group_cols = [\"å¯„å‡ºçœä»½\", \"å¯„å‡ºåŸå¸‚\"]\n",
    "            if all(c in df_main_report.columns for c in origin_group_cols):\n",
    "                df_origin_summary = calculate_weighted_summary(\n",
    "                    df_main_report, origin_group_cols\n",
    "                )\n",
    "                print(\"  - æ­£åœ¨å†™å…¥'å¯„å‡ºåœ°æ±‡æ€»' Sheet...\")\n",
    "                df_origin_summary_to_write = df_origin_summary.copy()\n",
    "                df_origin_summary_to_write.dropna(axis=1, how=\"all\", inplace=True)\n",
    "                for col in df_origin_summary_to_write.columns:\n",
    "                    if (\n",
    "                        \"é€è¾¾å¤©æ•°\" in col\n",
    "                        and \"æ’å\" not in col\n",
    "                        and df_origin_summary_to_write[col].dtype != \"object\"\n",
    "                    ):\n",
    "                        df_origin_summary_to_write[col] = pd.to_numeric(\n",
    "                            df_origin_summary_to_write[col], errors=\"coerce\"\n",
    "                        ).round()\n",
    "                        df_origin_summary_to_write[col] = df_origin_summary_to_write[\n",
    "                            col\n",
    "                        ].apply(lambda x: f\"T+{int(x)}\" if pd.notna(x) else x)\n",
    "                df_origin_summary_to_write.to_excel(\n",
    "                    writer, sheet_name=\"å¯„å‡ºåœ°æ±‡æ€»\", index=False\n",
    "                )\n",
    "\n",
    "            # å¯„è¾¾åœ°æ±‡æ€»\n",
    "            dest_group_cols = [\"å¯„è¾¾çœä»½\", \"å¯„è¾¾åŸå¸‚\"]\n",
    "            if all(c in df_main_report.columns for c in dest_group_cols):\n",
    "                # ã€ä¿®å¤ã€‘: åœ¨mainå‡½æ•°å†…å®šä¹‰metrics_to_weightä»¥è§£å†³ä½œç”¨åŸŸé—®é¢˜\n",
    "                metrics_to_weight = [\n",
    "                    \"å…¨ç¨‹æ—¶é™\",\n",
    "                    \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "                    \"è¿è¾“æ—¶é™\",\n",
    "                    \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "                    \"æŠ•é€’æ—¶é™\",\n",
    "                    \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "                    \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„å‡ºåœ°åŸå¸‚æ—¶é•¿\",\n",
    "                    \"åˆ°è¾¾å¯„è¾¾åœ°åŸå¸‚-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "                    \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-æ´¾ä»¶\",\n",
    "                    \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "                    \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "                    \"é€è¾¾å¤©æ•°\",\n",
    "                    \"å¹³å‡ä¸­è½¬æ¬¡æ•°\",\n",
    "                ]\n",
    "\n",
    "                needed_cols = dest_group_cols.copy()\n",
    "                all_metrics_and_qty = metrics_to_weight + [\"å¿«é€’æ•°é‡\"]\n",
    "                for comp in COMPANIES_ALL_TEN:\n",
    "                    for metric in all_metrics_and_qty:\n",
    "                        if metric == \"å¹³å‡ä¸­è½¬æ¬¡æ•°\":\n",
    "                            col_name = f\"{comp}{metric}\"\n",
    "                        else:\n",
    "                            col_name = f\"{metric}{comp}\"\n",
    "                        if col_name in df_main_report.columns:\n",
    "                            needed_cols.append(col_name)\n",
    "\n",
    "                df_dest_subset = df_main_report[needed_cols]\n",
    "                df_dest_temp = df_dest_subset.rename(\n",
    "                    columns={\"å¯„è¾¾çœä»½\": \"å¯„å‡ºçœä»½\", \"å¯„è¾¾åŸå¸‚\": \"å¯„å‡ºåŸå¸‚\"}\n",
    "                )\n",
    "\n",
    "                df_dest_summary = calculate_weighted_summary(\n",
    "                    df_dest_temp, origin_group_cols\n",
    "                )\n",
    "                df_dest_summary.rename(\n",
    "                    columns={\"å¯„å‡ºçœä»½\": \"å¯„è¾¾çœä»½\", \"å¯„å‡ºåŸå¸‚\": \"å¯„è¾¾åŸå¸‚\"},\n",
    "                    inplace=True,\n",
    "                )\n",
    "                print(\"  - æ­£åœ¨å†™å…¥'å¯„è¾¾åœ°æ±‡æ€»' Sheet...\")\n",
    "                df_dest_summary_to_write = df_dest_summary.copy()\n",
    "                df_dest_summary_to_write.dropna(axis=1, how=\"all\", inplace=True)\n",
    "                for col in df_dest_summary_to_write.columns:\n",
    "                    if (\n",
    "                        \"é€è¾¾å¤©æ•°\" in col\n",
    "                        and \"æ’å\" not in col\n",
    "                        and df_dest_summary_to_write[col].dtype != \"object\"\n",
    "                    ):\n",
    "                        df_dest_summary_to_write[col] = pd.to_numeric(\n",
    "                            df_dest_summary_to_write[col], errors=\"coerce\"\n",
    "                        ).round()\n",
    "                        df_dest_summary_to_write[col] = df_dest_summary_to_write[\n",
    "                            col\n",
    "                        ].apply(lambda x: f\"T+{int(x)}\" if pd.notna(x) else x)\n",
    "                df_dest_summary_to_write.to_excel(\n",
    "                    writer, sheet_name=\"å¯„è¾¾åœ°æ±‡æ€»\", index=False\n",
    "                )\n",
    "\n",
    "        print(\"--- âœ“ æ•°æ®ä¸­å°ç”Ÿæˆå®Œæ¯• (åŒ…å«æ±‡æ€»è¡¨) ---\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> ğŸ”¥ğŸ”¥ğŸ”¥ å†™å…¥æ•°æ®ä¸­å°å¤±è´¥: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "    print(\"\\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ•°æ®ä¸­å°ä»»åŠ¡å·²å…¨éƒ¨æ‰§è¡Œå®Œæ¯•ï¼ğŸ‰ğŸ‰ğŸ‰\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c099ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ä»»åŠ¡å¼€å§‹ï¼šç”Ÿæˆä¸­é€šæœˆæŠ¥ (åŸºäºå·²ç”Ÿæˆçš„ä¸»æŠ¥å‘Š) ---\n",
      "  - âœ“ æˆåŠŸè¯»å–ä¸»æŠ¥å‘Šã€‚\n",
      "  -> å¼€å§‹è®¡ç®—æ‰€æœ‰å…¬å¸çš„è¾¾æˆç‡ (åŸºäºæ—¥å†æ—¥)...\n",
      "    - æ­£åœ¨è®¡ç®— 'å¿«åŒ…' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'äº¬ä¸œ' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'ç”³é€š' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'æå…”' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'åœ†é€š' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'é¡ºä¸°' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'éŸµè¾¾' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'å¾·é‚¦' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'EMS' çš„è¾¾æˆç‡...\n",
      "    - æ­£åœ¨è®¡ç®— 'ä¸­é€š' çš„è¾¾æˆç‡...\n",
      "  -> âœ“ æ‰€æœ‰å…¬å¸è¾¾æˆç‡è®¡ç®—å®Œæ¯•ã€‚\n",
      "  - [1/5] æ­£åœ¨å‡†å¤‡'ä¸­é€šæŠ¥å‘Šæ•°æ®' Sheet...\n",
      "  - [2/5] æ­£åœ¨å‡†å¤‡'çº¿è·¯è¯¦ç»†åˆ†æ' Sheet...\n",
      "  - [3/5] æ­£åœ¨å‡†å¤‡'å¯„å‡ºåŸå¸‚æ±‡æ€»'å’Œ'å¯„è¾¾åŸå¸‚æ±‡æ€»' Sheets...\n",
      "  - [4/5] âœ“ æ±‡æ€»è¡¨å‡†å¤‡å®Œæ¯•ã€‚\n",
      "  - [5/5] æ­£åœ¨å†™å…¥Excelæ–‡ä»¶å¹¶åº”ç”¨æ ¼å¼...\n",
      "--- âœ“ ä¸­é€šæœˆæŠ¥ç”Ÿæˆå®Œæ¯• --- \n",
      "æ–‡ä»¶å·²ä¿å­˜è‡³: /Users/lava/Documents/å›½å®¶é‚®æ”¿å±€å‘å±•ç ”ç©¶ä¸­å¿ƒå®ä¹ /python_data_analysis/æŠ¥å‘Šæ•°æ®/è¾“å‡º/ä¸­é€šæœˆæŠ¥.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 5: ä¸­é€šæœˆæŠ¥ç”Ÿæˆ\n",
    "# ==============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- é‡æ–°å®šä¹‰å¿…è¦çš„å…¨å±€å˜é‡å’Œå‡½æ•° ---\n",
    "base_path = Path.cwd()\n",
    "report_path = base_path / \"æŠ¥å‘Šæ•°æ®\"\n",
    "input_path = report_path / \"è¾“å…¥\"\n",
    "output_path = report_path / \"è¾“å‡º\"\n",
    "base_data_path = input_path / \"basic_data.xlsx\"\n",
    "\n",
    "COMPANY_MAPPING = {\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"å¾·é‚¦\": \"å¾·é‚¦\",\n",
    "    \"æå…”\": \"æå…”\",\n",
    "    \"åœ†é€š\": \"åœ†é€š\",\n",
    "    \"é¡ºä¸°\": \"é¡ºä¸°\",\n",
    "    \"ä¸­é€š\": \"ä¸­é€š\",\n",
    "    \"äº¬ä¸œ\": \"äº¬ä¸œ\",\n",
    "    \"éŸµè¾¾\": \"éŸµè¾¾\",\n",
    "    \"ç”³é€š\": \"ç”³é€š\",\n",
    "    \"é‚®æ”¿\": \"å¿«åŒ…\",\n",
    "    \"å¿«åŒ…\": \"å¿«åŒ…\",\n",
    "}\n",
    "COMPANIES_NINE_MAJOR = [\n",
    "    \"EMS\",\n",
    "    \"ä¸­é€š\",\n",
    "    \"äº¬ä¸œ\",\n",
    "    \"åœ†é€š\",\n",
    "    \"å¾·é‚¦\",\n",
    "    \"æå…”\",\n",
    "    \"ç”³é€š\",\n",
    "    \"éŸµè¾¾\",\n",
    "    \"é¡ºä¸°\",\n",
    "]\n",
    "COMPANIES_TONGDATU = [\"åœ†é€š\", \"ç”³é€š\", \"ä¸­é€š\", \"æå…”\", \"éŸµè¾¾\"]\n",
    "\n",
    "\n",
    "def _find_company_key_from_filename(filename):\n",
    "    name = Path(filename).stem.replace(\"_data_analysis_result\", \"\")\n",
    "    for keyword, key in COMPANY_MAPPING.items():\n",
    "        if keyword in name:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- è®¡ç®—æ‰€æœ‰å…¬å¸è¾¾æˆç‡çš„è¾…åŠ©å‡½æ•° ---\n",
    "def _calculate_all_achievement_rates(df_standard):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ‰€æœ‰å…¬å¸åœ¨æ‰€æœ‰æ ‡å‡†çº¿è·¯ä¸Šçš„è¾¾æˆç‡\n",
    "    åŸºäºâ€œæ½æ”¶æ—¶é—´â€å’Œâ€œç­¾æ”¶æ—¶é—´â€çš„æ—¥å†æ—¥å·®å¼‚è®¡ç®—\n",
    "    T+0 ä¸ºå½“å¤©è¾¾ï¼ŒT+1 ä¸ºæ¬¡æ—¥è¾¾ã€‚\n",
    "    \"\"\"\n",
    "    print(\"  -> å¼€å§‹è®¡ç®—æ‰€æœ‰å…¬å¸çš„è¾¾æˆç‡ (åŸºäºæ—¥å†æ—¥)...\")\n",
    "    data_analysis_path = output_path / \"data_analysis_result\"\n",
    "    df_all_rates = df_standard[[\"è·¯çº¿\"]].copy()\n",
    "\n",
    "    START_DATE_COL = \"æ½æ”¶æ—¶é—´\"\n",
    "    END_DATE_COL = \"ç­¾æ”¶æ—¶é—´\"\n",
    "\n",
    "    for file_path in data_analysis_path.glob(\"*_data_analysis_result.xlsx\"):\n",
    "        company_key = _find_company_key_from_filename(file_path.name)\n",
    "        if not company_key:\n",
    "            continue\n",
    "        print(f\"    - æ­£åœ¨è®¡ç®— '{company_key}' çš„è¾¾æˆç‡...\")\n",
    "        try:\n",
    "            df_detail = pd.read_excel(file_path, sheet_name=\"çº¿è·¯è¯¦ç»†æ•°æ®\")\n",
    "            df_detail[\"è·¯çº¿\"] = df_detail[\"å¯„å‡ºåŸå¸‚\"] + \"-\" + df_detail[\"å¯„è¾¾åŸå¸‚\"]\n",
    "\n",
    "            if (\n",
    "                START_DATE_COL not in df_detail.columns\n",
    "                or END_DATE_COL not in df_detail.columns\n",
    "            ):\n",
    "                print(\n",
    "                    f\"      -> è­¦å‘Šï¼šæ–‡ä»¶ '{file_path.name}' ç¼ºå°‘ '{START_DATE_COL}' æˆ– '{END_DATE_COL}' åˆ—ï¼Œå·²è·³è¿‡ã€‚\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            df_detail[START_DATE_COL] = pd.to_datetime(\n",
    "                df_detail[START_DATE_COL], errors=\"coerce\"\n",
    "            )\n",
    "            df_detail[END_DATE_COL] = pd.to_datetime(\n",
    "                df_detail[END_DATE_COL], errors=\"coerce\"\n",
    "            )\n",
    "            df_detail.dropna(subset=[START_DATE_COL, END_DATE_COL], inplace=True)\n",
    "            if df_detail.empty:\n",
    "                print(\n",
    "                    f\"      -> è­¦å‘Šï¼šæ–‡ä»¶ '{file_path.name}' æ¸…ç†åæ— æœ‰æ•ˆæ—¥æœŸæ•°æ®ï¼Œå·²è·³è¿‡ã€‚\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            df_detail[\"å®é™…å¤©æ•°\"] = (\n",
    "                df_detail[END_DATE_COL].dt.normalize()\n",
    "                - df_detail[START_DATE_COL].dt.normalize()\n",
    "            ).dt.days\n",
    "\n",
    "            for std_type in [\"æ™®æ ‡\", \"é«˜æ ‡\"]:\n",
    "                std_col = f\"{std_type}æ—¶æ•ˆæ ‡å‡†\"\n",
    "                target_col = f\"è¾¾æˆç‡_{company_key}_{std_type}\"\n",
    "                temp_df = pd.merge(\n",
    "                    df_detail,\n",
    "                    df_standard[[\"è·¯çº¿\", f\"{std_col}_days\"]],\n",
    "                    on=\"è·¯çº¿\",\n",
    "                    how=\"inner\",\n",
    "                )\n",
    "                if temp_df.empty:\n",
    "                    continue\n",
    "\n",
    "                temp_df[\"is_met\"] = temp_df[\"å®é™…å¤©æ•°\"] <= temp_df[f\"{std_col}_days\"]\n",
    "                route_rates = temp_df.groupby(\"è·¯çº¿\")[\"is_met\"].mean().reset_index()\n",
    "                route_rates.rename(columns={\"is_met\": target_col}, inplace=True)\n",
    "                df_all_rates = pd.merge(\n",
    "                    df_all_rates, route_rates, on=\"è·¯çº¿\", how=\"left\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"      -> è®¡ç®—'{company_key}'è¾¾æˆç‡æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "    print(\"  -> âœ“ æ‰€æœ‰å…¬å¸è¾¾æˆç‡è®¡ç®—å®Œæ¯•ã€‚\")\n",
    "    return df_all_rates\n",
    "\n",
    "\n",
    "def generate_zto_monthly_report():\n",
    "    main_report_path = output_path / \"åˆ†ææ€»æŠ¥å‘Š.xlsx\"\n",
    "    zto_monthly_report_path = output_path / \"ä¸­é€šæœˆæŠ¥.xlsx\"\n",
    "\n",
    "    if not main_report_path.exists():\n",
    "        print(\n",
    "            f\"ğŸ”¥ğŸ”¥ğŸ”¥ é”™è¯¯ï¼šä¸»æŠ¥å‘Š '{main_report_path.name}' ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆä¸­é€šæŠ¥å‘Šã€‚è¯·å…ˆè¿è¡Œç›¸å…³æ­¥éª¤ã€‚\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- ä»»åŠ¡å¼€å§‹ï¼šç”Ÿæˆä¸­é€šæœˆæŠ¥ (åŸºäºå·²ç”Ÿæˆçš„ä¸»æŠ¥å‘Š) ---\")\n",
    "    try:\n",
    "        df_main_report = pd.read_excel(main_report_path)\n",
    "        print(\"  - âœ“ æˆåŠŸè¯»å–ä¸»æŠ¥å‘Šã€‚\")\n",
    "\n",
    "        # --- Part 1: è®¡ç®—æ‰€æœ‰å…¬å¸çš„è¾¾æˆç‡ ---\n",
    "        df_standard = pd.read_excel(\n",
    "            base_data_path, sheet_name=\"ZTO_standard_time_limit\"\n",
    "        )\n",
    "        if \"çº¿è·¯\" in df_standard.columns:\n",
    "            df_standard.rename(columns={\"çº¿è·¯\": \"è·¯çº¿\"}, inplace=True)\n",
    "        if \"è·¯çº¿\" not in df_main_report.columns:\n",
    "            df_main_report[\"è·¯çº¿\"] = (\n",
    "                df_main_report[\"å¯„å‡ºåŸå¸‚\"] + \"-\" + df_main_report[\"å¯„è¾¾åŸå¸‚\"]\n",
    "            )\n",
    "\n",
    "        for std_type in [\"æ™®æ ‡\", \"é«˜æ ‡\"]:\n",
    "            df_standard[f\"{std_type}æ—¶æ•ˆæ ‡å‡†_days\"] = (\n",
    "                df_standard[f\"{std_type}æ—¶æ•ˆæ ‡å‡†\"]\n",
    "                .str.replace(r\"T\\+\", \"\", regex=True)\n",
    "                .astype(int)\n",
    "            )\n",
    "\n",
    "        df_all_rates = _calculate_all_achievement_rates(df_standard)\n",
    "\n",
    "        # --- Part 2: å‡†å¤‡Sheet 2 (ä¸­é€šæŠ¥å‘Šæ•°æ®) ---\n",
    "        print(\"  - [1/5] æ­£åœ¨å‡†å¤‡'ä¸­é€šæŠ¥å‘Šæ•°æ®' Sheet...\")\n",
    "        df_sheet2 = df_standard.copy()\n",
    "        for std_type in [\"æ™®æ ‡\", \"é«˜æ ‡\"]:\n",
    "            std_col_name = f\"{std_type}æ—¶æ•ˆæ ‡å‡†\"\n",
    "            df_sheet2[f\"ä¸­é€šè¾¾æˆç‡_{std_col_name}\"] = df_all_rates.get(\n",
    "                f\"è¾¾æˆç‡_ä¸­é€š_{std_type}\"\n",
    "            )\n",
    "\n",
    "            tongdatu_cols = [\n",
    "                f\"è¾¾æˆç‡_{c}_{std_type}\"\n",
    "                for c in COMPANIES_TONGDATU\n",
    "                if f\"è¾¾æˆç‡_{c}_{std_type}\" in df_all_rates.columns\n",
    "            ]\n",
    "            if tongdatu_cols:\n",
    "                df_sheet2[f\"é€šè¾¾å…”æœ€ä¼˜_{std_col_name}\"] = df_all_rates[\n",
    "                    tongdatu_cols\n",
    "                ].max(axis=1)\n",
    "            else:\n",
    "                df_sheet2[f\"é€šè¾¾å…”æœ€ä¼˜_{std_col_name}\"] = np.nan\n",
    "\n",
    "            industry_cols = [\n",
    "                f\"è¾¾æˆç‡_{c}_{std_type}\"\n",
    "                for c in COMPANIES_NINE_MAJOR\n",
    "                if f\"è¾¾æˆç‡_{c}_{std_type}\" in df_all_rates.columns\n",
    "            ]\n",
    "            if industry_cols:\n",
    "                df_sheet2[f\"è¡Œä¸šæœ€ä¼˜_{std_col_name}\"] = df_all_rates[industry_cols].max(\n",
    "                    axis=1\n",
    "                )\n",
    "            else:\n",
    "                df_sheet2[f\"è¡Œä¸šæœ€ä¼˜_{std_col_name}\"] = np.nan\n",
    "\n",
    "        df_sheet2.drop(\n",
    "            columns=[c for c in df_sheet2.columns if \"_days\" in c], inplace=True\n",
    "        )\n",
    "\n",
    "        # --- Part 3: å‡†å¤‡Sheet 1 (çº¿è·¯è¯¦ç»†åˆ†æ) ---\n",
    "        print(\"  - [2/5] æ­£åœ¨å‡†å¤‡'çº¿è·¯è¯¦ç»†åˆ†æ' Sheet...\")\n",
    "        additional_info_cols = [\"çº¿è·¯é‡Œç¨‹\", \"åŸå¸‚åœˆ\"]\n",
    "        existing_additional_cols = [\n",
    "            c for c in additional_info_cols if c in df_main_report.columns\n",
    "        ]\n",
    "        zto_metric_cols = [c for c in df_main_report.columns if \"ä¸­é€š\" in c]\n",
    "        cols_to_extract = list(\n",
    "            dict.fromkeys([\"è·¯çº¿\"] + existing_additional_cols + zto_metric_cols)\n",
    "        )\n",
    "        df_zto_extra_data = df_main_report[cols_to_extract]\n",
    "        df_sheet1 = pd.merge(df_sheet2.copy(), df_zto_extra_data, on=\"è·¯çº¿\", how=\"left\")\n",
    "\n",
    "        # --- Part 4: å‡†å¤‡æ±‡æ€»Sheet (å¯„å‡º/å¯„è¾¾åŸå¸‚æ±‡æ€») ---\n",
    "        print(\"  - [3/5] æ­£åœ¨å‡†å¤‡'å¯„å‡ºåŸå¸‚æ±‡æ€»'å’Œ'å¯„è¾¾åŸå¸‚æ±‡æ€»' Sheets...\")\n",
    "\n",
    "        source_cols_map = {\n",
    "            # ä¸­é€šæ—¶æ•ˆ\n",
    "            \"å…¨ç¨‹æ—¶é™ä¸­é€š\": \"å…¨ç¨‹æ—¶é™\",\n",
    "            \"å¯„å‡ºåœ°å¤„ç†æ—¶é™ä¸­é€š\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "            \"è¿è¾“æ—¶é™ä¸­é€š\": \"è¿è¾“æ—¶é™\",\n",
    "            \"å¯„è¾¾åœ°å¤„ç†æ—¶é™ä¸­é€š\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "            \"æŠ•é€’æ—¶é™ä¸­é€š\": \"æŠ•é€’æ—¶é™\",\n",
    "            # è¡Œä¸šæœ€ä¼˜ (minimum)\n",
    "            \"å…¨ç¨‹æ—¶é™_minimum\": \"å…¨ç¨‹æ—¶é™è¡Œä¸šæœ€ä¼˜\",\n",
    "            \"å¯„å‡ºåœ°å¤„ç†æ—¶é™_minimum\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é™è¡Œä¸šæœ€ä¼˜\",\n",
    "            \"è¿è¾“æ—¶é™_minimum\": \"è¿è¾“æ—¶é™è¡Œä¸šæœ€ä¼˜\",\n",
    "            \"å¯„è¾¾åœ°å¤„ç†æ—¶é™_minimum\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é™è¡Œä¸šæœ€ä¼˜\",\n",
    "            \"æŠ•é€’æ—¶é™_minimum\": \"æŠ•é€’æ—¶é™è¡Œä¸šæœ€ä¼˜\",\n",
    "            # è¡Œä¸šå‡å€¼ (average)\n",
    "            \"å…¨ç¨‹æ—¶é™_average\": \"å…¨ç¨‹æ—¶é™è¡Œä¸šå‡å€¼\",\n",
    "            \"å¯„å‡ºåœ°å¤„ç†æ—¶é™_average\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é™è¡Œä¸šå‡å€¼\",\n",
    "            \"è¿è¾“æ—¶é™_average\": \"è¿è¾“æ—¶é™è¡Œä¸šå‡å€¼\",\n",
    "            \"å¯„è¾¾åœ°å¤„ç†æ—¶é™_average\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é™è¡Œä¸šå‡å€¼\",\n",
    "            \"æŠ•é€’æ—¶é™_average\": \"æŠ•é€’æ—¶é™è¡Œä¸šå‡å€¼\",\n",
    "        }\n",
    "\n",
    "        # æå–éœ€è¦èšåˆçš„æºåˆ—å\n",
    "        agg_source_cols = list(source_cols_map.keys())\n",
    "\n",
    "        # å‡†å¤‡Sheet 3: å¯„å‡ºåŸå¸‚æ±‡æ€»\n",
    "        origin_group_cols = [\"å¯„å‡ºçœä»½\", \"å¯„å‡ºåŸå¸‚\"]\n",
    "        cols_for_origin_summary = origin_group_cols + agg_source_cols\n",
    "        # æ£€æŸ¥æ‰€éœ€åˆ—æ˜¯å¦å­˜åœ¨\n",
    "        if all(c in df_main_report.columns for c in cols_for_origin_summary):\n",
    "            df_sheet3_origin = df_main_report.groupby(\n",
    "                origin_group_cols, as_index=False\n",
    "            )[agg_source_cols].mean()\n",
    "            df_sheet3_origin.rename(columns=source_cols_map, inplace=True)\n",
    "        else:\n",
    "            print(\"  -> è­¦å‘Šï¼šä¸»æŠ¥å‘Šä¸­ç¼ºå°‘'å¯„å‡ºåŸå¸‚æ±‡æ€»'æ‰€éœ€çš„åˆ—ï¼Œå°†ç”Ÿæˆç©ºè¡¨ã€‚\")\n",
    "            missing_cols = [\n",
    "                c for c in cols_for_origin_summary if c not in df_main_report.columns\n",
    "            ]\n",
    "            print(f\"     ç¼ºå°‘çš„æºåˆ—: {missing_cols}\")\n",
    "            final_cols = origin_group_cols + list(source_cols_map.values())\n",
    "            df_sheet3_origin = pd.DataFrame(columns=final_cols)\n",
    "\n",
    "        # å‡†å¤‡Sheet 4: å¯„è¾¾åŸå¸‚æ±‡æ€»\n",
    "        dest_group_cols = [\"å¯„è¾¾çœä»½\", \"å¯„è¾¾åŸå¸‚\"]\n",
    "        cols_for_dest_summary = dest_group_cols + agg_source_cols\n",
    "        # æ£€æŸ¥æ‰€éœ€åˆ—æ˜¯å¦å­˜åœ¨\n",
    "        if all(c in df_main_report.columns for c in cols_for_dest_summary):\n",
    "            df_sheet4_dest = df_main_report.groupby(dest_group_cols, as_index=False)[\n",
    "                agg_source_cols\n",
    "            ].mean()\n",
    "            df_sheet4_dest.rename(columns=source_cols_map, inplace=True)\n",
    "        else:\n",
    "            print(\"  -> è­¦å‘Šï¼šä¸»æŠ¥å‘Šä¸­ç¼ºå°‘'å¯„è¾¾åŸå¸‚æ±‡æ€»'æ‰€éœ€çš„åˆ—ï¼Œå°†ç”Ÿæˆç©ºè¡¨ã€‚\")\n",
    "            missing_cols = [\n",
    "                c for c in cols_for_dest_summary if c not in df_main_report.columns\n",
    "            ]\n",
    "            print(f\"     ç¼ºå°‘çš„æºåˆ—: {missing_cols}\")\n",
    "            final_cols = dest_group_cols + list(source_cols_map.values())\n",
    "            df_sheet4_dest = pd.DataFrame(columns=final_cols)\n",
    "\n",
    "        print(\"  - [4/5] âœ“ æ±‡æ€»è¡¨å‡†å¤‡å®Œæ¯•ã€‚\")\n",
    "\n",
    "        # --- Part 5: å†™å…¥Excel ---\n",
    "        print(\"  - [5/5] æ­£åœ¨å†™å…¥Excelæ–‡ä»¶å¹¶åº”ç”¨æ ¼å¼...\")\n",
    "        with pd.ExcelWriter(zto_monthly_report_path, engine=\"xlsxwriter\") as writer:\n",
    "            # å†™å…¥Sheet 1\n",
    "            df_sheet1.to_excel(writer, sheet_name=\"çº¿è·¯è¯¦ç»†åˆ†æ\", index=False)\n",
    "            # å†™å…¥Sheet 2\n",
    "            df_sheet2.to_excel(writer, sheet_name=\"ä¸­é€šæŠ¥å‘Šæ•°æ®\", index=False)\n",
    "            # å†™å…¥Sheet 3 å’Œ Sheet 4\n",
    "            df_sheet3_origin.to_excel(writer, sheet_name=\"å¯„å‡ºåŸå¸‚æ±‡æ€»\", index=False)\n",
    "            df_sheet4_dest.to_excel(writer, sheet_name=\"å¯„è¾¾åŸå¸‚æ±‡æ€»\", index=False)\n",
    "\n",
    "            # ---- åº”ç”¨æ ¼å¼ ----\n",
    "            workbook = writer.book\n",
    "            worksheet2 = writer.sheets[\"ä¸­é€šæŠ¥å‘Šæ•°æ®\"]\n",
    "            percent_format = workbook.add_format({\"num_format\": \"0.00%\"})\n",
    "            cols_to_format = [\n",
    "                \"ä¸­é€šè¾¾æˆç‡_æ™®æ ‡æ—¶æ•ˆæ ‡å‡†\",\n",
    "                \"é€šè¾¾å…”æœ€ä¼˜_æ™®æ ‡æ—¶æ•ˆæ ‡å‡†\",\n",
    "                \"è¡Œä¸šæœ€ä¼˜_æ™®æ ‡æ—¶æ•ˆæ ‡å‡†\",\n",
    "                \"ä¸­é€šè¾¾æˆç‡_é«˜æ ‡æ—¶æ•ˆæ ‡å‡†\",\n",
    "                \"é€šè¾¾å…”æœ€ä¼˜_é«˜æ ‡æ—¶æ•ˆæ ‡å‡†\",\n",
    "                \"è¡Œä¸šæœ€ä¼˜_é«˜æ ‡æ—¶æ•ˆæ ‡å‡†\",\n",
    "            ]\n",
    "\n",
    "            # ä¸ºSheet2çš„è¾¾æˆç‡åˆ—åº”ç”¨æ ¼å¼\n",
    "            for col_name in cols_to_format:\n",
    "                if col_name in df_sheet2.columns:\n",
    "                    col_idx = df_sheet2.columns.get_loc(col_name)\n",
    "                    worksheet2.set_column(col_idx, col_idx, 18, percent_format)\n",
    "\n",
    "        print(f\"--- âœ“ ä¸­é€šæœˆæŠ¥ç”Ÿæˆå®Œæ¯• --- \\næ–‡ä»¶å·²ä¿å­˜è‡³: {zto_monthly_report_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "\n",
    "        print(f\"    -> ğŸ”¥ğŸ”¥ğŸ”¥ ç”Ÿæˆä¸­é€šæœˆæŠ¥å¤±è´¥: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "# --- æ‰§è¡Œä¸­é€šæŠ¥å‘Šç”Ÿæˆå‡½æ•° ---\n",
    "# åœ¨å®é™…è¿è¡Œå‰ï¼Œè¯·ç¡®ä¿Cell 1ä¸­çš„è·¯å¾„ç­‰å˜é‡å·²æ­£ç¡®è®¾ç½®\n",
    "generate_zto_monthly_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf8235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== å¼€å§‹ç”Ÿæˆ: é‚®æ”¿æœˆæŠ¥.xlsx ====================\n",
      "  - æ­£åœ¨ç‹¬ç«‹åŠ è½½æ‰€æœ‰å…¬å¸çš„'çº¿è·¯æ±‡æ€»æ•°æ®'...\n",
      "  - æ­£åœ¨è®¡ç®—'æœ€ä¼˜ä¸­è½¬æ¬¡æ•°'...\n",
      "âœ… æˆåŠŸä» '30_top_volume_city_2024' åŠ è½½ 30 ä¸ªåŸå¸‚ã€‚\n",
      "  - [1/4] æ­£åœ¨å‡†å¤‡ 'çº¿è·¯æ˜ç»†' Sheet for EMS, å¿«åŒ…...\n",
      "  - [2/4] æ­£åœ¨å‡†å¤‡ 'é‚®ä»¶æ˜ç»†' Sheet for EMS, å¿«åŒ…...\n",
      "      -> [å…¼å®¹æ¨¡å¼] æ£€æµ‹åˆ°'å®Œæ•´ç‰©æµä¿¡æ¯'åˆ—åå†²çªï¼Œæ™ºèƒ½åˆå¹¶ä¸­...\n",
      "  - [3/4] æ­£åœ¨å‡†å¤‡ 'åˆ†cityæ˜ç»†' (æ¢å¤åŸå§‹è®¡ç®—æ¡†æ¶ + åŠ æƒå¹³å‡)...\n",
      "  - [4/4] æ­£åœ¨å‡†å¤‡ 'åˆ†provinceæ˜ç»†' (æ¢å¤åŸå§‹è®¡ç®—æ¡†æ¶ + åŠ æƒå¹³å‡)...\n",
      "\n",
      "--- æ‰€æœ‰æ•°æ®è®¡ç®—å®Œæˆï¼Œæ­£åœ¨å†™å…¥æœ€ç»ˆæ–‡ä»¶: é‚®æ”¿æœˆæŠ¥.xlsx ---\n",
      "  - æ­£åœ¨å†™å…¥Sheet: çº¿è·¯æ˜ç»†...\n",
      "  - æ­£åœ¨å†™å…¥Sheet: é‚®ä»¶æ˜ç»†...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/7fywcbxd7t52ftqjt6sy8cs40000gn/T/ipykernel_71567/1049195596.py:1024: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - æ­£åœ¨å†™å…¥Sheet: åˆ†åŸå¸‚æ˜ç»†...\n",
      "  - æ­£åœ¨å†™å…¥Sheet: åˆ†çœä»½æ˜ç»†...\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼å·²æˆåŠŸç”ŸæˆæŠ¥å‘Š 'é‚®æ”¿æœˆæŠ¥.xlsx'ï¼ğŸ‰ğŸ‰ğŸ‰\n",
      "\n",
      "==================== å¼€å§‹ç”Ÿæˆ: æå…”æœˆæŠ¥.xlsx ====================\n",
      "  - æ­£åœ¨ç‹¬ç«‹åŠ è½½æ‰€æœ‰å…¬å¸çš„'çº¿è·¯æ±‡æ€»æ•°æ®'...\n",
      "  - æ­£åœ¨è®¡ç®—'æœ€ä¼˜ä¸­è½¬æ¬¡æ•°'...\n",
      "âœ… æˆåŠŸä» '30_top_volume_city_2024' åŠ è½½ 30 ä¸ªåŸå¸‚ã€‚\n",
      "  - [1/4] æ­£åœ¨å‡†å¤‡ 'çº¿è·¯æ˜ç»†' Sheet for æå…”...\n",
      "  - [2/4] æ­£åœ¨å‡†å¤‡ 'é‚®ä»¶æ˜ç»†' Sheet for æå…”...\n",
      "      -> [å…¼å®¹æ¨¡å¼] æ£€æµ‹åˆ°'å®Œæ•´ç‰©æµä¿¡æ¯'åˆ—åå†²çªï¼Œæ™ºèƒ½åˆå¹¶ä¸­...\n",
      "  - [3/4] æ­£åœ¨å‡†å¤‡ 'åˆ†cityæ˜ç»†' (æ¢å¤åŸå§‹è®¡ç®—æ¡†æ¶ + åŠ æƒå¹³å‡)...\n",
      "  - [4/4] æ­£åœ¨å‡†å¤‡ 'åˆ†provinceæ˜ç»†' (æ¢å¤åŸå§‹è®¡ç®—æ¡†æ¶ + åŠ æƒå¹³å‡)...\n",
      "\n",
      "--- æ‰€æœ‰æ•°æ®è®¡ç®—å®Œæˆï¼Œæ­£åœ¨å†™å…¥æœ€ç»ˆæ–‡ä»¶: æå…”æœˆæŠ¥.xlsx ---\n",
      "  - æ­£åœ¨å†™å…¥Sheet: çº¿è·¯æ˜ç»†...\n",
      "  - æ­£åœ¨å†™å…¥Sheet: é‚®ä»¶æ˜ç»†...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/7fywcbxd7t52ftqjt6sy8cs40000gn/T/ipykernel_71567/1049195596.py:1024: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - æ­£åœ¨å†™å…¥Sheet: åˆ†åŸå¸‚æ˜ç»†...\n",
      "  - æ­£åœ¨å†™å…¥Sheet: åˆ†çœä»½æ˜ç»†...\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼å·²æˆåŠŸç”ŸæˆæŠ¥å‘Š 'æå…”æœˆæŠ¥.xlsx'ï¼ğŸ‰ğŸ‰ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 6: é‚®æ”¿ã€æå…”æœˆæŠ¥ç”Ÿæˆ (æœ€ç»ˆæ­£ç¡®ç‰ˆ - ç»Ÿä¸€å£å¾„)\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import traceback\n",
    "import zipfile\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. å…¨å±€é…ç½®ä¸è¾…åŠ©å‡½æ•° ---\n",
    "COMPANIES_NINE_MAJOR = [\n",
    "    \"EMS\",\n",
    "    \"ä¸­é€š\",\n",
    "    \"äº¬ä¸œ\",\n",
    "    \"åœ†é€š\",\n",
    "    \"å¾·é‚¦\",\n",
    "    \"æå…”\",\n",
    "    \"ç”³é€š\",\n",
    "    \"éŸµè¾¾\",\n",
    "    \"é¡ºä¸°\",\n",
    "]\n",
    "COMPANIES_EIGHT_OTHERS = [\n",
    "    \"ä¸­é€š\",\n",
    "    \"äº¬ä¸œ\",\n",
    "    \"åœ†é€š\",\n",
    "    \"å¾·é‚¦\",\n",
    "    \"æå…”\",\n",
    "    \"ç”³é€š\",\n",
    "    \"éŸµè¾¾\",\n",
    "    \"é¡ºä¸°\",\n",
    "]\n",
    "COMPANIES_FOR_INDUSTRY_COMPARISON = COMPANIES_NINE_MAJOR\n",
    "COMPANIES_ALL_TEN = COMPANIES_FOR_INDUSTRY_COMPARISON + [\"å¿«åŒ…\"]\n",
    "\n",
    "REPORT_CONFIG = {\n",
    "    \"é‚®æ”¿\": {\n",
    "        \"products\": [\"EMS\", \"å¿«åŒ…\"],\n",
    "        \"output_filename\": \"é‚®æ”¿æœˆæŠ¥.xlsx\",\n",
    "        \"product_to_filename\": {\"EMS\": \"EMS\", \"å¿«åŒ…\": \"é‚®æ”¿\"},\n",
    "    },\n",
    "    \"æå…”\": {\n",
    "        \"products\": [\"æå…”\"],\n",
    "        \"output_filename\": \"æå…”æœˆæŠ¥.xlsx\",\n",
    "        \"product_to_filename\": {\"æå…”\": \"æå…”\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "FILENAME_TO_COMPANY_MAP = {\n",
    "    v: k\n",
    "    for k, v in {\n",
    "        \"EMS\": \"EMS\",\n",
    "        \"ä¸­é€š\": \"ä¸­é€š\",\n",
    "        \"äº¬ä¸œ\": \"äº¬ä¸œ\",\n",
    "        \"åœ†é€š\": \"åœ†é€š\",\n",
    "        \"å¾·é‚¦\": \"å¾·é‚¦\",\n",
    "        \"æå…”\": \"æå…”\",\n",
    "        \"ç”³é€š\": \"ç”³é€š\",\n",
    "        \"éŸµè¾¾\": \"éŸµè¾¾\",\n",
    "        \"é¡ºä¸°\": \"é¡ºä¸°\",\n",
    "        \"å¿«åŒ…\": \"é‚®æ”¿\",\n",
    "    }.items()\n",
    "}\n",
    "\n",
    "\n",
    "def _find_company_key_from_filename(filename):\n",
    "    name = (\n",
    "        Path(filename)\n",
    "        .stem.replace(\"_data_analysis_result\", \"\")\n",
    "        .replace(\"_transit_data\", \"\")\n",
    "    )\n",
    "    filename_to_company = {v: k for k, v in FILENAME_TO_COMPANY_MAP.items()}\n",
    "    filename_to_company.update(\n",
    "        {\n",
    "            \"EMS\": \"EMS\",\n",
    "            \"å¾·é‚¦\": \"å¾·é‚¦\",\n",
    "            \"æå…”\": \"æå…”\",\n",
    "            \"åœ†é€š\": \"åœ†é€š\",\n",
    "            \"é¡ºä¸°\": \"é¡ºä¸°\",\n",
    "            \"ä¸­é€š\": \"ä¸­é€š\",\n",
    "            \"äº¬ä¸œ\": \"äº¬ä¸œ\",\n",
    "            \"éŸµè¾¾\": \"éŸµè¾¾\",\n",
    "            \"ç”³é€š\": \"ç”³é€š\",\n",
    "            \"é‚®æ”¿\": \"å¿«åŒ…\",\n",
    "        }\n",
    "    )\n",
    "    sorted_keys = sorted(filename_to_company.keys(), key=len, reverse=True)\n",
    "    for keyword in sorted_keys:\n",
    "        if keyword in name:\n",
    "            return filename_to_company[keyword]\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_top_cities(file_path: Path, sheet_name: str) -> set:\n",
    "    try:\n",
    "        if not file_path.exists():\n",
    "            print(f\"âŒ é”™è¯¯ï¼šåŸºç¡€æ•°æ®æ–‡ä»¶ 'basic_data.xlsx' æœªæ‰¾åˆ°: {file_path}\")\n",
    "            return set()\n",
    "        df_cities = pd.read_excel(file_path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "        if \"åŸå¸‚\" not in df_cities.columns:\n",
    "            return set()\n",
    "        city_list = df_cities[\"åŸå¸‚\"].dropna().astype(str).str.strip().unique().tolist()\n",
    "        print(f\"âœ… æˆåŠŸä» '{sheet_name}' åŠ è½½ {len(city_list)} ä¸ªåŸå¸‚ã€‚\")\n",
    "        return set(city_list)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šåŠ è½½åŸå¸‚åˆ—è¡¨ '{sheet_name}' å¤±è´¥: {e}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def auto_adjust_xlsx_columns(writer, df, sheet_name):\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    for i, col_name in enumerate(df.columns):\n",
    "        column_data = df[col_name]\n",
    "        if isinstance(column_data, pd.DataFrame):\n",
    "            column_data = column_data.iloc[:, 0]\n",
    "        if col_name == \"å®Œæ•´ç‰©æµä¿¡æ¯\":\n",
    "            width = 50\n",
    "        else:\n",
    "            column_len = (\n",
    "                column_data.astype(str).str.len().max() if not column_data.empty else 0\n",
    "            )\n",
    "            width = min(max(column_len, len(str(col_name)), 8) + 2, 40)\n",
    "        worksheet.set_column(i, i, width)\n",
    "\n",
    "\n",
    "# --- 2. å„Sheetçš„ç”Ÿæˆå‡½æ•° ---\n",
    "def create_sheet1_route_details(df_main, products: list):\n",
    "    \"\"\"ã€æ¢å¤ã€‘æ­¤å‡½æ•°å·²å®Œå…¨æ¢å¤åˆ°æ‚¨çš„åŸå§‹ç‰ˆæœ¬\"\"\"\n",
    "    print(f\"  - [1/4] æ­£åœ¨å‡†å¤‡ 'çº¿è·¯æ˜ç»†' Sheet for {', '.join(products)}...\")\n",
    "    column_mapping = {\n",
    "        \"å¯„å‡ºçœä»½\": \"å¯„å‡ºçœä»½\",\n",
    "        \"å¯„å‡ºåŸå¸‚\": \"å¯„å‡ºåŸå¸‚\",\n",
    "        \"å¯„è¾¾çœä»½\": \"å¯„è¾¾çœä»½\",\n",
    "        \"å¯„è¾¾åŸå¸‚\": \"å¯„è¾¾åŸå¸‚\",\n",
    "        \"çº¿è·¯\": \"è·¯çº¿\",\n",
    "        \"çº¿è·¯é‡Œç¨‹\": \"çº¿è·¯é‡Œç¨‹\",\n",
    "        \"åŸå¸‚åœˆ\": \"åŸå¸‚åœˆ\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_ä¸­è½¬æ¬¡æ•°\": \"æœ€ä¼˜ä¸­è½¬æ¬¡æ•°\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_48å°æ—¶å¦¥æŠ•ç‡\": \"48å°æ—¶å‡†æ—¶ç‡_maximum\",\n",
    "        \"è¡Œä¸šå‡å€¼_48å°æ—¶å¦¥æŠ•ç‡\": \"48å°æ—¶å‡†æ—¶ç‡_average\",\n",
    "        \"è¡Œä¸šæœ€å·®_48å°æ—¶å¦¥æŠ•ç‡\": \"48å°æ—¶å‡†æ—¶ç‡_minimum\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_72å°æ—¶å¦¥æŠ•ç‡\": \"72å°æ—¶å‡†æ—¶ç‡_maximum\",\n",
    "        \"è¡Œä¸šå‡å€¼_72å°æ—¶å¦¥æŠ•ç‡\": \"72å°æ—¶å‡†æ—¶ç‡_average\",\n",
    "        \"è¡Œä¸šæœ€å·®_72å°æ—¶å¦¥æŠ•ç‡\": \"72å°æ—¶å‡†æ—¶ç‡_minimum\",\n",
    "        \"è¡Œä¸šæœ€å·®_å…¨ç¨‹æ—¶é™\": \"å…¨ç¨‹æ—¶é™_maximum\",\n",
    "        \"è¡Œä¸šå‡å€¼_å…¨ç¨‹æ—¶é™\": \"å…¨ç¨‹æ—¶é™_average\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_å…¨ç¨‹æ—¶é™\": \"å…¨ç¨‹æ—¶é™_minimum\",\n",
    "        \"è¡Œä¸šæœ€å·®_T+N\": \"é€è¾¾å¤©æ•°_maximum\",\n",
    "        \"è¡Œä¸šå‡å€¼_T+N\": \"é€è¾¾å¤©æ•°_average\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_T+N\": \"é€è¾¾å¤©æ•°_minimum\",\n",
    "        \"è¡Œä¸šæœ€å·®_å¯„å‡ºåœ°å¤„ç†æ—¶é•¿\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é™_maximum\",\n",
    "        \"è¡Œä¸šå‡å€¼_å¯„å‡ºåœ°å¤„ç†æ—¶é•¿\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é™_average\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_å¯„å‡ºåœ°å¤„ç†æ—¶é•¿\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é™_minimum\",\n",
    "        \"è¡Œä¸šæœ€å·®_è¿è¾“æ—¶é•¿\": \"è¿è¾“æ—¶é™_maximum\",\n",
    "        \"è¡Œä¸šå‡å€¼_è¿è¾“æ—¶é•¿\": \"è¿è¾“æ—¶é™_average\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_è¿è¾“æ—¶é•¿\": \"è¿è¾“æ—¶é™_minimum\",\n",
    "        \"è¡Œä¸šæœ€å·®_å¯„è¾¾åœ°å¤„ç†æ—¶é•¿\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é™_maximum\",\n",
    "        \"è¡Œä¸šå‡å€¼_å¯„è¾¾åœ°å¤„ç†æ—¶é•¿\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é™_average\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_å¯„è¾¾åœ°å¤„ç†æ—¶é•¿\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é™_minimum\",\n",
    "        \"è¡Œä¸šæœ€å·®_æŠ•é€’æ—¶é•¿\": \"æŠ•é€’æ—¶é™_maximum\",\n",
    "        \"è¡Œä¸šå‡å€¼_æŠ•é€’æ—¶é•¿\": \"æŠ•é€’æ—¶é™_average\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_æŠ•é€’æ—¶é•¿\": \"æŠ•é€’æ—¶é™_minimum\",\n",
    "    }\n",
    "    source_metric_map = {\n",
    "        \"ä¸­è½¬æ¬¡æ•°\": \"å¹³å‡ä¸­è½¬æ¬¡æ•°\",\n",
    "        \"48å°æ—¶å¦¥æŠ•ç‡\": \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"72å°æ—¶å¦¥æŠ•ç‡\": \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "        \"T+N\": \"é€è¾¾å¤©æ•°\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é•¿\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é•¿\": \"è¿è¾“æ—¶é™\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é•¿\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é•¿\": \"æŠ•é€’æ—¶é™\",\n",
    "        \"å…¨ç¨‹æ—¶é™\": \"å…¨ç¨‹æ—¶é™\",\n",
    "    }\n",
    "    for prod in products:\n",
    "        for metric, source_metric_name in source_metric_map.items():\n",
    "            rank_source_suffix = f\"_{prod.lower()}æ’å\"\n",
    "            if prod == \"EMS\":\n",
    "                rank_source_suffix = \"_emsæ’å\"\n",
    "            elif prod == \"å¿«åŒ…\":\n",
    "                rank_source_suffix = \"_å¿«åŒ…æ’å\"\n",
    "            if metric == \"ä¸­è½¬æ¬¡æ•°\":\n",
    "                column_mapping[f\"{prod}_{metric}\"] = f\"{prod}{source_metric_name}\"\n",
    "            else:\n",
    "                column_mapping[f\"{prod}_{metric}\"] = f\"{source_metric_name}{prod}\"\n",
    "                column_mapping[f\"{prod}æ’å_{metric}\"] = (\n",
    "                    f\"{source_metric_name}{rank_source_suffix}\"\n",
    "                )\n",
    "    df_sheet1 = pd.DataFrame()\n",
    "    for new_col, source_col in column_mapping.items():\n",
    "        if source_col in df_main.columns:\n",
    "            df_sheet1[new_col] = df_main[source_col]\n",
    "        else:\n",
    "            df_sheet1[new_col] = pd.NA\n",
    "    filter_metrics = [\n",
    "        \"è¿è¾“æ—¶é•¿\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é•¿\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é•¿\",\n",
    "        \"æŠ•é€’æ—¶é•¿\",\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "    ]\n",
    "    for metric in filter_metrics:\n",
    "        for prod in products:\n",
    "            prod_metric_col = f\"{prod}_{metric}\"\n",
    "            compare_col = \"\"\n",
    "            if prod in [\"EMS\", \"æå…”\"]:\n",
    "                compare_col = f\"è¡Œä¸šæœ€ä¼˜_{metric}\"\n",
    "            elif prod == \"å¿«åŒ…\":\n",
    "                compare_col = f\"è¡Œä¸šå‡å€¼_{metric}\"\n",
    "            else:\n",
    "                continue\n",
    "            filter_col_name = f\"{prod}ç­›é€‰æŒ‡æ ‡_{metric}\"\n",
    "            if (\n",
    "                prod_metric_col in df_sheet1.columns\n",
    "                and compare_col in df_sheet1.columns\n",
    "            ):\n",
    "                prod_values = pd.to_numeric(df_sheet1[prod_metric_col], errors=\"coerce\")\n",
    "                compare_values = pd.to_numeric(df_sheet1[compare_col], errors=\"coerce\")\n",
    "                df_sheet1[filter_col_name] = prod_values - compare_values\n",
    "            else:\n",
    "                df_sheet1[filter_col_name] = pd.NA\n",
    "    original_cols = list(column_mapping.keys())\n",
    "    new_filter_cols = [col for col in df_sheet1.columns if \"ç­›é€‰æŒ‡æ ‡\" in col]\n",
    "    final_cols_order = original_cols + new_filter_cols\n",
    "    existing_cols = [col for col in final_cols_order if col in df_sheet1.columns]\n",
    "    return df_sheet1[existing_cols]\n",
    "\n",
    "\n",
    "def create_sheet2_mail_details(\n",
    "    data_analysis_path,\n",
    "    zhuzhuyun_merge_path,\n",
    "    products: list,\n",
    "    product_to_filename: dict,\n",
    "    top_30_cities: set,\n",
    "    df_main,\n",
    "):\n",
    "    print(f\"  - [2/4] æ­£åœ¨å‡†å¤‡ 'é‚®ä»¶æ˜ç»†' Sheet for {', '.join(products)}...\")\n",
    "    # æ­¥éª¤1ï¼šåŠ è½½æºæ•°æ® (æ­¤æ—¶ df_s2 å¯èƒ½æœ‰ï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰â€œå®Œæ•´ç‰©æµä¿¡æ¯â€åˆ—)\n",
    "    dfs = []\n",
    "    for prod_name, filename_part in product_to_filename.items():\n",
    "        file_path = data_analysis_path / f\"{filename_part}_data_analysis_result.xlsx\"\n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                df = pd.read_excel(\n",
    "                    file_path,\n",
    "                    sheet_name=\"çº¿è·¯è¯¦ç»†æ•°æ®\",\n",
    "                    dtype={\"å•å·\": str},\n",
    "                    engine=\"openpyxl\",\n",
    "                )\n",
    "                if not df.empty:\n",
    "                    df[\"å•å·\"] = df[\"å•å·\"].astype(str).str.strip()\n",
    "                    df[\"äº§å“ç§ç±»\"] = prod_name\n",
    "                    dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"      -> è­¦å‘Š: è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}\")\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_s2 = pd.concat(dfs, ignore_index=True)\n",
    "    df_s2.rename(\n",
    "        columns={\n",
    "            \"å•å·\": \"é‚®ä»¶å·\",\n",
    "            \"å…¬é‡Œ\": \"çº¿è·¯é‡Œç¨‹\",\n",
    "            \"ç­¾æ”¶æ—¶é—´\": \"å®ŒæˆæŠ•é€’æ—¶é—´\",\n",
    "            \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\": \"å¯„å‡ºåœ°å¤„ç†æ—¶é•¿\",\n",
    "            \"è¿è¾“æ—¶é™\": \"è¿è¾“æ—¶é•¿\",\n",
    "            \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\": \"å¯„è¾¾åœ°å¤„ç†æ—¶é•¿\",\n",
    "            \"æŠ•é€’æ—¶é™\": \"æŠ•é€’æ—¶é•¿\",\n",
    "            \"æ½æ”¶æ—¶é—´\": \"æ½ä»¶æ—¶é—´\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # æ­¥éª¤2ï¼šå§‹ç»ˆä»â€œçŒªçŒªäº‘â€åŠ è½½å¹¶åˆå¹¶æ•°æ®ï¼Œè¿™æ˜¯å¿…é¡»çš„\n",
    "    trace_dfs = []\n",
    "    for product_type, filename in {\n",
    "        p: f\"{product_to_filename.get(p, p)}.xlsx\" for p in products\n",
    "    }.items():\n",
    "        trace_file_path = zhuzhuyun_merge_path / filename\n",
    "        if trace_file_path.exists():\n",
    "            try:\n",
    "                df_trace_part = pd.read_excel(\n",
    "                    trace_file_path,\n",
    "                    usecols=[\"å¿«é€’å•å·\", \"å®Œæ•´ç‰©æµä¿¡æ¯\"],\n",
    "                    dtype={\"å¿«é€’å•å·\": str},\n",
    "                    engine=\"openpyxl\",\n",
    "                )\n",
    "                if not df_trace_part.empty:\n",
    "                    df_trace_part[\"å¿«é€’å•å·\"] = (\n",
    "                        df_trace_part[\"å¿«é€’å•å·\"].astype(str).str.strip()\n",
    "                    )\n",
    "                    df_trace_part[\"äº§å“ç§ç±»\"] = product_type\n",
    "                    trace_dfs.append(df_trace_part)\n",
    "            except Exception as e:\n",
    "                print(f\"      -> è­¦å‘Š: è¯»å–è½¨è¿¹æ–‡ä»¶ {filename} å¤±è´¥: {e}\")\n",
    "\n",
    "    if trace_dfs:\n",
    "        df_traces = pd.concat(trace_dfs, ignore_index=True).drop_duplicates(\n",
    "            subset=[\"å¿«é€’å•å·\", \"äº§å“ç§ç±»\"]\n",
    "        )\n",
    "        df_s2 = pd.merge(\n",
    "            df_s2,\n",
    "            df_traces,\n",
    "            how=\"left\",\n",
    "            left_on=[\"é‚®ä»¶å·\", \"äº§å“ç§ç±»\"],\n",
    "            right_on=[\"å¿«é€’å•å·\", \"äº§å“ç§ç±»\"],\n",
    "        )\n",
    "        df_s2.drop(columns=[\"å¿«é€’å•å·\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # ========================= æ ¸å¿ƒé€»è¾‘ï¼šå…¼å®¹æ€§å¤„ç† (START) =========================\n",
    "    #\n",
    "    if \"å®Œæ•´ç‰©æµä¿¡æ¯_y\" in df_s2.columns:\n",
    "        # **å¤„ç†æƒ…å†µ1ï¼šæºæ–‡ä»¶å’ŒçŒªçŒªäº‘éƒ½æœ‰è¯¥åˆ—ï¼Œå¯¼è‡´å†²çªï¼Œç”Ÿæˆäº†_xå’Œ_y**\n",
    "        # ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨çŒªçŒªäº‘(_y)çš„æ•°æ®ï¼Œå¦‚æœçŒªçŒªäº‘æ²¡åŒ¹é…ä¸Š(å€¼ä¸ºNaN)ï¼Œåˆ™ç”¨æºæ–‡ä»¶(_x)çš„æ•°æ®å¡«å……\n",
    "        print(\"      -> [å…¼å®¹æ¨¡å¼] æ£€æµ‹åˆ°'å®Œæ•´ç‰©æµä¿¡æ¯'åˆ—åå†²çªï¼Œæ™ºèƒ½åˆå¹¶ä¸­...\")\n",
    "        df_s2[\"å®Œæ•´ç‰©æµä¿¡æ¯\"] = df_s2[\"å®Œæ•´ç‰©æµä¿¡æ¯_y\"].fillna(df_s2[\"å®Œæ•´ç‰©æµä¿¡æ¯_x\"])\n",
    "        # æ¸…ç†æ‰ä¸´æ—¶çš„_xå’Œ_yåˆ—\n",
    "        df_s2.drop(columns=[\"å®Œæ•´ç‰©æµä¿¡æ¯_x\", \"å®Œæ•´ç‰©æµä¿¡æ¯_y\"], inplace=True)\n",
    "\n",
    "    elif \"å®Œæ•´ç‰©æµä¿¡æ¯\" in df_s2.columns:\n",
    "        print(\"      -> [å…¼å®¹æ¨¡å¼] 'å®Œæ•´ç‰©æµä¿¡æ¯'åˆ—å·²å­˜åœ¨ä¸”æ— å†²çªï¼Œæµç¨‹ç»§ç»­ã€‚\")\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(\"      -> [å…¼å®¹æ¨¡å¼] æœªæ‰¾åˆ°'å®Œæ•´ç‰©æµä¿¡æ¯'åˆ—ï¼Œå·²åˆ›å»ºç©ºåˆ—ä½œä¸ºä¿éšœã€‚\")\n",
    "        df_s2[\"å®Œæ•´ç‰©æµä¿¡æ¯\"] = \"\"\n",
    "\n",
    "    df_s2[\"å®Œæ•´ç‰©æµä¿¡æ¯\"].fillna(\"\", inplace=True)\n",
    "    #\n",
    "    # ========================== æ ¸å¿ƒé€»è¾‘ï¼šå…¼å®¹æ€§å¤„ç† (END) ==========================\n",
    "\n",
    "    df_s2[\"çº¿è·¯\"] = df_s2[\"å¯„å‡ºåŸå¸‚\"] + \"-\" + df_s2[\"å¯„è¾¾åŸå¸‚\"]\n",
    "    time_cols = [\n",
    "        \"æ½ä»¶æ—¶é—´\",\n",
    "        \"å®ŒæˆæŠ•é€’æ—¶é—´\",\n",
    "        \"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´\",\n",
    "        \"åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´\",\n",
    "        \"æ´¾é€æ—¶é—´\",\n",
    "    ]\n",
    "    for col in time_cols:\n",
    "        if col in df_s2.columns:\n",
    "            df_s2[col] = pd.to_datetime(df_s2[col], errors=\"coerce\")\n",
    "\n",
    "    if \"æ½ä»¶æ—¶é—´\" in df_s2.columns and \"å®ŒæˆæŠ•é€’æ—¶é—´\" in df_s2.columns:\n",
    "        valid_times = df_s2[\"æ½ä»¶æ—¶é—´\"].notna() & df_s2[\"å®ŒæˆæŠ•é€’æ—¶é—´\"].notna()\n",
    "        days_diff = (\n",
    "            df_s2.loc[valid_times, \"å®ŒæˆæŠ•é€’æ—¶é—´\"].dt.normalize()\n",
    "            - df_s2.loc[valid_times, \"æ½ä»¶æ—¶é—´\"].dt.normalize()\n",
    "        ).dt.days\n",
    "        df_s2.loc[valid_times, \"T+N\"] = \"T+\" + days_diff.astype(int).astype(str)\n",
    "\n",
    "    df_s2[\"æ˜¯å¦è¾¾æˆ72å°æ—¶å¦¥æŠ•ç‡\"] = np.where(df_s2[\"å…¨ç¨‹æ—¶é™\"] <= 72, \"æ˜¯\", \"å¦\")\n",
    "    top_30_mask = (df_s2[\"å¯„å‡ºåŸå¸‚\"].isin(top_30_cities)) & (\n",
    "        df_s2[\"å¯„è¾¾åŸå¸‚\"].isin(top_30_cities)\n",
    "    )\n",
    "    df_s2[\"æ˜¯å¦è¾¾æˆ48å°æ—¶å¦¥æŠ•ç‡\"] = \"ä¸é€‚ç”¨\"\n",
    "    df_s2.loc[top_30_mask, \"æ˜¯å¦è¾¾æˆ48å°æ—¶å¦¥æŠ•ç‡\"] = np.where(\n",
    "        df_s2.loc[top_30_mask, \"å…¨ç¨‹æ—¶é™\"] <= 48, \"æ˜¯\", \"å¦\"\n",
    "    )\n",
    "\n",
    "    # ... åç»­ä»£ç éƒ¨åˆ†ä¿æŒä¸å˜ ...\n",
    "    df_main_renamed = df_main.rename(columns={\"è·¯çº¿\": \"çº¿è·¯\"})\n",
    "    cols_to_merge_from_main = [\"çº¿è·¯\", \"é€è¾¾å¤©æ•°_average\", \"é€è¾¾å¤©æ•°_minimum\"]\n",
    "    metrics_for_merge = [\"48å°æ—¶å‡†æ—¶ç‡\", \"72å°æ—¶å‡†æ—¶ç‡\", \"å…¨ç¨‹æ—¶é™\"]\n",
    "    for m in metrics_for_merge:\n",
    "        for p in products:\n",
    "            cols_to_merge_from_main.append(f\"{m}{p}\")\n",
    "        for agg in [\"maximum\", \"average\", \"minimum\"]:\n",
    "            cols_to_merge_from_main.append(f\"{m}_{agg}\")\n",
    "\n",
    "    if \"å…¨ç¨‹æ—¶é™_minimum\" in df_main_renamed.columns:\n",
    "        cols_to_merge_from_main.append(\"å…¨ç¨‹æ—¶é™_minimum\")\n",
    "    if \"å…¨ç¨‹æ—¶é™_maximum\" in df_main_renamed.columns:\n",
    "        cols_to_merge_from_main.append(\"å…¨ç¨‹æ—¶é™_maximum\")\n",
    "\n",
    "    existing_cols_to_merge = [\n",
    "        c for c in cols_to_merge_from_main if c in df_main_renamed.columns\n",
    "    ]\n",
    "    df_s2 = pd.merge(\n",
    "        df_s2, df_main_renamed[existing_cols_to_merge], on=\"çº¿è·¯\", how=\"left\"\n",
    "    )\n",
    "    df_s2[\"è¡Œä¸šå‡å€¼\"] = df_s2.get(\"é€è¾¾å¤©æ•°_average\", \"\").fillna(\"\").astype(str)\n",
    "    df_s2[\"è¡Œä¸šæœ€ä¼˜\"] = df_s2.get(\"é€è¾¾å¤©æ•°_minimum\", \"\").fillna(\"\").astype(str)\n",
    "\n",
    "    rename_dict = {\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡_maximum\": \"è¡Œä¸šæœ€ä¼˜_48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡_average\": \"è¡Œä¸šå‡å€¼_48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"48å°æ—¶å‡†æ—¶ç‡_minimum\": \"è¡Œä¸šæœ€å·®_48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡_maximum\": \"è¡Œä¸šæœ€ä¼˜_72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡_average\": \"è¡Œä¸šå‡å€¼_72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"72å°æ—¶å‡†æ—¶ç‡_minimum\": \"è¡Œä¸šæœ€å·®_72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"å…¨ç¨‹æ—¶é™_minimum\": \"è¡Œä¸šæœ€ä¼˜_çº¿è·¯å…¨ç¨‹æ—¶é™\",\n",
    "        \"å…¨ç¨‹æ—¶é™_average\": \"è¡Œä¸šå‡å€¼_çº¿è·¯å…¨ç¨‹æ—¶é™\",\n",
    "        \"å…¨ç¨‹æ—¶é™_maximum\": \"è¡Œä¸šæœ€å·®_çº¿è·¯å…¨ç¨‹æ—¶é™\",\n",
    "    }\n",
    "    for p in products:\n",
    "        rename_dict[f\"48å°æ—¶å‡†æ—¶ç‡{p}\"] = f\"{p}_48å°æ—¶å¦¥æŠ•ç‡\"\n",
    "        rename_dict[f\"72å°æ—¶å‡†æ—¶ç‡{p}\"] = f\"{p}_72å°æ—¶å¦¥æŠ•ç‡\"\n",
    "        rename_dict[f\"å…¨ç¨‹æ—¶é™{p}\"] = f\"{p}_çº¿è·¯å…¨ç¨‹æ—¶é™\"\n",
    "    df_s2.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    final_columns_order_s2 = [\n",
    "        \"é‚®ä»¶å·\",\n",
    "        \"äº§å“ç§ç±»\",\n",
    "        \"å¯„å‡ºçœä»½\",\n",
    "        \"å¯„å‡ºåŸå¸‚\",\n",
    "        \"å¯„è¾¾çœä»½\",\n",
    "        \"å¯„è¾¾åŸå¸‚\",\n",
    "        \"çº¿è·¯\",\n",
    "        \"çº¿è·¯é‡Œç¨‹\",\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "        \"T+N\",\n",
    "        \"è¡Œä¸šå‡å€¼\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜\",\n",
    "        \"æ˜¯å¦è¾¾æˆ48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"æ˜¯å¦è¾¾æˆ72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é•¿\",\n",
    "        \"è¿è¾“æ—¶é•¿\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é•¿\",\n",
    "        \"æŠ•é€’æ—¶é•¿\",\n",
    "        \"æ½ä»¶æ—¶é—´\",\n",
    "        \"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´\",\n",
    "        \"åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´\",\n",
    "        \"æ´¾é€æ—¶é—´\",\n",
    "        \"å®ŒæˆæŠ•é€’æ—¶é—´\",\n",
    "    ]\n",
    "\n",
    "    newly_added_columns = [\n",
    "        \"EMS_48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"å¿«åŒ…_48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"è¡Œä¸šå‡å€¼_48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"è¡Œä¸šæœ€å·®_48å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"EMS_72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"å¿«åŒ…_72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"è¡Œä¸šå‡å€¼_72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"è¡Œä¸šæœ€å·®_72å°æ—¶å¦¥æŠ•ç‡\",\n",
    "        \"EMS_çº¿è·¯å…¨ç¨‹æ—¶é™\",\n",
    "        \"å¿«åŒ…_çº¿è·¯å…¨ç¨‹æ—¶é™\",\n",
    "        \"è¡Œä¸šæœ€ä¼˜_çº¿è·¯å…¨ç¨‹æ—¶é™\",\n",
    "        \"è¡Œä¸šå‡å€¼_çº¿è·¯å…¨ç¨‹æ—¶é™\",\n",
    "        \"è¡Œä¸šæœ€å·®_çº¿è·¯å…¨ç¨‹æ—¶é™\",\n",
    "    ]\n",
    "    if \"æå…”\" in products:\n",
    "        newly_added_columns = [\n",
    "            c for c in newly_added_columns if \"æå…”\" in c or \"è¡Œä¸š\" in c\n",
    "        ]\n",
    "\n",
    "    final_columns_order_s2.extend(newly_added_columns)\n",
    "    final_columns_order_s2.append(\"å®Œæ•´ç‰©æµä¿¡æ¯\")\n",
    "\n",
    "    existing_cols = [col for col in final_columns_order_s2 if col in df_s2.columns]\n",
    "    return df_s2[existing_cols]\n",
    "\n",
    "\n",
    "def weighted_agg(group, metrics, company):\n",
    "    \"\"\"è¾…åŠ©å‡½æ•°ï¼šå¯¹å•ä¸ªåˆ†ç»„è¿›è¡ŒåŠ æƒå¹³å‡è®¡ç®—\"\"\"\n",
    "    weight_col = f\"å¿«é€’æ•°é‡{company}\"\n",
    "    if weight_col not in group.columns or group[weight_col].sum() == 0:\n",
    "        return pd.Series([np.nan] * len(metrics), index=metrics)\n",
    "\n",
    "    total_weight = group[weight_col].sum()\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        metric_col = f\"{metric}{company}\"\n",
    "        if metric_col in group.columns:\n",
    "            weighted_sum = (group[metric_col] * group[weight_col]).sum()\n",
    "            results[metric] = weighted_sum / total_weight\n",
    "        else:\n",
    "            results[metric] = np.nan\n",
    "    return pd.Series(results)\n",
    "\n",
    "\n",
    "def create_regional_report_data(\n",
    "    df_main: pd.DataFrame, grouping_level: str, final_col_order: list, products: list\n",
    "):\n",
    "    \"\"\"ã€æœ€ç»ˆä¿®å¤ã€‘: æ¢å¤åŸå§‹è®¡ç®—æ¡†æ¶ï¼Œå¹¶æ³¨å…¥åŠ æƒå¹³å‡\"\"\"\n",
    "    print(\n",
    "        f\"  - [{3 if grouping_level == 'city' else 4}/4] æ­£åœ¨å‡†å¤‡ 'åˆ†{grouping_level}æ˜ç»†' (æ¢å¤åŸå§‹è®¡ç®—æ¡†æ¶ + åŠ æƒå¹³å‡)...\"\n",
    "    )\n",
    "\n",
    "    # 1. æ¢å¤â€œåˆ†ç¦»å¼èšåˆâ€æ¡†æ¶\n",
    "    metrics_by_destination = [\n",
    "        \"åˆ°è¾¾å¯„è¾¾åœ°åŸå¸‚-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "        \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-æ´¾ä»¶\",\n",
    "        \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        \"æŠ•é€’æ—¶é™\",\n",
    "    ]\n",
    "    metrics_by_origin = [\n",
    "        \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "        \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„å‡ºåœ°åŸå¸‚æ—¶é•¿\",\n",
    "        \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        \"è¿è¾“æ—¶é™\",\n",
    "        \"å…¨ç¨‹æ—¶é™\",\n",
    "    ]\n",
    "\n",
    "    all_pivots_origin = []\n",
    "    group_cols_origin = {\"city\": [\"å¯„å‡ºçœä»½\", \"å¯„å‡ºåŸå¸‚\"], \"province\": [\"å¯„å‡ºçœä»½\"]}[\n",
    "        grouping_level\n",
    "    ]\n",
    "    for comp in COMPANIES_ALL_TEN:\n",
    "        comp_metrics = [m for m in metrics_by_origin if f\"{m}{comp}\" in df_main.columns]\n",
    "        if not comp_metrics or f\"å¿«é€’æ•°é‡{comp}\" not in df_main.columns:\n",
    "            continue\n",
    "\n",
    "        # ========================= ä¿®æ”¹ç‚¹ 1 of 3 =========================\n",
    "        df_agg = (\n",
    "            df_main.groupby(group_cols_origin)\n",
    "            .apply(\n",
    "                weighted_agg,\n",
    "                metrics=comp_metrics,\n",
    "                company=comp,  # <-- åˆ é™¤ include_groups=False\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        # ===============================================================\n",
    "        df_agg[\"company\"] = comp\n",
    "        all_pivots_origin.append(df_agg)\n",
    "\n",
    "    if not all_pivots_origin:\n",
    "        df_pivot_origin = pd.DataFrame(\n",
    "            index=pd.MultiIndex.from_tuples([], names=group_cols_origin)\n",
    "        )\n",
    "    else:\n",
    "        df_pivot_origin = pd.concat(all_pivots_origin).pivot_table(\n",
    "            index=group_cols_origin, columns=\"company\", values=metrics_by_origin\n",
    "        )\n",
    "        if not df_pivot_origin.empty:\n",
    "            df_pivot_origin.columns = [f\"{v}_{c}\" for v, c in df_pivot_origin.columns]\n",
    "\n",
    "    all_pivots_dest = []\n",
    "    group_cols_dest = {\"city\": [\"å¯„è¾¾çœä»½\", \"å¯„è¾¾åŸå¸‚\"], \"province\": [\"å¯„è¾¾çœä»½\"]}[\n",
    "        grouping_level\n",
    "    ]\n",
    "\n",
    "    needed_cols = group_cols_dest.copy()\n",
    "    for comp in COMPANIES_ALL_TEN:\n",
    "        needed_cols.append(f\"å¿«é€’æ•°é‡{comp}\")\n",
    "        for metric in metrics_by_destination:\n",
    "            needed_cols.append(f\"{metric}{comp}\")\n",
    "\n",
    "    existing_needed_cols = [c for c in needed_cols if c in df_main.columns]\n",
    "\n",
    "    df_dest_subset = df_main[existing_needed_cols]\n",
    "    df_dest_temp = df_dest_subset.rename(\n",
    "        columns={\"å¯„è¾¾çœä»½\": \"å¯„å‡ºçœä»½\", \"å¯„è¾¾åŸå¸‚\": \"å¯„å‡ºåŸå¸‚\"}\n",
    "    )\n",
    "\n",
    "    for comp in COMPANIES_ALL_TEN:\n",
    "        comp_metrics = [\n",
    "            m for m in metrics_by_destination if f\"{m}{comp}\" in df_dest_temp.columns\n",
    "        ]\n",
    "        if not comp_metrics or f\"å¿«é€’æ•°é‡{comp}\" not in df_dest_temp.columns:\n",
    "            continue\n",
    "\n",
    "        # ========================= ä¿®æ”¹ç‚¹ 2 of 3 =========================\n",
    "        df_agg = (\n",
    "            df_dest_temp.groupby(group_cols_origin)\n",
    "            .apply(\n",
    "                weighted_agg,\n",
    "                metrics=comp_metrics,\n",
    "                company=comp,  # <-- åˆ é™¤ include_groups=False\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        # ===============================================================\n",
    "        df_agg[\"company\"] = comp\n",
    "        all_pivots_dest.append(df_agg)\n",
    "\n",
    "    if not all_pivots_dest:\n",
    "        df_pivot_dest = pd.DataFrame(\n",
    "            index=pd.MultiIndex.from_tuples([], names=group_cols_origin)\n",
    "        )\n",
    "    else:\n",
    "        df_pivot_dest = pd.concat(all_pivots_dest).pivot_table(\n",
    "            index=group_cols_origin, columns=\"company\", values=metrics_by_destination\n",
    "        )\n",
    "        if not df_pivot_dest.empty:\n",
    "            df_pivot_dest.columns = [f\"{v}_{c}\" for v, c in df_pivot_dest.columns]\n",
    "\n",
    "    df_merged_main = pd.merge(\n",
    "        df_pivot_origin, df_pivot_dest, left_index=True, right_index=True, how=\"outer\"\n",
    "    )\n",
    "\n",
    "    # 2. æ¢å¤â€œåŒæŒ‚å…¨ç¨‹æ—¶é™â€è®¡ç®—æ¡†æ¶ + æ³¨å…¥åŠ æƒå¹³å‡\n",
    "    df_sent = df_main.rename(columns={\"å¯„å‡ºçœä»½\": \"çœä»½\", \"å¯„å‡ºåŸå¸‚\": \"åŸå¸‚\"})\n",
    "    df_recv = df_main.rename(columns={\"å¯„è¾¾çœä»½\": \"çœä»½\", \"å¯„è¾¾åŸå¸‚\": \"åŸå¸‚\"})\n",
    "    df_dual = pd.concat([df_sent, df_recv]).dropna(subset=[\"çœä»½\"])\n",
    "    dual_group_cols = [\"çœä»½\", \"åŸå¸‚\"] if grouping_level == \"city\" else [\"çœä»½\"]\n",
    "\n",
    "    all_pivots_dual = []\n",
    "    for comp in COMPANIES_ALL_TEN:\n",
    "        if (\n",
    "            f\"å…¨ç¨‹æ—¶é™{comp}\" not in df_dual.columns\n",
    "            or f\"å¿«é€’æ•°é‡{comp}\" not in df_dual.columns\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # ========================= ä¿®æ”¹ç‚¹ 3 of 3 =========================\n",
    "        df_agg = (\n",
    "            df_dual.groupby(dual_group_cols)\n",
    "            .apply(\n",
    "                weighted_agg,\n",
    "                metrics=[\"å…¨ç¨‹æ—¶é™\"],\n",
    "                company=comp,  # <-- åˆ é™¤ include_groups=False\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        # ===============================================================\n",
    "        df_agg[\"company\"] = comp\n",
    "        all_pivots_dual.append(df_agg)\n",
    "\n",
    "    if not all_pivots_dual:\n",
    "        df_pivot_dual = pd.DataFrame(\n",
    "            index=pd.MultiIndex.from_tuples([], names=dual_group_cols)\n",
    "        )\n",
    "    else:\n",
    "        df_pivot_dual = pd.concat(all_pivots_dual).pivot_table(\n",
    "            index=dual_group_cols, columns=\"company\", values=\"å…¨ç¨‹æ—¶é™\"\n",
    "        )\n",
    "        if not df_pivot_dual.empty:\n",
    "            df_pivot_dual.columns = [\n",
    "                f\"å…¨ç¨‹ï¼ˆåŒæŒ‚ï¼‰æ—¶é™_{c}\" for c in df_pivot_dual.columns\n",
    "            ]\n",
    "            df_pivot_dual.index.names = group_cols_origin\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_merged_main, df_pivot_dual, left_index=True, right_index=True, how=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # ã€ä¿®å¤ã€‘: æ¢å¤åŸå§‹ä»£ç ä¸­å¯¹åˆ—åçš„å¤„ç†æ–¹å¼\n",
    "    final_report_data = df_merged.copy()\n",
    "    rename_map = {}\n",
    "    for col in final_report_data.columns:\n",
    "        if isinstance(col, str) and \"_\" in col:\n",
    "            parts = col.split(\"_\")\n",
    "            metric = parts[0]\n",
    "            company = parts[1]\n",
    "            if metric == \"å…¨ç¨‹æ—¶é™\":\n",
    "                metric = \"å…¨ç¨‹ï¼ˆå¯„å‡ºåœ°ï¼‰æ—¶é™\"\n",
    "            if company in COMPANIES_ALL_TEN:\n",
    "                rename_map[col] = f\"{company}_{metric}\"\n",
    "    final_report_data.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # 3. æ¢å¤åç»­çš„è¡Œä¸šç»Ÿè®¡ã€æ’åå’Œé™„åŠ è®¡ç®—\n",
    "    all_metrics_for_ranking = [\n",
    "        m.replace(\"å…¨ç¨‹æ—¶é™\", \"å…¨ç¨‹ï¼ˆå¯„å‡ºåœ°ï¼‰æ—¶é™\")\n",
    "        for m in (metrics_by_origin + metrics_by_destination)\n",
    "    ] + [\"å…¨ç¨‹ï¼ˆåŒæŒ‚ï¼‰æ—¶é™\"]\n",
    "    all_metrics_for_ranking = list(set(all_metrics_for_ranking))\n",
    "\n",
    "    for metric in all_metrics_for_ranking:\n",
    "        industry_cols = [\n",
    "            f\"{prod}_{metric}\"\n",
    "            for prod in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "            if f\"{prod}_{metric}\" in final_report_data.columns\n",
    "        ]\n",
    "        if not industry_cols:\n",
    "            continue\n",
    "        final_report_data[f\"è¡Œä¸šå‡å€¼_{metric}\"] = final_report_data[industry_cols].mean(\n",
    "            axis=1\n",
    "        )\n",
    "        final_report_data[f\"è¡Œä¸šæœ€ä¼˜_{metric}\"] = final_report_data[industry_cols].min(\n",
    "            axis=1\n",
    "        )\n",
    "        for prod in products:\n",
    "            rank_pool = (\n",
    "                COMPANIES_EIGHT_OTHERS + [\"å¿«åŒ…\"]\n",
    "                if prod == \"å¿«åŒ…\"\n",
    "                else list(COMPANIES_NINE_MAJOR)\n",
    "            )\n",
    "            rank_cols = [\n",
    "                f\"{p}_{metric}\"\n",
    "                for p in rank_pool\n",
    "                if f\"{p}_{metric}\" in final_report_data.columns\n",
    "            ]\n",
    "            prod_col_name = f\"{prod}_{metric}\"\n",
    "            if rank_cols and prod_col_name in final_report_data.columns:\n",
    "                final_report_data[f\"{prod}æ’å_{metric}\"] = final_report_data[\n",
    "                    rank_cols\n",
    "                ].rank(axis=1, method=\"min\", ascending=True)[prod_col_name]\n",
    "\n",
    "    sum_metric = \"å¯„å‡ºåœ°å¤„ç†æ—¶é™+å¯„è¾¾åœ°å¤„ç†æ—¶é™+æŠ•é€’æ—¶é™\"\n",
    "    part_metrics = [\"å¯„å‡ºåœ°å¤„ç†æ—¶é™\", \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\", \"æŠ•é€’æ—¶é™\"]\n",
    "    all_report_companies = products + [\n",
    "        c for c in COMPANIES_FOR_INDUSTRY_COMPARISON if c not in products\n",
    "    ]\n",
    "    for company in all_report_companies:\n",
    "        part_cols = [f\"{company}_{p}\" for p in part_metrics]\n",
    "        if all(c in final_report_data.columns for c in part_cols):\n",
    "            final_report_data[f\"{company}_{sum_metric}\"] = final_report_data[\n",
    "                part_cols\n",
    "            ].sum(axis=1, min_count=3)\n",
    "    avg_part_cols = [f\"è¡Œä¸šå‡å€¼_{p}\" for p in part_metrics]\n",
    "    if all(c in final_report_data.columns for c in avg_part_cols):\n",
    "        final_report_data[f\"è¡Œä¸šå‡å€¼_{sum_metric}\"] = final_report_data[\n",
    "            avg_part_cols\n",
    "        ].sum(axis=1, min_count=3)\n",
    "    total_sum_cols = [\n",
    "        f\"{c}_{sum_metric}\"\n",
    "        for c in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "        if f\"{c}_{sum_metric}\" in final_report_data.columns\n",
    "    ]\n",
    "    if total_sum_cols:\n",
    "        final_report_data[f\"è¡Œä¸šæœ€ä¼˜_{sum_metric}\"] = final_report_data[\n",
    "            total_sum_cols\n",
    "        ].min(axis=1, skipna=True)\n",
    "    for prod in products:\n",
    "        rank_pool = (\n",
    "            COMPANIES_EIGHT_OTHERS + [\"å¿«åŒ…\"]\n",
    "            if prod == \"å¿«åŒ…\"\n",
    "            else list(COMPANIES_NINE_MAJOR)\n",
    "        )\n",
    "        rank_cols = [\n",
    "            f\"{c}_{sum_metric}\"\n",
    "            for c in rank_pool\n",
    "            if f\"{c}_{sum_metric}\" in final_report_data.columns\n",
    "        ]\n",
    "        prod_sum_col = f\"{prod}_{sum_metric}\"\n",
    "        if rank_cols and prod_sum_col in final_report_data.columns:\n",
    "            final_report_data[f\"{prod}æ’å_{sum_metric}\"] = final_report_data[\n",
    "                rank_cols\n",
    "            ].rank(axis=1, method=\"min\", ascending=True)[prod_sum_col]\n",
    "\n",
    "    # 4. æœ€åæ•´ç†\n",
    "    final_report_data.rename(\n",
    "        columns={\"å¯„å‡ºçœä»½\": \"çœä»½\", \"å¯„å‡ºåŸå¸‚\": \"åœ°å¸‚\"}, inplace=True\n",
    "    )\n",
    "    existing_cols = [c for c in final_col_order if c in final_report_data.columns]\n",
    "    final_df = final_report_data[existing_cols].copy()\n",
    "    sort_keys = [\"çœä»½\", \"åœ°å¸‚\"] if grouping_level == \"city\" else [\"çœä»½\"]\n",
    "    if any(k in final_df.columns for k in sort_keys):\n",
    "        final_df.sort_values(\n",
    "            by=[k for k in sort_keys if k in final_df.columns], inplace=True\n",
    "        )\n",
    "    if not final_df.empty:\n",
    "        final_df.insert(0, \"åºå·\", range(1, len(final_df) + 1))\n",
    "    return final_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _generate_dynamic_column_order(products: list, level: str) -> list:\n",
    "    \"\"\"ã€æ¢å¤ã€‘æ­¤å‡½æ•°å·²å®Œå…¨æ¢å¤åˆ°æ‚¨çš„åŸå§‹ç‰ˆæœ¬\"\"\"\n",
    "    base_cols = [\"åºå·\", \"çœä»½\"]\n",
    "    if level == \"city\":\n",
    "        base_cols.append(\"åœ°å¸‚\")\n",
    "    order = base_cols.copy()\n",
    "    sections = {\n",
    "        \"å¯„å‡ºåœ°å¤„ç†\": [\n",
    "            \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "            \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„å‡ºåœ°åŸå¸‚æ—¶é•¿\",\n",
    "            \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "        ],\n",
    "        \"å¯„è¾¾åœ°å¤„ç†\": [\n",
    "            \"åˆ°è¾¾å¯„è¾¾åœ°åŸå¸‚-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "            \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-æ´¾ä»¶\",\n",
    "            \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "        ],\n",
    "        \"æŠ•é€’\": [\"æŠ•é€’æ—¶é™\"],\n",
    "        \"ç«¯åˆ°ç«¯å¤„ç†\": [\"å¯„å‡ºåœ°å¤„ç†æ—¶é™+å¯„è¾¾åœ°å¤„ç†æ—¶é™+æŠ•é€’æ—¶é™\"],\n",
    "        \"å¯„å‡ºåœ°å…¨ç¨‹\": [\"å…¨ç¨‹ï¼ˆå¯„å‡ºåœ°ï¼‰æ—¶é™\"],\n",
    "        \"åŒæŒ‚å…¨ç¨‹\": [\"å…¨ç¨‹ï¼ˆåŒæŒ‚ï¼‰æ—¶é™\"],\n",
    "    }\n",
    "    for _, metrics in sections.items():\n",
    "        for prod in products:\n",
    "            for metric in metrics:\n",
    "                order.extend([f\"{prod}_{metric}\", f\"{prod}æ’å_{metric}\"])\n",
    "        for metric in metrics:\n",
    "            order.append(f\"è¡Œä¸šå‡å€¼_{metric}\")\n",
    "        for metric in metrics:\n",
    "            order.append(f\"è¡Œä¸šæœ€ä¼˜_{metric}\")\n",
    "    return order\n",
    "\n",
    "\n",
    "# --- 3. ä¸»æ‰§è¡Œå‡½æ•° ---\n",
    "def generate_company_monthly_report(report_type: str):\n",
    "    if report_type not in REPORT_CONFIG:\n",
    "        print(f\"ğŸ”¥ğŸ”¥ğŸ”¥ é”™è¯¯: æœªçŸ¥çš„æŠ¥å‘Šç±»å‹ '{report_type}'ã€‚\")\n",
    "        return\n",
    "    config = REPORT_CONFIG[report_type]\n",
    "    print(f\"\\n{'=' * 20} å¼€å§‹ç”Ÿæˆ: {config['output_filename']} {'=' * 20}\")\n",
    "\n",
    "    base_path = Path.cwd()\n",
    "    output_path = base_path / \"æŠ¥å‘Šæ•°æ®\" / \"è¾“å‡º\"\n",
    "    data_analysis_path = output_path / \"data_analysis_result\"\n",
    "    zhuzhuyun_merge_path = base_path / \"æŠ¥å‘Šæ•°æ®\" / \"temp\" / \"3_çŒªçŒªäº‘åˆå¹¶æ•°æ®\"\n",
    "    basic_data_path = base_path / \"æŠ¥å‘Šæ•°æ®\" / \"è¾“å…¥\" / \"basic_data.xlsx\"\n",
    "    final_output_path = output_path / config[\"output_filename\"]\n",
    "    for p in [output_path, data_analysis_path, zhuzhuyun_merge_path]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"  - æ­£åœ¨ç‹¬ç«‹åŠ è½½æ‰€æœ‰å…¬å¸çš„'çº¿è·¯æ±‡æ€»æ•°æ®'...\")\n",
    "    try:\n",
    "        all_routes = set()\n",
    "        files_in_analysis_result = list(\n",
    "            data_analysis_path.glob(\"*_data_analysis_result.xlsx\")\n",
    "        )\n",
    "        if not files_in_analysis_result:\n",
    "            raise FileNotFoundError(\n",
    "                f\"åœ¨è·¯å¾„ '{data_analysis_path.resolve()}' ä¸­æœªæ‰¾åˆ°ä»»ä½• '*_data_analysis_result.xlsx' æ–‡ä»¶ã€‚\"\n",
    "            )\n",
    "\n",
    "        for file_path in files_in_analysis_result:\n",
    "            if file_path.name.startswith(\"~$\"):\n",
    "                continue\n",
    "            try:\n",
    "                all_routes.update(\n",
    "                    pd.read_excel(\n",
    "                        file_path,\n",
    "                        sheet_name=\"çº¿è·¯æ±‡æ€»æ•°æ®\",\n",
    "                        usecols=[\"è·¯çº¿\"],\n",
    "                        engine=\"openpyxl\",\n",
    "                    )[\"è·¯çº¿\"].unique()\n",
    "                )\n",
    "            except zipfile.BadZipFile:\n",
    "                print(\n",
    "                    f\"      -> è­¦å‘Š: æ–‡ä»¶ {file_path.name} å·²æŸåæˆ–ä¸æ˜¯æœ‰æ•ˆçš„Excelæ–‡ä»¶ï¼Œå·²è·³è¿‡ã€‚\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        df_main = pd.DataFrame(list(all_routes), columns=[\"è·¯çº¿\"])\n",
    "        df_base_info = pd.read_excel(\n",
    "            basic_data_path, sheet_name=\"inter-city_routes\", engine=\"openpyxl\"\n",
    "        ).rename(columns={\"å…¬é‡Œ\": \"çº¿è·¯é‡Œç¨‹\", \"ç»æµåœˆ\": \"åŸå¸‚åœˆ\"})\n",
    "        cols_to_merge = [\n",
    "            \"å¯„å‡ºçœä»½\",\n",
    "            \"å¯„å‡ºåŸå¸‚\",\n",
    "            \"å¯„è¾¾çœä»½\",\n",
    "            \"å¯„è¾¾åŸå¸‚\",\n",
    "            \"è·¯çº¿\",\n",
    "            \"çº¿è·¯é‡Œç¨‹\",\n",
    "            \"åŸå¸‚åœˆ\",\n",
    "        ]\n",
    "        df_main = pd.merge(\n",
    "            df_main,\n",
    "            df_base_info[[c for c in cols_to_merge if c in df_base_info.columns]],\n",
    "            on=\"è·¯çº¿\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        metrics_to_extract = [\n",
    "            \"å¿«é€’æ•°é‡\",\n",
    "            \"å…¨ç¨‹æ—¶é™\",\n",
    "            \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "            \"è¿è¾“æ—¶é™\",\n",
    "            \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "            \"æŠ•é€’æ—¶é™\",\n",
    "            \"æ½æ”¶-åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "            \"åˆ°è¾¾å¯„å‡ºåœ°åˆ†æ‹£ä¸­å¿ƒ-ç¦»å¼€å¯„å‡ºåœ°åŸå¸‚æ—¶é•¿\",\n",
    "            \"åˆ°è¾¾å¯„è¾¾åœ°åŸå¸‚-ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒæ—¶é•¿\",\n",
    "            \"ç¦»å¼€å¯„è¾¾åœ°åˆ†æ‹£ä¸­å¿ƒ-æ´¾ä»¶\",\n",
    "            \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "            \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "            \"é€è¾¾å¤©æ•°_80åˆ†ä½\",\n",
    "            \"ä¸­è½¬æ¬¡æ•°\",\n",
    "        ]\n",
    "\n",
    "        for file_path in files_in_analysis_result:\n",
    "            if file_path.name.startswith(\"~$\"):\n",
    "                continue\n",
    "            company_key = _find_company_key_from_filename(file_path.name)\n",
    "            if not company_key:\n",
    "                continue\n",
    "            try:\n",
    "                df_summary = pd.read_excel(\n",
    "                    file_path, sheet_name=\"çº¿è·¯æ±‡æ€»æ•°æ®\", engine=\"openpyxl\"\n",
    "                )\n",
    "                for metric in metrics_to_extract:\n",
    "                    if metric in df_summary.columns:\n",
    "                        if metric == \"ä¸­è½¬æ¬¡æ•°\":\n",
    "                            new_col_name = f\"{company_key}å¹³å‡ä¸­è½¬æ¬¡æ•°\"\n",
    "                        else:\n",
    "                            new_col_name = (\n",
    "                                f\"{metric.replace('_80åˆ†ä½', '')}{company_key}\"\n",
    "                            )\n",
    "\n",
    "                        df_metric = (\n",
    "                            df_summary[[\"è·¯çº¿\", metric]]\n",
    "                            .copy()\n",
    "                            .rename(columns={metric: new_col_name})\n",
    "                        )\n",
    "                        df_main = pd.merge(df_main, df_metric, on=\"è·¯çº¿\", how=\"left\")\n",
    "            except zipfile.BadZipFile:\n",
    "                print(\n",
    "                    f\"      -> è­¦å‘Š: æ–‡ä»¶ {file_path.name} å·²æŸåæˆ–ä¸æ˜¯æœ‰æ•ˆçš„Excelæ–‡ä»¶ï¼Œå·²è·³è¿‡ã€‚\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        df_main.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "        print(\"  - æ­£åœ¨è®¡ç®—'æœ€ä¼˜ä¸­è½¬æ¬¡æ•°'...\")\n",
    "        all_transfer_cols = [\n",
    "            f\"{comp}å¹³å‡ä¸­è½¬æ¬¡æ•°\"\n",
    "            for comp in COMPANIES_ALL_TEN\n",
    "            if f\"{comp}å¹³å‡ä¸­è½¬æ¬¡æ•°\" in df_main.columns\n",
    "        ]\n",
    "        if all_transfer_cols:\n",
    "            cols_for_best_turnover = [c for c in all_transfer_cols if \"å¿«åŒ…\" not in c]\n",
    "            if cols_for_best_turnover:\n",
    "                df_main[\"æœ€ä¼˜ä¸­è½¬æ¬¡æ•°\"] = df_main[cols_for_best_turnover].min(axis=1)\n",
    "\n",
    "        # é‡æ–°è®¡ç®—è¡Œä¸šç»Ÿè®¡å€¼\n",
    "        stat_metrics = [\n",
    "            metric\n",
    "            for metric in metrics_to_extract\n",
    "            if metric not in [\"å¿«é€’æ•°é‡\", \"ä¸­è½¬æ¬¡æ•°\"]\n",
    "        ]\n",
    "        for metric in stat_metrics:\n",
    "            base_metric_name = metric.replace(\"_80åˆ†ä½\", \"\")\n",
    "            metric_cols = [\n",
    "                f\"{base_metric_name}{comp}\"\n",
    "                for comp in COMPANIES_FOR_INDUSTRY_COMPARISON\n",
    "                if f\"{base_metric_name}{comp}\" in df_main.columns\n",
    "            ]\n",
    "            if metric_cols:\n",
    "                df_main[f\"{base_metric_name}_average\"] = df_main[metric_cols].mean(\n",
    "                    axis=1\n",
    "                )\n",
    "                df_main[f\"{base_metric_name}_minimum\"] = df_main[metric_cols].min(\n",
    "                    axis=1\n",
    "                )\n",
    "                df_main[f\"{base_metric_name}_maximum\"] = df_main[metric_cols].max(\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "        # é‡æ–°è®¡ç®—æ’å\n",
    "        rank_metrics = [\n",
    "            \"48å°æ—¶å‡†æ—¶ç‡\",\n",
    "            \"72å°æ—¶å‡†æ—¶ç‡\",\n",
    "            \"å…¨ç¨‹æ—¶é™\",\n",
    "            \"å¯„å‡ºåœ°å¤„ç†æ—¶é™\",\n",
    "            \"è¿è¾“æ—¶é™\",\n",
    "            \"å¯„è¾¾åœ°å¤„ç†æ—¶é™\",\n",
    "            \"æŠ•é€’æ—¶é™\",\n",
    "            \"é€è¾¾å¤©æ•°\",\n",
    "        ]\n",
    "        for param in rank_metrics:\n",
    "            is_desc = \"å‡†æ—¶ç‡\" in param\n",
    "            ems_cols = [\n",
    "                f\"{param}{comp}\"\n",
    "                for comp in COMPANIES_NINE_MAJOR\n",
    "                if f\"{param}{comp}\" in df_main.columns\n",
    "            ]\n",
    "            if ems_cols and f\"{param}EMS\" in df_main.columns:\n",
    "                df_main[f\"{param}_emsæ’å\"] = df_main[ems_cols].rank(\n",
    "                    axis=1, method=\"min\", ascending=not is_desc\n",
    "                )[f\"{param}EMS\"]\n",
    "            kb_cols = [\n",
    "                f\"{param}{comp}\"\n",
    "                for comp in COMPANIES_ALL_TEN\n",
    "                if comp != \"EMS\" and f\"{param}{comp}\" in df_main.columns\n",
    "            ]\n",
    "            if kb_cols and f\"{param}å¿«åŒ…\" in df_main.columns:\n",
    "                df_main[f\"{param}_å¿«åŒ…æ’å\"] = df_main[kb_cols].rank(\n",
    "                    axis=1, method=\"min\", ascending=not is_desc\n",
    "                )[f\"{param}å¿«åŒ…\"]\n",
    "\n",
    "        # T+N æ ¼å¼åŒ–\n",
    "        for col in df_main.columns:\n",
    "            if (\n",
    "                \"é€è¾¾å¤©æ•°\" in col\n",
    "                and \"æ’å\" not in col\n",
    "                and df_main[col].dtype != \"object\"\n",
    "            ):\n",
    "                df_main[col] = pd.to_numeric(df_main[col], errors=\"coerce\").round()\n",
    "                df_main[col] = df_main[col].apply(\n",
    "                    lambda x: f\"T+{int(x)}\" if pd.notna(x) else x\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸ”¥ğŸ”¥ğŸ”¥ ç‹¬ç«‹åŠ è½½æ•°æ®å¤±è´¥: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # top_30_cities çš„åŠ è½½ä»ç„¶æ˜¯éœ€è¦çš„ï¼Œå› ä¸ºå®ƒå¯èƒ½è¢«å…¶ä»–å‡½æ•°ï¼ˆæˆ–æœªæ¥çš„éœ€æ±‚ï¼‰ä½¿ç”¨\n",
    "    # ä½†åœ¨ create_sheet2_mail_details ä¸­ä¸å†ç›´æ¥ç”¨äºåˆ¤æ–­å•ä¸ªé‚®ä»¶\n",
    "    top_30_cities = load_top_cities(basic_data_path, \"30_top_volume_city_2024\")\n",
    "    if not top_30_cities:\n",
    "        print(\"ğŸ”¥ğŸ”¥ğŸ”¥ é”™è¯¯ï¼šæœªèƒ½åŠ è½½Top 30åŸå¸‚åˆ—è¡¨ï¼Œæµç¨‹ä¸­æ­¢ã€‚\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_s1 = create_sheet1_route_details(df_main, config[\"products\"])\n",
    "        # åœ¨è°ƒç”¨æ—¶ï¼Œä¸å†éœ€è¦ä¼ é€’ top_30_cities\n",
    "        df_s2 = create_sheet2_mail_details(\n",
    "            data_analysis_path,\n",
    "            zhuzhuyun_merge_path,\n",
    "            config[\"products\"],\n",
    "            config[\"product_to_filename\"],\n",
    "            top_30_cities,\n",
    "            df_main,\n",
    "        )\n",
    "\n",
    "        city_cols_order = _generate_dynamic_column_order(config[\"products\"], \"city\")\n",
    "        province_cols_order = _generate_dynamic_column_order(\n",
    "            config[\"products\"], \"province\"\n",
    "        )\n",
    "\n",
    "        df_s3 = create_regional_report_data(\n",
    "            df_main, \"city\", city_cols_order, config[\"products\"]\n",
    "        )\n",
    "        df_s4 = create_regional_report_data(\n",
    "            df_main, \"province\", province_cols_order, config[\"products\"]\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- æ‰€æœ‰æ•°æ®è®¡ç®—å®Œæˆï¼Œæ­£åœ¨å†™å…¥æœ€ç»ˆæ–‡ä»¶: {final_output_path.name} ---\")\n",
    "        with pd.ExcelWriter(final_output_path, engine=\"xlsxwriter\") as writer:\n",
    "            sheets_to_write = {\n",
    "                \"çº¿è·¯æ˜ç»†\": df_s1,\n",
    "                \"é‚®ä»¶æ˜ç»†\": df_s2,\n",
    "                \"åˆ†åŸå¸‚æ˜ç»†\": df_s3,\n",
    "                \"åˆ†çœä»½æ˜ç»†\": df_s4,\n",
    "            }\n",
    "            for sheet_name, df in sheets_to_write.items():\n",
    "                print(f\"  - æ­£åœ¨å†™å…¥Sheet: {sheet_name}...\")\n",
    "                if df is not None and not df.empty:\n",
    "                    df = df.loc[:, ~df.columns.duplicated()]\n",
    "                    for col in df.select_dtypes(\n",
    "                        include=[\"datetime64[ns]\", \"datetimetz\"]\n",
    "                    ).columns:\n",
    "                        df[col] = (\n",
    "                            df[col].dt.strftime(\"%Y-%m-%d %H:%M:%S\").replace(\"NaT\", \"\")\n",
    "                        )\n",
    "                    df.to_excel(\n",
    "                        writer, sheet_name=sheet_name, index=False, float_format=\"%.2f\"\n",
    "                    )\n",
    "                    auto_adjust_xlsx_columns(writer, df, sheet_name)\n",
    "\n",
    "        print(f\"\\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼å·²æˆåŠŸç”ŸæˆæŠ¥å‘Š '{config['output_filename']}'ï¼ğŸ‰ğŸ‰ğŸ‰\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸ”¥ğŸ”¥ğŸ”¥ ç”ŸæˆæŠ¥å‘Š '{config['output_filename']}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# --- 4. æ‰§è¡Œä¸»å‡½æ•° ---\n",
    "if __name__ == \"__main__\":\n",
    "    generate_company_monthly_report(\"é‚®æ”¿\")\n",
    "    generate_company_monthly_report(\"æå…”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649c511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®è¾“å…¥è·¯å¾„: /Users/lava/Documents/å›½å®¶é‚®æ”¿å±€å‘å±•ç ”ç©¶ä¸­å¿ƒå®ä¹ /python_data_analysis/æŠ¥å‘Šæ•°æ®/è¾“å‡º/data_analysis_result\n",
      "å›¾ç‰‡è¾“å‡ºè·¯å¾„: /Users/lava/Documents/å›½å®¶é‚®æ”¿å±€å‘å±•ç ”ç©¶ä¸­å¿ƒå®ä¹ /python_data_analysis/æŠ¥å‘Šæ•°æ®/è¾“å‡º/4_æŠ¥å‘Šå›¾ç‰‡\n",
      "Matplotlib å…¨å±€å­—ä½“å·²æˆåŠŸè®¾ç½®ä¸º: Microsoft YaHei\n",
      "ğŸš€ å¼€å§‹æ‰§è¡Œç»˜å›¾ä¸åˆ†æä»»åŠ¡ (æ–°ç‰ˆæµç¨‹)...\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: EMS ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: EMS_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: EMS_åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: EMS_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: EMS_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: EMS_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: EMS_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: å¿«åŒ… ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¿«åŒ…_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¿«åŒ…_åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¿«åŒ…_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¿«åŒ…_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¿«åŒ…_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¿«åŒ…_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: ä¸­é€š ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ä¸­é€š_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> ä¸­é€š çš„ 'åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´' æ•°æ®ä¸ºç©ºæˆ–æ— æ³•å¤„ç†ï¼Œä¸ç”Ÿæˆå›¾ç‰‡ã€‚\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ä¸­é€š_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ä¸­é€š_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ä¸­é€š_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ä¸­é€š_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: äº¬ä¸œ ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: äº¬ä¸œ_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: äº¬ä¸œ_åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: äº¬ä¸œ_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: äº¬ä¸œ_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: äº¬ä¸œ_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: äº¬ä¸œ_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: åœ†é€š ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: åœ†é€š_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: åœ†é€š_åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: åœ†é€š_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: åœ†é€š_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: åœ†é€š_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: åœ†é€š_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: å¾·é‚¦ ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¾·é‚¦_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¾·é‚¦_åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¾·é‚¦_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¾·é‚¦_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¾·é‚¦_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: å¾·é‚¦_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: æå…” ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: æå…”_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: æå…”_åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: æå…”_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: æå…”_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: æå…”_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: æå…”_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: ç”³é€š ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ç”³é€š_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ç”³é€š_åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ç”³é€š_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ç”³é€š_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ç”³é€š_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: ç”³é€š_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: éŸµè¾¾ ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: éŸµè¾¾_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: éŸµè¾¾_åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: éŸµè¾¾_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: éŸµè¾¾_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: éŸµè¾¾_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: éŸµè¾¾_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å…¬å¸: é¡ºä¸° ---\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: é¡ºä¸°_æ½æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> é¡ºä¸° çš„ 'åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´' æ•°æ®ä¸ºç©ºæˆ–æ— æ³•å¤„ç†ï¼Œä¸ç”Ÿæˆå›¾ç‰‡ã€‚\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: é¡ºä¸°_ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: é¡ºä¸°_åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: é¡ºä¸°_æ´¾é€æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "    -> å›¾ç‰‡å·²ä¿å­˜: é¡ºä¸°_ç­¾æ”¶æ—¶é—´_åˆ†å¸ƒå›¾.png\n",
      "\n",
      "\n",
      "--- ğŸ“ˆ åˆ†æç»“æœæ±‡æ€» ---\n",
      "\n",
      "--- åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´ (16:00 - 22:00) å æ¯”æ’å ---\n",
      "æ’å | å…¬å¸   | å æ¯”\n",
      "-----|--------|-------\n",
      "1    | å¾·é‚¦     | 79.69%\n",
      "2    | EMS    | 61.22%\n",
      "3    | æå…”     | 55.08%\n",
      "4    | éŸµè¾¾     | 49.64%\n",
      "5    | å¿«åŒ…     | 47.61%\n",
      "6    | åœ†é€š     | 47.48%\n",
      "7    | ç”³é€š     | 47.45%\n",
      "8    | äº¬ä¸œ     | 31.82%\n",
      "\n",
      "--- ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´ (16:00 - 22:00) å æ¯”æ’å ---\n",
      "æ’å | å…¬å¸   | å æ¯”\n",
      "-----|--------|-------\n",
      "1    | ä¸­é€š     | 55.57%\n",
      "2    | äº¬ä¸œ     | 51.83%\n",
      "3    | æå…”     | 48.30%\n",
      "4    | EMS    | 46.93%\n",
      "5    | ç”³é€š     | 43.20%\n",
      "6    | éŸµè¾¾     | 42.72%\n",
      "7    | å¿«åŒ…     | 34.38%\n",
      "8    | åœ†é€š     | 29.71%\n",
      "9    | é¡ºä¸°     | 18.42%\n",
      "10   | å¾·é‚¦     | 17.65%\n",
      "\n",
      "--- ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´ (22:00 - 24:00) å æ¯”æ’å ---\n",
      "æ’å | å…¬å¸   | å æ¯”\n",
      "-----|--------|-------\n",
      "1    | äº¬ä¸œ     | 27.67%\n",
      "2    | éŸµè¾¾     | 26.64%\n",
      "3    | ç”³é€š     | 25.95%\n",
      "4    | æå…”     | 23.84%\n",
      "5    | ä¸­é€š     | 22.67%\n",
      "6    | åœ†é€š     | 22.20%\n",
      "7    | é¡ºä¸°     | 19.27%\n",
      "8    | EMS    | 17.96%\n",
      "9    | å¿«åŒ…     | 13.38%\n",
      "10   | å¾·é‚¦     |  3.95%\n",
      "\n",
      "ğŸ‰ å…¨éƒ¨ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 7: ç”»å›¾\n",
    "# ==============================================================================\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties, fontManager\n",
    "\n",
    "# --- 1. é…ç½®åŒºåŸŸ ---\n",
    "\n",
    "# è·¯å¾„é…ç½®\n",
    "ROOT_PATH = Path.cwd()\n",
    "DATA_ANALYSIS_PATH = ROOT_PATH / \"æŠ¥å‘Šæ•°æ®\" / \"è¾“å‡º\" / \"data_analysis_result\"\n",
    "OUTPUT_IMAGE_PATH = ROOT_PATH / \"æŠ¥å‘Šæ•°æ®\" / \"è¾“å‡º\" / \"4_æŠ¥å‘Šå›¾ç‰‡\"\n",
    "\n",
    "OUTPUT_IMAGE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"æ•°æ®è¾“å…¥è·¯å¾„: {DATA_ANALYSIS_PATH}\")\n",
    "print(f\"å›¾ç‰‡è¾“å‡ºè·¯å¾„: {OUTPUT_IMAGE_PATH}\")\n",
    "\n",
    "# å…¬å¸åˆ—è¡¨ä¸æ–‡ä»¶åæ˜ å°„\n",
    "COMPANY_MAPPING = {\n",
    "    \"EMS\": \"EMS\",\n",
    "    \"ä¸­é€š\": \"ä¸­é€š\",\n",
    "    \"äº¬ä¸œ\": \"äº¬ä¸œ\",\n",
    "    \"åœ†é€š\": \"åœ†é€š\",\n",
    "    \"å¾·é‚¦\": \"å¾·é‚¦\",\n",
    "    \"æå…”\": \"æå…”\",\n",
    "    \"ç”³é€š\": \"ç”³é€š\",\n",
    "    \"éŸµè¾¾\": \"éŸµè¾¾\",\n",
    "    \"é¡ºä¸°\": \"é¡ºä¸°\",\n",
    "    \"é‚®æ”¿\": \"é‚®æ”¿\",  # \"é‚®æ”¿\" å¯¹åº” \"é‚®æ”¿\" æ–‡ä»¶\n",
    "}\n",
    "# åˆ†æå¯¹è±¡åŒ…å« \"å¿«åŒ…\"ï¼Œä½†åœ¨æ–‡ä»¶åå±‚é¢å®ƒç”± \"é‚®æ”¿\" æ–‡ä»¶ä»£è¡¨\n",
    "COMPANIES_TO_ANALYZE = [\n",
    "    \"EMS\",\n",
    "    \"å¿«åŒ…\",\n",
    "    \"ä¸­é€š\",\n",
    "    \"äº¬ä¸œ\",\n",
    "    \"åœ†é€š\",\n",
    "    \"å¾·é‚¦\",\n",
    "    \"æå…”\",\n",
    "    \"ç”³é€š\",\n",
    "    \"éŸµè¾¾\",\n",
    "    \"é¡ºä¸°\",\n",
    "]\n",
    "\n",
    "METRICS_TO_PLOT = [\n",
    "    \"æ½æ”¶æ—¶é—´\",\n",
    "    \"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\",\n",
    "    \"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´\",\n",
    "    \"åˆ°è¾¾æ”¶ä»¶åŸå¸‚æ—¶é—´\",\n",
    "    \"æ´¾é€æ—¶é—´\",\n",
    "    \"ç­¾æ”¶æ—¶é—´\",\n",
    "]\n",
    "\n",
    "# --- 2. å­—ä½“è§£å†³æ–¹æ¡ˆ ---\n",
    "font_path_str = \"å¾®è½¯é›…é»‘.ttf\"\n",
    "font_path_obj = Path(font_path_str)\n",
    "if font_path_obj.exists():\n",
    "    fontManager.addfont(str(font_path_obj))\n",
    "    chinese_font = FontProperties(fname=font_path_str)\n",
    "    plt.rcParams[\"font.sans-serif\"] = [chinese_font.get_name()]\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "    print(f\"Matplotlib å…¨å±€å­—ä½“å·²æˆåŠŸè®¾ç½®ä¸º: {chinese_font.get_name()}\")\n",
    "else:\n",
    "    print(f\"é”™è¯¯: åœ¨å½“å‰ç›®å½•æ‰¾ä¸åˆ°å­—ä½“æ–‡ä»¶ '{font_path_str}'ã€‚\")\n",
    "\n",
    "TIME_LABELS = [f\"{h:02d}:00-{(h + 1) % 24:02d}:00\" for h in range(24)]\n",
    "COLORS = [\n",
    "    \"olive\",\n",
    "    \"grey\",\n",
    "    \"yellow\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"palegoldenrod\",\n",
    "    \"darkolivegreen\",\n",
    "    \"pink\",\n",
    "    \"Thistle\",\n",
    "    \"steelblue\",\n",
    "    \"darkslategrey\",\n",
    "    \"slategray\",\n",
    "    \"tan\",\n",
    "    \"darkolivegreen\",\n",
    "    \"grey\",\n",
    "    \"pink\",\n",
    "    \"goldenrod\",\n",
    "    \"mediumslateblue\",\n",
    "    \"saddlebrown\",\n",
    "    \"olive\",\n",
    "    \"navy\",\n",
    "    \"sandybrown\",\n",
    "    \"moccasin\",\n",
    "    \"black\",\n",
    "]\n",
    "\n",
    "# --- 3. è¾…åŠ©å‡½æ•° ---\n",
    "\n",
    "\n",
    "def calculate_hourly_distribution(time_series: pd.Series) -> list:\n",
    "    \"\"\"é«˜æ•ˆè®¡ç®—ç»™å®šæ—¶é—´åºåˆ—ä¸­æ¯å°æ—¶çš„æ•°æ®ç‚¹æ•°é‡\"\"\"\n",
    "    if time_series.empty or time_series.isna().all():\n",
    "        return [0] * 24\n",
    "\n",
    "    dt_series = pd.to_datetime(time_series, errors=\"coerce\").dropna()\n",
    "    if dt_series.empty:\n",
    "        return [0] * 24\n",
    "\n",
    "    counts = dt_series.dt.hour.value_counts().sort_index()\n",
    "    hourly_counts = [0] * 24\n",
    "    for hour, count in counts.items():\n",
    "        if 0 <= hour < 24:\n",
    "            hourly_counts[hour] = int(count)\n",
    "    return hourly_counts\n",
    "\n",
    "\n",
    "def plot_and_save_distribution(\n",
    "    company_name: str, metric_name: str, hourly_counts: list, output_path: Path\n",
    "):\n",
    "    if sum(hourly_counts) == 0:\n",
    "        print(\n",
    "            f\"    -> {company_name} çš„ '{metric_name}' æ•°æ®ä¸ºç©ºæˆ–æ— æ³•å¤„ç†ï¼Œä¸ç”Ÿæˆå›¾ç‰‡ã€‚\"\n",
    "        )\n",
    "        return\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    bars = plt.bar(TIME_LABELS, hourly_counts, width=0.5, color=COLORS)\n",
    "    total_count = sum(hourly_counts)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            percentage = height / total_count\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{percentage:.1%}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=16,\n",
    "            )\n",
    "    plt.xlabel(\"24å°æ—¶åˆ†å¸ƒ\", fontsize=16)\n",
    "    plt.ylabel(\"å¿«ä»¶æ•°é‡\", fontsize=16)\n",
    "    plt.xticks(rotation=75, fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    save_path = output_path / f\"{company_name}_{metric_name}_åˆ†å¸ƒå›¾.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"    -> å›¾ç‰‡å·²ä¿å­˜: {save_path.name}\")\n",
    "\n",
    "\n",
    "# --- 4. ä¸»æ‰§è¡Œæµç¨‹ ---\n",
    "\n",
    "\n",
    "def run_plotting_and_analysis():\n",
    "    print(\"ğŸš€ å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡...\")\n",
    "    analysis_results = {}\n",
    "\n",
    "    for company in COMPANIES_TO_ANALYZE:\n",
    "        print(f\"\\n--- æ­£åœ¨å¤„ç†å…¬å¸: {company} ---\")\n",
    "\n",
    "        file_prefix = (\n",
    "            \"é‚®æ”¿\" if company == \"å¿«åŒ…\" else COMPANY_MAPPING.get(company, company)\n",
    "        )\n",
    "        file_path = DATA_ANALYSIS_PATH / f\"{file_prefix}_data_analysis_result.xlsx\"\n",
    "\n",
    "        if not file_path.exists():\n",
    "            print(f\"  -> æœªæ‰¾åˆ°æ–‡ä»¶: {file_path.name}ï¼Œè·³è¿‡è¯¥å…¬å¸ã€‚\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # è¯»å– 'çº¿è·¯è¯¦ç»†æ•°æ®' sheet\n",
    "            df = pd.read_excel(file_path, sheet_name=\"çº¿è·¯è¯¦ç»†æ•°æ®\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}ï¼Œè·³è¿‡è¯¥å…¬å¸ã€‚\")\n",
    "            continue\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"  -> æ–‡ä»¶ {file_path.name} çš„ 'çº¿è·¯è¯¦ç»†æ•°æ®' sheet ä¸ºç©ºï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "\n",
    "        analysis_results[company] = {}\n",
    "\n",
    "        for metric in METRICS_TO_PLOT:\n",
    "            if metric not in df.columns:\n",
    "                print(f\"    -> '{metric}' åˆ—ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚\")\n",
    "                continue\n",
    "\n",
    "            hourly_counts = calculate_hourly_distribution(df[metric])\n",
    "            plot_and_save_distribution(\n",
    "                company, metric, hourly_counts, OUTPUT_IMAGE_PATH\n",
    "            )\n",
    "\n",
    "            total_count = sum(hourly_counts)\n",
    "            if total_count == 0:\n",
    "                continue\n",
    "\n",
    "            if metric == \"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´\":\n",
    "                count_16_22 = sum(hourly_counts[16:22])\n",
    "                analysis_results[company][\"åˆ†æ‹£ä¸­å¿ƒ_16_22_å æ¯”\"] = (\n",
    "                    count_16_22 / total_count\n",
    "                )\n",
    "            if metric == \"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´\":\n",
    "                count_16_22 = sum(hourly_counts[16:22])\n",
    "                analysis_results[company][\"ç¦»å¼€åŸå¸‚_16_22_å æ¯”\"] = (\n",
    "                    count_16_22 / total_count\n",
    "                )\n",
    "                count_22_24 = sum(hourly_counts[22:24])\n",
    "                analysis_results[company][\"ç¦»å¼€åŸå¸‚_22_24_å æ¯”\"] = (\n",
    "                    count_22_24 / total_count\n",
    "                )\n",
    "\n",
    "    print(\"\\n\\n--- ğŸ“ˆ åˆ†æç»“æœæ±‡æ€» ---\")\n",
    "\n",
    "    def print_ranking_results(metric_key: str, description: str):\n",
    "        print(f\"\\n--- {description} å æ¯”æ’å ---\")\n",
    "        company_ratios = []\n",
    "        for company, metrics in analysis_results.items():\n",
    "            if metric_key in metrics:\n",
    "                company_ratios.append((company, metrics[metric_key]))\n",
    "        if not company_ratios:\n",
    "            print(\"æ— ç›¸å…³æ•°æ®å¯ä¾›æ’åã€‚\")\n",
    "            return\n",
    "        sorted_ratios = sorted(company_ratios, key=lambda item: item[1], reverse=True)\n",
    "        print(\"æ’å | å…¬å¸   | å æ¯”\")\n",
    "        print(\"-----|--------|-------\")\n",
    "        for i, (company, ratio) in enumerate(sorted_ratios):\n",
    "            print(f\"{i + 1:<4} | {company:<6} | {ratio:>6.2%}\")\n",
    "\n",
    "    print_ranking_results(\"åˆ†æ‹£ä¸­å¿ƒ_16_22_å æ¯”\", \"åˆ°è¾¾åˆ†æ‹£ä¸­å¿ƒæ—¶é—´ (16:00 - 22:00)\")\n",
    "    print_ranking_results(\"ç¦»å¼€åŸå¸‚_16_22_å æ¯”\", \"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´ (16:00 - 22:00)\")\n",
    "    print_ranking_results(\"ç¦»å¼€åŸå¸‚_22_24_å æ¯”\", \"ç¦»å¼€å¯„ä»¶åŸå¸‚æ—¶é—´ (22:00 - 24:00)\")\n",
    "    print(\"\\nğŸ‰ å…¨éƒ¨ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼\")\n",
    "\n",
    "\n",
    "# --- 5. æ‰§è¡Œä¸»å‡½æ•° ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_plotting_and_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
